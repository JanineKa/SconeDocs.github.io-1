{
    "docs": [
        {
            "location": "/", 
            "text": "Welcome to SCONE\n\n\n\n\nNEWS (2018-03-17) SCONE supports nginx\n\n\nSCONE adds an updated curated nginx image (sconecuratedimages/apps:nginx-1.13-alpine)\n\n\n\n\n\n\nNEWS (2018-03-07): SCONE supports JavaScript (V8) as well as Node\n\n\nSCONE adds an updated curated \nNode\n image (sconecuratedimages/apps:node-8.9.4)\n\n\n\n\n\n\nNEWS (2018-02-02): SCONE supports Java and zookeeper\n\n\nSCONE adds support for \nJava\n and also for Zookeeper (sconecuratedimages/crosscompilers:zookeeper).\n\n\n\n\n\n\nSOON TO COME: Improved protection against side channel attacks\n\n\nWhile SGX is affected by Spectre, we are currently adding protection against Spectre (variants 1 and 2) as well as L1 and L2 side channel attacks to SCONE\n\n\n\n\nOverview\n\n\nSCONE is a platform to \nbuild and run secure applications\n with the help of \nIntel SGX\n (Software Guard eXtensions)\n1\n. \nIn a nutshell, our objective is to run applications such that data is \nalways encrypted\n, i.e., all data at rest, all data on the wire as well as all data in main memory is encrypted. Even the program code can be encrypted. SCONE helps to protect data, computations and code against \nattackers with root access\n.\n\n\nOur aim is it to make it \nas easy as possible\n to secure existing application. Switching to SCONE is simple since applications do not need to be modified. SCONE supports the most popular programming languages like JavaScript, Python - including PyPy, Java, Rust, Go, C, and C++ but also some ancient languages like Fortran.  Avoiding source code changes helps to ensure that applications can later run on different trusted execution environments. Moreover, there is no risk for hardware lock-in nor software lock-in -  even into SCONE.   \n\n\nSCONE can be used on top of Kubernetes, Ranger and Docker. We provide a tight integration with Docker Swarm.  In case you are already using Docker \nstack files\n, we provide a \nsimple way\n to secure your services and applications.  We will provide a similar integration with Kubernetes later in 2018.\n\n\nSCONE \nscales better\n than competing solutions since it uses an advanced thread management and s very efficient way how to perform \nsystem calls\n. SCONE has an \nintegrated secrets and configuration management\n - simplifying the distribution of secrets without application changes by performing an attestation of applications. \n\n\nWhile SCONE focuses on securing containers and cloud-native applications, SCONE can help you to secure almost any program running on top of Linux.\n\n\nSo, what problems can SCONE help me to solve?\n\n\nSecure application configuration\n\n\nSCONE provides applications with secrets in a secure fashion. \nWhy is that a problem?\n  Say, you want to run MySQL and you configure MySQL to encrypt its data at rest. To do so, MySQL requires a key to decrypt and encrypt its files. One can store this key in the MySQL configuration file but this configuration file cannot be encrypted since MySQL would need a key to decrypt the file. SCONE helps developers to solve such configuration issues in the following ways:\n\n\n\n\n\n\nsecure configuration files\n. SCONE can transparently decrypt encrypted configuration files. It will give access to the plain text only to a given program, like, MySQL. No source code changes are needed for this to work.\n\n\n\n\n\n\nsecure environment variables\n. SCONE gives applications access to environment variables that are not visible to anybody else - even users with root access or the operating system. \nWhy would I need this?\n Consider the MySQL example from above. You can pass user passwords via environment variables like \nMYSQL_ROOT_PASSWORD\n and \nMYSQL_PASSWORD\n to MySQL. We need to protect these environment variables to prevent unauthorized accesses to the MySQL database.\n\n\n\n\n\n\nsecure command line arguments\n. Some applications might not use environment variables but command line arguments to pass secrets to the application. SCONE provides a secure way to pass arguments to your application without other privileged parties, like the operating system, being able to see the arguments.\n\n\n\n\n\n\nTransparent attestation\n\n\nSCONE verifies that the correct code is running\n before passing any configuration info to the application. To ensure this, SCONE provides a \nlocal attestation and configuration service\n: this service provides only the code with the correct signature (\nMRENCLAVE\n) with its secrets. For debugging and development, you can run code inside of enclaves without attestation.\n\n\nSecure main memory\n\n\nAn adversary with root access can read the memory content of any process. In this way, an adversary can gain access to keys that an application is using, for example, the keys to protect its data at rest. SCONE helps to \nprotect the main memory\n:\n\n\n\n\n\n\nno access by adversaries - even those who have root access,\n\n\n\n\n\n\nno access by the operating system - even if compromised,\n\n\n\n\n\n\nno access by the hypervisor - even if compromised, and\n\n\n\n\n\n\nno access by the cloud provider, and\n\n\n\n\n\n\nno access by evil maids - despite having physical access to the host.\n\n\n\n\n\n\nIntegration with secure key store\n\n\nEncryption keys must be protected. In many installations, one does not want humans to be able to see encryption keys. Hence, one generates keys and stores these in keystores. SCONE supports the integration with a keystore.\n\n\nTransparent TLS encryption\n\n\nSome popular applications like \nmemcached\n or \nZookeeper\n2\n do not support TLS out of the box. SCONE can transparently add TLS encryption to TCP connections. In SCONE, we never uses an external process for TLS termination\n3\n. In this way, the plain text is never seen by the operating system or any adversary.\n\n\nTransparent file protection\n\n\nSCONE protects the integrity and confidentiality of files via \ntransparent file protection\n. This protection does not require any source code changes. A file can either be \nintegrity-protected\n only (i.e., the file is stored in plain text but modifications are detected) or \nconfidentiality-\n and \nintegrity-protected\n (i.e., the file is encrypted and modifications are detected).\n\n\nEase of use\n\n\nWe provide slightly \nextended Docker stack files\n to start an application consisting of a set of services inside of enclaves.\n\n\n \nscontain.com\n, March 2018. \nQuestions or Suggestions?\n\n\n\n\n\n\n\n\n\n\nWe plan to support alternative trusted execution environments in future releases of SCONE.\n\n\n\n\n\n\nZookeeper replicates its state amongst a group of servers.  Zookeeper does not support protecting the communication between these \nservers by TLS. SCONE can add transparent support TLS for Zookeeper to ensure that the integrity and confidentiality of the data exchanged between the Zookeeper server is protected.\n\n\n\n\n\n\nMemcached could be protected, for example, with the help of a \nstunnel\n.", 
            "title": "Introduction"
        }, 
        {
            "location": "/#welcome-to-scone", 
            "text": "NEWS (2018-03-17) SCONE supports nginx  SCONE adds an updated curated nginx image (sconecuratedimages/apps:nginx-1.13-alpine)    NEWS (2018-03-07): SCONE supports JavaScript (V8) as well as Node  SCONE adds an updated curated  Node  image (sconecuratedimages/apps:node-8.9.4)    NEWS (2018-02-02): SCONE supports Java and zookeeper  SCONE adds support for  Java  and also for Zookeeper (sconecuratedimages/crosscompilers:zookeeper).    SOON TO COME: Improved protection against side channel attacks  While SGX is affected by Spectre, we are currently adding protection against Spectre (variants 1 and 2) as well as L1 and L2 side channel attacks to SCONE", 
            "title": "Welcome to SCONE"
        }, 
        {
            "location": "/#overview", 
            "text": "SCONE is a platform to  build and run secure applications  with the help of  Intel SGX  (Software Guard eXtensions) 1 . \nIn a nutshell, our objective is to run applications such that data is  always encrypted , i.e., all data at rest, all data on the wire as well as all data in main memory is encrypted. Even the program code can be encrypted. SCONE helps to protect data, computations and code against  attackers with root access .  Our aim is it to make it  as easy as possible  to secure existing application. Switching to SCONE is simple since applications do not need to be modified. SCONE supports the most popular programming languages like JavaScript, Python - including PyPy, Java, Rust, Go, C, and C++ but also some ancient languages like Fortran.  Avoiding source code changes helps to ensure that applications can later run on different trusted execution environments. Moreover, there is no risk for hardware lock-in nor software lock-in -  even into SCONE.     SCONE can be used on top of Kubernetes, Ranger and Docker. We provide a tight integration with Docker Swarm.  In case you are already using Docker  stack files , we provide a  simple way  to secure your services and applications.  We will provide a similar integration with Kubernetes later in 2018.  SCONE  scales better  than competing solutions since it uses an advanced thread management and s very efficient way how to perform  system calls . SCONE has an  integrated secrets and configuration management  - simplifying the distribution of secrets without application changes by performing an attestation of applications.   While SCONE focuses on securing containers and cloud-native applications, SCONE can help you to secure almost any program running on top of Linux.", 
            "title": "Overview"
        }, 
        {
            "location": "/#so-what-problems-can-scone-help-me-to-solve", 
            "text": "", 
            "title": "So, what problems can SCONE help me to solve?"
        }, 
        {
            "location": "/#secure-application-configuration", 
            "text": "SCONE provides applications with secrets in a secure fashion.  Why is that a problem?   Say, you want to run MySQL and you configure MySQL to encrypt its data at rest. To do so, MySQL requires a key to decrypt and encrypt its files. One can store this key in the MySQL configuration file but this configuration file cannot be encrypted since MySQL would need a key to decrypt the file. SCONE helps developers to solve such configuration issues in the following ways:    secure configuration files . SCONE can transparently decrypt encrypted configuration files. It will give access to the plain text only to a given program, like, MySQL. No source code changes are needed for this to work.    secure environment variables . SCONE gives applications access to environment variables that are not visible to anybody else - even users with root access or the operating system.  Why would I need this?  Consider the MySQL example from above. You can pass user passwords via environment variables like  MYSQL_ROOT_PASSWORD  and  MYSQL_PASSWORD  to MySQL. We need to protect these environment variables to prevent unauthorized accesses to the MySQL database.    secure command line arguments . Some applications might not use environment variables but command line arguments to pass secrets to the application. SCONE provides a secure way to pass arguments to your application without other privileged parties, like the operating system, being able to see the arguments.", 
            "title": "Secure application configuration"
        }, 
        {
            "location": "/#transparent-attestation", 
            "text": "SCONE verifies that the correct code is running  before passing any configuration info to the application. To ensure this, SCONE provides a  local attestation and configuration service : this service provides only the code with the correct signature ( MRENCLAVE ) with its secrets. For debugging and development, you can run code inside of enclaves without attestation.", 
            "title": "Transparent attestation"
        }, 
        {
            "location": "/#secure-main-memory", 
            "text": "An adversary with root access can read the memory content of any process. In this way, an adversary can gain access to keys that an application is using, for example, the keys to protect its data at rest. SCONE helps to  protect the main memory :    no access by adversaries - even those who have root access,    no access by the operating system - even if compromised,    no access by the hypervisor - even if compromised, and    no access by the cloud provider, and    no access by evil maids - despite having physical access to the host.", 
            "title": "Secure main memory"
        }, 
        {
            "location": "/#integration-with-secure-key-store", 
            "text": "Encryption keys must be protected. In many installations, one does not want humans to be able to see encryption keys. Hence, one generates keys and stores these in keystores. SCONE supports the integration with a keystore.", 
            "title": "Integration with secure key store"
        }, 
        {
            "location": "/#transparent-tls-encryption", 
            "text": "Some popular applications like  memcached  or  Zookeeper 2  do not support TLS out of the box. SCONE can transparently add TLS encryption to TCP connections. In SCONE, we never uses an external process for TLS termination 3 . In this way, the plain text is never seen by the operating system or any adversary.", 
            "title": "Transparent TLS encryption"
        }, 
        {
            "location": "/#transparent-file-protection", 
            "text": "SCONE protects the integrity and confidentiality of files via  transparent file protection . This protection does not require any source code changes. A file can either be  integrity-protected  only (i.e., the file is stored in plain text but modifications are detected) or  confidentiality-  and  integrity-protected  (i.e., the file is encrypted and modifications are detected).", 
            "title": "Transparent file protection"
        }, 
        {
            "location": "/#ease-of-use", 
            "text": "We provide slightly  extended Docker stack files  to start an application consisting of a set of services inside of enclaves.    scontain.com , March 2018.  Questions or Suggestions?      We plan to support alternative trusted execution environments in future releases of SCONE.    Zookeeper replicates its state amongst a group of servers.  Zookeeper does not support protecting the communication between these \nservers by TLS. SCONE can add transparent support TLS for Zookeeper to ensure that the integrity and confidentiality of the data exchanged between the Zookeeper server is protected.    Memcached could be protected, for example, with the help of a  stunnel .", 
            "title": "Ease of use"
        }, 
        {
            "location": "/outline/", 
            "text": "Getting Started\n\n\nThe simplest way to get started with SCONE is to use it in \nsimulation\n mode.\nIn \nsimulation mode\n, you do not need to install any new software on your host - as long as you have already a Docker engine running.\nYou could start by looking at the introduction of the \nSCONE CLI\n. \n\n\nAfter that, you could try to compile a simple \nhello world\n program. For now, we have a light access control to some of the SCONE docker images - like the ones needed for compiling the hello world program. Just send an email with your free \nDocker ID\n to \ninfo@scontain.com\n. After gaining access, you can compile the hello world program as shown in the \nSCONE Tutorial\n. You can run the program in simulation mode.  After that,  check out how you could automate the compilation with the help of a \nDockerfile\n. Next would be to run an example application in a \nSwarm\n.\n\n\nHardware Mode\n\n\nIf you have access to a SGX-capable host, you can, of course, run your programs in Intel SGX enclaves. For ease of use, we create all Docker images used in this tutorial such that applications run inside of enclaves if available. Otherwise, they run in simulation mode, i.e., outside of an enclave but all SCONE software running. When running your software in operations, you would of course force the programs to run only inside of enclaves. We show later how this can be enforced with the help of SCONE remote attestation.\n\n\nTo be able to use hardware mode, programs need access to the SGX device. If your hosts have already a Intel SGX driver installed, you are all set. Otherwise, SCONE can help you to install a lightly patched SGX driver that provides some more monitoring information - like the number of free EPC pages of a host (\nSCONE Host Setup\n).\n\n\nIf you want to run your programs in containers across multiple hosts with the help of a Docker swarm, you need to give access to the sgx device. Right now, Docker Swarm does not support mapping a device on all hosts. We have a simple patched version that gives access to the sgx device to all containers across all hosts. \nSCONE Host Setup\n installs this patched Docker engine.\n\n\nAfter having installed the SGX driver, you can run different applications inside of enclaves. In addition to \nC\n, \nC++\n, \nGO\n, \nFortran\n and \nRust\n (as part of SCONE crosscompiler image \nssconecuratedimages/crosscompilers:scone\n ), we also support \nPython\n, \nJava\n,  Node.js, and soon PHP and Lua.\n\n\nOutline\n\n\nThe SCONE platform technical documentation is structured as follows:\n\n\n\n\n\n\nSCONE Application-Oriented Security\n: a short introduction into SCONE's unique security approach.\n\n\n\n\n\n\nSCONE Background\n: a short introduction into why good systems security is difficult to achieve and how SCONE helps to complement systems security.\n\n\n\n\n\n\nSCONE CLI\n: introduces the SCONE command line interface and how to install the SCONE CLI.\n\n\n\n\n\n\nSCONE Host Setup\n: to run secure containers, we need to configure each host to run a Linux SGX driver and also a (for convenience, a patched) Docker engine.\n\n\n\n\n\n\nSCONE SGX Toolchain\n: SCONE supports cross-compilers (C, C++, more to come soon) to compile and build  applications for SGX enclaves.\n\n\n\n\n\n\nSCONE Curated Container Images\n: we will support a set of secure container images to simplify the use of SCONE.\n\n\n\n\n\n\nSCONE Tutorial\n: we show how to compile simple example applications with the help of the SCONE SGX Toolchain.\n\n\n\n\n\n\nSCONE Create Image\n: we show how to create a container image for Docker hub.\n\n\n\n\n\n\nSCONE Dockerfile\n: we support the use of Dockerfiles to generate Docker images that contain services running inside of SGX enclaves (this requires a \npatched Docker Engine\n).\n\n\n\n\n\n\nSCONE Swarm Example\n: shows an example of how to start SCONE services on top of a Docker Swarm.\n\n\n\n\n\n\nSCONE File Protection\n: we show how to transparently encrypt or authenticate files in the file system.\n\n\n\n\n\n\nGO inside Enclave\n: shows how to run GO programs inside of enclaves.\n\n\n\n\n\n\nPython inside Enclave\n: shows how to run simple Python applications inside of enclaves.\n\n\n\n\n\n\nSCONE Environment variables\n: short description of the SCONE environment variables.\n\n\n\n\n\n\nDetailed description of \nscone command line interface:\n\n\n\n\n\n\nscone host\n: installs patched SGX driver and patched Docker Engine on a host\n\n\n\n\n\n\nscone swarm\n: check if all hosts of a docker swarm are correctly installed\n\n\n\n\n\n\n\n\n\n\nPublications\n:  some of the technical aspects of SCONE have been published in scientific papers. We provide a short summary of the papers and links to the pdfs.\n\n\n\n\n\n\nGlossary\n:  definition of terms used within the SCONE technical documentation.\n\n\n\n\n\n\n \nscontain.com\n, November 2017. \nQuestions or Suggestions?", 
            "title": "Getting Started"
        }, 
        {
            "location": "/outline/#getting-started", 
            "text": "The simplest way to get started with SCONE is to use it in  simulation  mode.\nIn  simulation mode , you do not need to install any new software on your host - as long as you have already a Docker engine running.\nYou could start by looking at the introduction of the  SCONE CLI .   After that, you could try to compile a simple  hello world  program. For now, we have a light access control to some of the SCONE docker images - like the ones needed for compiling the hello world program. Just send an email with your free  Docker ID  to  info@scontain.com . After gaining access, you can compile the hello world program as shown in the  SCONE Tutorial . You can run the program in simulation mode.  After that,  check out how you could automate the compilation with the help of a  Dockerfile . Next would be to run an example application in a  Swarm .", 
            "title": "Getting Started"
        }, 
        {
            "location": "/outline/#hardware-mode", 
            "text": "If you have access to a SGX-capable host, you can, of course, run your programs in Intel SGX enclaves. For ease of use, we create all Docker images used in this tutorial such that applications run inside of enclaves if available. Otherwise, they run in simulation mode, i.e., outside of an enclave but all SCONE software running. When running your software in operations, you would of course force the programs to run only inside of enclaves. We show later how this can be enforced with the help of SCONE remote attestation.  To be able to use hardware mode, programs need access to the SGX device. If your hosts have already a Intel SGX driver installed, you are all set. Otherwise, SCONE can help you to install a lightly patched SGX driver that provides some more monitoring information - like the number of free EPC pages of a host ( SCONE Host Setup ).  If you want to run your programs in containers across multiple hosts with the help of a Docker swarm, you need to give access to the sgx device. Right now, Docker Swarm does not support mapping a device on all hosts. We have a simple patched version that gives access to the sgx device to all containers across all hosts.  SCONE Host Setup  installs this patched Docker engine.  After having installed the SGX driver, you can run different applications inside of enclaves. In addition to  C ,  C++ ,  GO ,  Fortran  and  Rust  (as part of SCONE crosscompiler image  ssconecuratedimages/crosscompilers:scone  ), we also support  Python ,  Java ,  Node.js, and soon PHP and Lua.", 
            "title": "Hardware Mode"
        }, 
        {
            "location": "/outline/#outline", 
            "text": "The SCONE platform technical documentation is structured as follows:    SCONE Application-Oriented Security : a short introduction into SCONE's unique security approach.    SCONE Background : a short introduction into why good systems security is difficult to achieve and how SCONE helps to complement systems security.    SCONE CLI : introduces the SCONE command line interface and how to install the SCONE CLI.    SCONE Host Setup : to run secure containers, we need to configure each host to run a Linux SGX driver and also a (for convenience, a patched) Docker engine.    SCONE SGX Toolchain : SCONE supports cross-compilers (C, C++, more to come soon) to compile and build  applications for SGX enclaves.    SCONE Curated Container Images : we will support a set of secure container images to simplify the use of SCONE.    SCONE Tutorial : we show how to compile simple example applications with the help of the SCONE SGX Toolchain.    SCONE Create Image : we show how to create a container image for Docker hub.    SCONE Dockerfile : we support the use of Dockerfiles to generate Docker images that contain services running inside of SGX enclaves (this requires a  patched Docker Engine ).    SCONE Swarm Example : shows an example of how to start SCONE services on top of a Docker Swarm.    SCONE File Protection : we show how to transparently encrypt or authenticate files in the file system.    GO inside Enclave : shows how to run GO programs inside of enclaves.    Python inside Enclave : shows how to run simple Python applications inside of enclaves.    SCONE Environment variables : short description of the SCONE environment variables.    Detailed description of  scone command line interface:    scone host : installs patched SGX driver and patched Docker Engine on a host    scone swarm : check if all hosts of a docker swarm are correctly installed      Publications :  some of the technical aspects of SCONE have been published in scientific papers. We provide a short summary of the papers and links to the pdfs.    Glossary :  definition of terms used within the SCONE technical documentation.      scontain.com , November 2017.  Questions or Suggestions?", 
            "title": "Outline"
        }, 
        {
            "location": "/appsecurity/", 
            "text": "Application-Oriented Security\n\n\nSCONE supports developers and \nservice providers\n (i.e., companies operating applications accessible via the Internet)\nto protect the confidentiality and integrity of their applications - even when running in environments that\ncannot be completely trusted. SCONE's focus is on supporting the development of programs running inside of containers like \nmicroservice-based applications\n as well as \ncloud-native applications\n. However, SCONE can protect most programs running on top of Linux.\n\n\n\n\nSCONE supports developers and service providers to ensure end-to-end encryption in the sense that \ndata is always encrypted\n, i.e., while being transmitted,\nwhile being at rest and even while being processed. The latter has only recently become possible with the help of a novel CPU extension by Intel (SGX). To reduce the required computing resources, a service provider can decide what to protect and what not to protect.  For example, a service that operates only on encrypted data might not need to be protected with SGX.\n\n\nOur general recommendation is, however, that developers should protect all parts of an application. The cost of computing resources have been dropping dramatically and hence, the reduction in cost might not be justified when compared with the potential costs - and also loss of reputation - by data breaches. SCONE supports horizontal scalability, i.e., throughput and latency can typically be controlled via the number of instances of a service.\n\n\nEase of Use\n\n\nSCONE supports strong application-oriented security with a workflow like Docker, i.e., SCONE supports \nDockerfiles\n as well as extended Docker \ncompose\n files. This simplifies the construction and operation of applications consisting of a set of containers. This fits, in particular, modern cloud-native applications consisting of microservices and each microservice runs either in a standard or a secure container.\n\n\nThe Docker Engine itself is not protected. The Docker Engine, like the operating system, never sees any plain text data. This facilitates that the Docker Engine or the Docker Swarm can be managed by a cloud provider. SCONE helps a service providers to ensure the confidentiality and integrity of the application data while the cloud provider will ensure the availability of the service. For example, with the help of Docker Swarm, failed containers will automatically be restarted on an appropriate host.\n\n\n \nscontain.com\n, November 2017. \nQuestions or Suggestions?", 
            "title": "Application-oriented security"
        }, 
        {
            "location": "/appsecurity/#application-oriented-security", 
            "text": "SCONE supports developers and  service providers  (i.e., companies operating applications accessible via the Internet)\nto protect the confidentiality and integrity of their applications - even when running in environments that\ncannot be completely trusted. SCONE's focus is on supporting the development of programs running inside of containers like  microservice-based applications  as well as  cloud-native applications . However, SCONE can protect most programs running on top of Linux.   SCONE supports developers and service providers to ensure end-to-end encryption in the sense that  data is always encrypted , i.e., while being transmitted,\nwhile being at rest and even while being processed. The latter has only recently become possible with the help of a novel CPU extension by Intel (SGX). To reduce the required computing resources, a service provider can decide what to protect and what not to protect.  For example, a service that operates only on encrypted data might not need to be protected with SGX.  Our general recommendation is, however, that developers should protect all parts of an application. The cost of computing resources have been dropping dramatically and hence, the reduction in cost might not be justified when compared with the potential costs - and also loss of reputation - by data breaches. SCONE supports horizontal scalability, i.e., throughput and latency can typically be controlled via the number of instances of a service.", 
            "title": "Application-Oriented Security"
        }, 
        {
            "location": "/appsecurity/#ease-of-use", 
            "text": "SCONE supports strong application-oriented security with a workflow like Docker, i.e., SCONE supports  Dockerfiles  as well as extended Docker  compose  files. This simplifies the construction and operation of applications consisting of a set of containers. This fits, in particular, modern cloud-native applications consisting of microservices and each microservice runs either in a standard or a secure container.  The Docker Engine itself is not protected. The Docker Engine, like the operating system, never sees any plain text data. This facilitates that the Docker Engine or the Docker Swarm can be managed by a cloud provider. SCONE helps a service providers to ensure the confidentiality and integrity of the application data while the cloud provider will ensure the availability of the service. For example, with the help of Docker Swarm, failed containers will automatically be restarted on an appropriate host.    scontain.com , November 2017.  Questions or Suggestions?", 
            "title": "Ease of Use"
        }, 
        {
            "location": "/background/", 
            "text": "SCONE Background\n\n\n\n\nCloud Security\n. The objective of SCONE is to help service providers to build secure applications for public, private or hybrid clouds. This means that the focus of SCONE is on application-oriented security and not on the security of the underlying cloud system. Of course, SCONE-based applications benefit from strong security properties of the underlying cloud because this minimizes, for example, the attack surface of SCONE-based applications and by providing higher availability. SCONE helps to ensure the security of an application, i.e., the application's integrity and confidentiality, even if the security of the underlying cloud or system software would be compromised. The security of applications is ensured with the help of Intel SGX enclaves. \n\n\n\n\n\n\n\n\nWorkflow\n. SCONE combines strong security with the ease of use of Docker. SCONE supports a workflow very similar to that of Docker. It supports the construction of applications consisting of multiple containers while ensuring end-to-end encryption between all application components in the sense that all network traffic, all files and even all computation is encrypted. A service provider can ensure the confidentiality and integrity of all application data. In particular, SCONE supports the construction of applications such that no higher privileged software like the operating system or the hypervisor, nor any system administrator with root access nor cold boot attacks can gain access to application data.\n\n\n\n\n\n\n\n\nSide Channel Attacks\n. Side Channel attacks on Intel SGX are the focus of a several recent research papers. First, mounting a successful side-channels is much more difficult than just dumping the memory of an existing application. In SCONE, we provide scheduling within enclaves which makes it more difficult for an attacker to determine which core is executing what function. Moreover, we are working on a compiler extension that will harden applications against side channel attacks. Until will release this extension, a pragmatic solution would be to run applications that might be susceptible to side channel attacks either on OpenStack \nisolated hosts\n or on OpenStack \nbaremetal clouds\n.\n\n\n\n\nProblem: Defender's Dilemma\n\n\nTraditionally, one ensures the security of an application by ensuring that the system software, i.e., the hypervisor, operating system and cloud software is trustworthy. This not only protects the integrity and confidentiality of the system data but also protects the security of the applications. A service provider running applications in the cloud must trust all system software and also all administrators who have root or physical access to these systems.\n\n\nA popular way to intrude into a system is to steal the credentials of a system administrator. With these root credentials, one gains access to all data being processed in this system as well as all keys that are kept in main memory or in some plain text files. If stealing credentials would be too difficult, an attacker will look for other ways to attack a system, like, exploiting known code vulnerabilities.\n\n\nFor an attacker, it might be sufficient to exploit a single vulnerability in either the application or the system software to violate the application security. The problem is that the defenders must protect against the exploitation of all code vulnerabilities that might exist in the source code. A service provider might not have access to all source code of the system software that the cloud provider uses to operate the cloud. Even if the source code were available, this will typically be too large to be inspected.\n\n\nTo show that this is a difficult problem, let's look at the number of lines of source code of common system software components. While lines of source code is not an ideal  indicator for the number of vulnerabilities, it gives some indication of the problem we are facing. Some security researchers state that given the current state of the art, only code with up to 10,000s of lines of code can be reasonably inspected. Just the system software itself contains millions of lines of code. This is orders of magnitudes more than we can reasonably expect to be able to inspect.\n\n\nSCONE runs on top of Linux - which contains millions of lines of code and is still growing in size with each release:\n\n\n\n\nLinux Lines of Code (StefanPohl, CC0, \noriginal\n}\n\n\nOpenStack is a popular open source software to manage clouds. OpenStack - despite being relatively young - has been growing dramatically over the years that it has already reached 5 million lines of code (including comments and blank lines):\n\n\n\n\nOpenStack Lines of Code (OpenHub \noriginal\n)\n\n\nTo manage containers, we need an engine like Docker. Docker is younger than OpenStack but has nevertheless reached already more than 180,000 lines of code:\n\n\n\n\nDocker Lines of Code (OpenHub \noriginal\n)\n\n\nCode complexity\n.There is no one-to-one correlation between lines of codes and bugs. Static analysis of open source code repositories indicates approximately 0.61 defects per 1,000 LOC. A recent analysis of Linux shows that, despite an increasing number of defects being fixed, there are always approximately 5,000 defects waiting to be fixed. Not all of these defects can, however, be exploited for security attacks. Another analysis found that approximately 500 security-relevant bugs were fixed in Linux over the past five years - bugs that had been in the kernel for five years before being discovered and fixed. Commercial code had a slightly higher defect density than open source projects. Hence, we need to expect vulnerabilities in commercial software too.\n\n\nSCONE Approach\n\n\nThe approach of SCONE is to partition the code and to place essential components of an application into separate enclaves. Practically, it is quite difficult to split an existing code base of a single process into one component that runs inside an enclave and a component that runs outside of an enclave. However, many modern applications - like cloud-native applications - are already partitioned in several components running in separate address spaces. These components are typically called microservices. This partitioning facilitates a more intelligent scaling of services as well as a scaling of the development team.\n\n\nA large application might consist of a variety of microservices. Not all microservices of an application need to run inside enclaves to protect the application\u2019s integrity and confidentiality. For example, some services might only process encrypted data, like encrypted log data, and do not need to run inside enclaves.  Also, the resource manager does not need to run in an enclave either. However, we recommend that each microservice that has the credential to send requests to at least one microservice running inside an enclave, should itself also run inside of an enclave to restrict the access to enclaved microservices.\n\n\nCurrent SGX-capable CPUs have a limited EPC (Extended Page Cache) size. If the working set of a microservice does not fit inside the EPC, overheads can become high. The usage of microservices supports horizontal scalability. This helps to cope with limited EPC (extended page cache) by spreading secure microservices across different hosts.\n\n\n \nscontain.com\n, March 2018. \nQuestions or Suggestions?", 
            "title": "SCONE Background"
        }, 
        {
            "location": "/background/#scone-background", 
            "text": "Cloud Security . The objective of SCONE is to help service providers to build secure applications for public, private or hybrid clouds. This means that the focus of SCONE is on application-oriented security and not on the security of the underlying cloud system. Of course, SCONE-based applications benefit from strong security properties of the underlying cloud because this minimizes, for example, the attack surface of SCONE-based applications and by providing higher availability. SCONE helps to ensure the security of an application, i.e., the application's integrity and confidentiality, even if the security of the underlying cloud or system software would be compromised. The security of applications is ensured with the help of Intel SGX enclaves.      Workflow . SCONE combines strong security with the ease of use of Docker. SCONE supports a workflow very similar to that of Docker. It supports the construction of applications consisting of multiple containers while ensuring end-to-end encryption between all application components in the sense that all network traffic, all files and even all computation is encrypted. A service provider can ensure the confidentiality and integrity of all application data. In particular, SCONE supports the construction of applications such that no higher privileged software like the operating system or the hypervisor, nor any system administrator with root access nor cold boot attacks can gain access to application data.     Side Channel Attacks . Side Channel attacks on Intel SGX are the focus of a several recent research papers. First, mounting a successful side-channels is much more difficult than just dumping the memory of an existing application. In SCONE, we provide scheduling within enclaves which makes it more difficult for an attacker to determine which core is executing what function. Moreover, we are working on a compiler extension that will harden applications against side channel attacks. Until will release this extension, a pragmatic solution would be to run applications that might be susceptible to side channel attacks either on OpenStack  isolated hosts  or on OpenStack  baremetal clouds .", 
            "title": "SCONE Background"
        }, 
        {
            "location": "/background/#problem-defenders-dilemma", 
            "text": "Traditionally, one ensures the security of an application by ensuring that the system software, i.e., the hypervisor, operating system and cloud software is trustworthy. This not only protects the integrity and confidentiality of the system data but also protects the security of the applications. A service provider running applications in the cloud must trust all system software and also all administrators who have root or physical access to these systems.  A popular way to intrude into a system is to steal the credentials of a system administrator. With these root credentials, one gains access to all data being processed in this system as well as all keys that are kept in main memory or in some plain text files. If stealing credentials would be too difficult, an attacker will look for other ways to attack a system, like, exploiting known code vulnerabilities.  For an attacker, it might be sufficient to exploit a single vulnerability in either the application or the system software to violate the application security. The problem is that the defenders must protect against the exploitation of all code vulnerabilities that might exist in the source code. A service provider might not have access to all source code of the system software that the cloud provider uses to operate the cloud. Even if the source code were available, this will typically be too large to be inspected.  To show that this is a difficult problem, let's look at the number of lines of source code of common system software components. While lines of source code is not an ideal  indicator for the number of vulnerabilities, it gives some indication of the problem we are facing. Some security researchers state that given the current state of the art, only code with up to 10,000s of lines of code can be reasonably inspected. Just the system software itself contains millions of lines of code. This is orders of magnitudes more than we can reasonably expect to be able to inspect.  SCONE runs on top of Linux - which contains millions of lines of code and is still growing in size with each release:   Linux Lines of Code (StefanPohl, CC0,  original }  OpenStack is a popular open source software to manage clouds. OpenStack - despite being relatively young - has been growing dramatically over the years that it has already reached 5 million lines of code (including comments and blank lines):   OpenStack Lines of Code (OpenHub  original )  To manage containers, we need an engine like Docker. Docker is younger than OpenStack but has nevertheless reached already more than 180,000 lines of code:   Docker Lines of Code (OpenHub  original )  Code complexity .There is no one-to-one correlation between lines of codes and bugs. Static analysis of open source code repositories indicates approximately 0.61 defects per 1,000 LOC. A recent analysis of Linux shows that, despite an increasing number of defects being fixed, there are always approximately 5,000 defects waiting to be fixed. Not all of these defects can, however, be exploited for security attacks. Another analysis found that approximately 500 security-relevant bugs were fixed in Linux over the past five years - bugs that had been in the kernel for five years before being discovered and fixed. Commercial code had a slightly higher defect density than open source projects. Hence, we need to expect vulnerabilities in commercial software too.", 
            "title": "Problem: Defender's Dilemma"
        }, 
        {
            "location": "/background/#scone-approach", 
            "text": "The approach of SCONE is to partition the code and to place essential components of an application into separate enclaves. Practically, it is quite difficult to split an existing code base of a single process into one component that runs inside an enclave and a component that runs outside of an enclave. However, many modern applications - like cloud-native applications - are already partitioned in several components running in separate address spaces. These components are typically called microservices. This partitioning facilitates a more intelligent scaling of services as well as a scaling of the development team.  A large application might consist of a variety of microservices. Not all microservices of an application need to run inside enclaves to protect the application\u2019s integrity and confidentiality. For example, some services might only process encrypted data, like encrypted log data, and do not need to run inside enclaves.  Also, the resource manager does not need to run in an enclave either. However, we recommend that each microservice that has the credential to send requests to at least one microservice running inside an enclave, should itself also run inside of an enclave to restrict the access to enclaved microservices.  Current SGX-capable CPUs have a limited EPC (Extended Page Cache) size. If the working set of a microservice does not fit inside the EPC, overheads can become high. The usage of microservices supports horizontal scalability. This helps to cope with limited EPC (extended page cache) by spreading secure microservices across different hosts.    scontain.com , March 2018.  Questions or Suggestions?", 
            "title": "SCONE Approach"
        }, 
        {
            "location": "/SCONE_CLI/", 
            "text": "SCONE CLI\n\n\nWe maintain a single unified command line interface (CLI) \nscone\n that helps to to start and stop secure containers as well as secure applications. \nscone\n also provides functionality to install and monitor SCONE hosts.\n\n\nThe \nscone\n command is structured similar as the \ndocker\n CLI or the \ninfinit\n CLI:\n\n\n\n\nOne needs to specify an \nobject\n (like \nhost\n) and a \ncommand\n (like \ninstall\n) and some options. For some commands, some of the options are actually not optional but mandatory.\n\n\nSee below how to install scone on Ubuntu. Instead of installing \nscone\n in a VM or a host, you could just start it in a container. Assuming\nthat you have docker installed, you try the following examples by running the following container:\n\n\n docker run -it  sconecuratedimages/sconecli\n\n\n\n\nTypically, you will need some ssh credentials inside this container. Read section \nssh setup\n to learn how to use import ssh credentials inside of a container.\n\n\nNote\n\n\nThe \nscone CLI\n uses \nssh\n to log into remote hosts. We assume that \nssh\n is setup in such a way that you do not need passwords to log into these hosts.  Please read section \nssh setup\n to learn how to ensure this.\n\n\nHelp\n\n\nscone\n has a built in help. To get a list of all \nobjects\n, just type:\n\n\n$ scone --help\n\n\n\n\nTo get a list of all \ncommands\n for a given \nobject\n (like host), execute:\n\n\n$ scone host --help\n\n\n\n\nTo get a list of all options for a given \nobject\n and \ncommand\n (e.g., host install) and some examples, just execute:\n\n\n$ scone host install --help\n\n\n\n\nbash auto-completion\n\n\nIf you are using \nbash\n as your shell, \nscone\n supports auto-completion. This means that instead you can use the \nTAB\n key to see the options. For example,\n\n\n$ scone \nTAB\n\n\n\n\n\nwill show all available objects. If you have already specified an object, auto-completion helps you to list all commands:\n\n\n$ scone host \nTAB\n\n\n\n\n\nIf you also specified an command, it will provide you with a list of options (that you have not specified yet):\n\n\n$ scone host install \nTAB\n\n\n\n\n\nOf course, it also supports auto-completion:\n\n\n$ scone host install -n\nTAB\n\n\n\n\n\nwill result in \n\n\n$ scone host install -name\n\n\n\n\nInstallation of scone\n\n\nOn Ubuntu platform, you can just execute\n\n\n curl -L https://sconecontainers.github.io/install.sh \n|\n bash\n\n\n\n\nYou could alternatively first download the above installation script and store it as file \ninstall.sh\n, inspect it and then run it \n./install.sh\"\n.\n\n\nThe \nscone\n command is located in directory \n/opt/scone/bin/\n. You might want this directory to add this to you \nPATH\n\n\n \nPATH\n=\n/opt/scone/bin/:\n$PATH\n\n\n\n\n\nFor convenience, you might want to add above statement to your \n.bashrc\n script (- in case you are using bash).\n\n\nManual installation of SCONE Deb Package\n\n\nYou can execute the following commands (these are the same as in the above installation script):\n\n\nKEYNAME\n=\n96B9BADB\n\n\nREPO\n=\ndeb https://sconecontainers.github.io/SCONE ./\n\n\nsudo apt-get update\nsudo apt-get install -y linux-image-extra-\n$(\nuname -r\n)\n linux-image-extra-virtual\n\nsudo sudo apt-get install -y apt-transport-https ca-certificates\nsudo apt-key adv \n\\\n\n  --keyserver hkp://ha.pool.sks-keyservers.net:80 \n\\\n\n  --recv-keys \n$KEYNAME\n\n\n\necho\n \n$REPO\n \n|\n sudo tee /etc/apt/sources.list.d/scone.list\n\nsudo apt-get clean\nsudo apt-get update\n\napt-cache policy scone\n\nsudo apt-get install -y scone\n\n\n\n\nThe \nscone\n utility will be installed at \n/opt/scone/bin\n. Hence, it makes sense to add this path to your \nPATH\n:\n\n\n \nexport\n \nPATH\n=\n/opt/scone/bin:\n$PATH\n\n\n\n\n\nYou can then execute some scone commands to see if the installation was successful:\n\n\n scone --version\n\n scone --help\n\n scone volume --help\n\n\n\n\nScreencast\n\n\n\n\n \nscontain.com\n, November 2017. \nQuestions or Suggestions?", 
            "title": "SCONE CLI"
        }, 
        {
            "location": "/SCONE_CLI/#scone-cli", 
            "text": "We maintain a single unified command line interface (CLI)  scone  that helps to to start and stop secure containers as well as secure applications.  scone  also provides functionality to install and monitor SCONE hosts.  The  scone  command is structured similar as the  docker  CLI or the  infinit  CLI:   One needs to specify an  object  (like  host ) and a  command  (like  install ) and some options. For some commands, some of the options are actually not optional but mandatory.  See below how to install scone on Ubuntu. Instead of installing  scone  in a VM or a host, you could just start it in a container. Assuming\nthat you have docker installed, you try the following examples by running the following container:   docker run -it  sconecuratedimages/sconecli  Typically, you will need some ssh credentials inside this container. Read section  ssh setup  to learn how to use import ssh credentials inside of a container.", 
            "title": "SCONE CLI"
        }, 
        {
            "location": "/SCONE_CLI/#note", 
            "text": "The  scone CLI  uses  ssh  to log into remote hosts. We assume that  ssh  is setup in such a way that you do not need passwords to log into these hosts.  Please read section  ssh setup  to learn how to ensure this.", 
            "title": "Note"
        }, 
        {
            "location": "/SCONE_CLI/#help", 
            "text": "scone  has a built in help. To get a list of all  objects , just type:  $ scone --help  To get a list of all  commands  for a given  object  (like host), execute:  $ scone host --help  To get a list of all options for a given  object  and  command  (e.g., host install) and some examples, just execute:  $ scone host install --help", 
            "title": "Help"
        }, 
        {
            "location": "/SCONE_CLI/#bash-auto-completion", 
            "text": "If you are using  bash  as your shell,  scone  supports auto-completion. This means that instead you can use the  TAB  key to see the options. For example,  $ scone  TAB   will show all available objects. If you have already specified an object, auto-completion helps you to list all commands:  $ scone host  TAB   If you also specified an command, it will provide you with a list of options (that you have not specified yet):  $ scone host install  TAB   Of course, it also supports auto-completion:  $ scone host install -n TAB   will result in   $ scone host install -name", 
            "title": "bash auto-completion"
        }, 
        {
            "location": "/SCONE_CLI/#installation-of-scone", 
            "text": "On Ubuntu platform, you can just execute   curl -L https://sconecontainers.github.io/install.sh  |  bash  You could alternatively first download the above installation script and store it as file  install.sh , inspect it and then run it  ./install.sh\" .  The  scone  command is located in directory  /opt/scone/bin/ . You might want this directory to add this to you  PATH    PATH = /opt/scone/bin/: $PATH   For convenience, you might want to add above statement to your  .bashrc  script (- in case you are using bash).", 
            "title": "Installation of scone"
        }, 
        {
            "location": "/SCONE_CLI/#manual-installation-of-scone-deb-package", 
            "text": "You can execute the following commands (these are the same as in the above installation script):  KEYNAME = 96B9BADB  REPO = deb https://sconecontainers.github.io/SCONE ./ \n\nsudo apt-get update\nsudo apt-get install -y linux-image-extra- $( uname -r )  linux-image-extra-virtual\n\nsudo sudo apt-get install -y apt-transport-https ca-certificates\nsudo apt-key adv  \\ \n  --keyserver hkp://ha.pool.sks-keyservers.net:80  \\ \n  --recv-keys  $KEYNAME  echo   $REPO   |  sudo tee /etc/apt/sources.list.d/scone.list\n\nsudo apt-get clean\nsudo apt-get update\n\napt-cache policy scone\n\nsudo apt-get install -y scone  The  scone  utility will be installed at  /opt/scone/bin . Hence, it makes sense to add this path to your  PATH :    export   PATH = /opt/scone/bin: $PATH   You can then execute some scone commands to see if the installation was successful:   scone --version  scone --help  scone volume --help", 
            "title": "Manual installation of SCONE Deb Package"
        }, 
        {
            "location": "/SCONE_CLI/#screencast", 
            "text": "scontain.com , November 2017.  Questions or Suggestions?", 
            "title": "Screencast"
        }, 
        {
            "location": "/SCONE_HOST_SETUP/", 
            "text": "SCONE: Host Installation Guide\n\n\nThis page describes how\n\n\n\n\n\n\nto set up a host such that it can run SCONE secure containers, i.e., containers in which processes run inside of SGX enclaves, and\n\n\n\n\n\n\nwe remind you of how to set up your \nssh\n configuration to be able to use your scone CLI.\n\n\n\n\n\n\nDuring this setup, we\n\n\n\n\n\n\ninstall a \npatched\n Intel SGX driver - required for better monitoring support,\n\n\n\n\n\n\ninstall a \npatched\n docker engine - to ensure that all containers have access to SGX, and\n\n\n\n\n\n\nstart or join a docker swarm - if requested by command line options.\n\n\n\n\n\n\n\n\nPatched Driver and Docker Engine\n\n\nSince Docker added some previously missing features, in the next release of SCONE will install an unpatched Docker and SGX driver.\n\n\n\n\nPrerequisite\n:\n\n\n\n\n\n\nWe assume that you have set up the \nscone\n \ncommand line interface\n.\nThe \nscone\n CLI can run on your developer machine, a virtual machine or inside a container.\nThe easiest way to get started is to run \nscone\n in a Docker container.\nNo matter where \nscone\n is running, it requires that \nssh\n be properly installed.\n\n\n\n\n\n\nWe recommend to install Ubuntu 16.04 LTS, Ubuntu 17.10 or Ubuntu 18.04 LTS (as soon as it becomes available) on the Swarm machines. For older versions of Ubuntu, some minor manual fixing might be needed during installation of the SGX drivers and the Docker engine. The screencast below shows some of the issues one faces on older Ubuntu versions. The secure containers are mostly based on Ubuntu or Alpine Linux (smaller image size).\n\n\n\n\n\n\n\n\nHost Setup\n\n\nThis host setup will enable the execution of \nSCONE secure containers\n on SGX-enabled machines. These containers can run one or more \nsecure programs\n, i.e., programs that are executed inside SGX enclaves.   The secure programs of secure containers can be statically-linked or dynamically-linked (see \nSCONE SGX Toolchain\n). The host itself runs statically-linked secure programs only - this is to avoid failures do to library versioning issues.\n\n\n\n\nThe \nscone CLI\n uses *ssh\n to log into remote hosts. We assume that \nssh\n is setup in such a way that you do not need passwords to log into these hosts.  Please read section \nssh setup\n to learn how to ensure this.\n\n\nInstallation of a single host\n\n\nAfter you set up the \nscone CLI\n and your passwordless ssh works, you can install the scone related software on a new host, say, \nalice\n as follows:\n\n\n$ scone host install --name alice\n\n\n\n\nTo verify that a host is properly installed for SCONE and contains the newest patched Docker engine and SGX driver, just execute:\n\n\n$ scone host check --name alice\n\n\n\n\nThis command will issue an error unless the newest versions of the patched Docker engine and the patched SGX driver is installed.\n\n\nInstallation of a swarm\n\n\nFor a set of hosts to form a (Docker) swarm, you need to decide which hosts should be managers and which should be just members of the swarm. Say, you decided that \nalice\n and \nbob\n should be managers but \ncaroline\n a non-manager, execute the following:\n\n\n$ scone host install --name alice --as-manager\n$ scone host install --name bob --as-manager --join alice\n$ scone host install --name caroline  --join alice\n\n\n\n\nNote that the hosts must be able to communicate with each other (i.e., not partitioned through firewalls). Docker recommends/expects  that they will be in the same local area network.\n\n\nChecking your Installation\n\n\nTo test the installation, one can run a simple hello-world container:\n\n\n sudo docker run hello-world\n\n\n\n\nBackground Information\n\n\nPatched Docker Engine (Moby)\n\n\nFor an container to be able to use SGX, it has to have access to a device (/dev/isgx). This device permits the container to talk to the SGX driver. This driver is needed, in particular, to create SGX enclaves. \n\n\nSome docker commands (like \ndocker run\n) support an option --device (i.e., \n--device /dev/isgx\n) which allows us to give a container access to the SGX device. We need to point out that some docker commands (like \ndocker build\n) do, however, not yet support the device option. Therefore, we maintain and install a slightly patched docker engine (i.e., a variant of moby): this engine ensures that each container has access to the SGX device (/dev/isgx).  With the help of this patched engine, we can use Dockerfiles to generate container images (see this \nTutorial\n).\n\n\nRight now we provide a patched version of the currently active branch of Moby (a.k.a., the Docker engine): 17.05.0-ce, build 89658be (November 11, 2017).\n\n\nPatched SGX Driver\n\n\nWe also maintain a patched version of the SGX driver. This version adds some additional monitoring like the number of \navailable\n and \nfree\n EPC (Extended Page Cache) pages.\n\n\nRight now, we provide a patched version of the latest Intel SGX driver (November 11, 2017).\n\n\nNote\n\n\nWe have been updating SCONE such that SCONE does not need access to \n/dev/isgx\n during cross-compilation - which could, for example, be executed during \ndocker build\n. Hence, we will soon switch to an unpatched Docker engine and SGX driver.\n\n\nScreencast\n\n\nThis screencast shows the installation of three machines SGX-capable hosts. In this screencast, we show the installation on machines that run older versions of Ubuntu (sgx2 = 14.04, sgx3 = 14.04 with custom kernel, and sgx4 = 16.04). In this case, we will see some warnings since \nscone host\n depends on \nsystemd\n to start the swarm. In case \nsystemd\n is not available, \nscone host\n will still be able to install the patched SGX driver and the patched Docker engine.\n\n\nAnother issue that one sometimes faces is that an older SGX drivers is already installed but cannot be offloaded and replaced by the patched driver by \nscone host\n. The reason for that is that typically that some process is still using the \n/dev/isgx\n device. This needs to be manually fixed by stopping the process or by rebooting the machine. Alternatively, one can use the existing SGX driver. However, the monitoring of the used EPC pages will not be provided in this case.\n\n\n\n\n \nscontain.com\n, March 2018. \nQuestions or Suggestions?", 
            "title": "SCONE Host Setup"
        }, 
        {
            "location": "/SCONE_HOST_SETUP/#scone-host-installation-guide", 
            "text": "This page describes how    to set up a host such that it can run SCONE secure containers, i.e., containers in which processes run inside of SGX enclaves, and    we remind you of how to set up your  ssh  configuration to be able to use your scone CLI.    During this setup, we    install a  patched  Intel SGX driver - required for better monitoring support,    install a  patched  docker engine - to ensure that all containers have access to SGX, and    start or join a docker swarm - if requested by command line options.     Patched Driver and Docker Engine  Since Docker added some previously missing features, in the next release of SCONE will install an unpatched Docker and SGX driver.   Prerequisite :    We assume that you have set up the  scone   command line interface .\nThe  scone  CLI can run on your developer machine, a virtual machine or inside a container.\nThe easiest way to get started is to run  scone  in a Docker container.\nNo matter where  scone  is running, it requires that  ssh  be properly installed.    We recommend to install Ubuntu 16.04 LTS, Ubuntu 17.10 or Ubuntu 18.04 LTS (as soon as it becomes available) on the Swarm machines. For older versions of Ubuntu, some minor manual fixing might be needed during installation of the SGX drivers and the Docker engine. The screencast below shows some of the issues one faces on older Ubuntu versions. The secure containers are mostly based on Ubuntu or Alpine Linux (smaller image size).     Host Setup  This host setup will enable the execution of  SCONE secure containers  on SGX-enabled machines. These containers can run one or more  secure programs , i.e., programs that are executed inside SGX enclaves.   The secure programs of secure containers can be statically-linked or dynamically-linked (see  SCONE SGX Toolchain ). The host itself runs statically-linked secure programs only - this is to avoid failures do to library versioning issues.   The  scone CLI  uses *ssh  to log into remote hosts. We assume that  ssh  is setup in such a way that you do not need passwords to log into these hosts.  Please read section  ssh setup  to learn how to ensure this.", 
            "title": "SCONE: Host Installation Guide"
        }, 
        {
            "location": "/SCONE_HOST_SETUP/#installation-of-a-single-host", 
            "text": "After you set up the  scone CLI  and your passwordless ssh works, you can install the scone related software on a new host, say,  alice  as follows:  $ scone host install --name alice  To verify that a host is properly installed for SCONE and contains the newest patched Docker engine and SGX driver, just execute:  $ scone host check --name alice  This command will issue an error unless the newest versions of the patched Docker engine and the patched SGX driver is installed.", 
            "title": "Installation of a single host"
        }, 
        {
            "location": "/SCONE_HOST_SETUP/#installation-of-a-swarm", 
            "text": "For a set of hosts to form a (Docker) swarm, you need to decide which hosts should be managers and which should be just members of the swarm. Say, you decided that  alice  and  bob  should be managers but  caroline  a non-manager, execute the following:  $ scone host install --name alice --as-manager\n$ scone host install --name bob --as-manager --join alice\n$ scone host install --name caroline  --join alice  Note that the hosts must be able to communicate with each other (i.e., not partitioned through firewalls). Docker recommends/expects  that they will be in the same local area network.", 
            "title": "Installation of a swarm"
        }, 
        {
            "location": "/SCONE_HOST_SETUP/#checking-your-installation", 
            "text": "To test the installation, one can run a simple hello-world container:   sudo docker run hello-world", 
            "title": "Checking your Installation"
        }, 
        {
            "location": "/SCONE_HOST_SETUP/#background-information", 
            "text": "", 
            "title": "Background Information"
        }, 
        {
            "location": "/SCONE_HOST_SETUP/#patched-docker-engine-moby", 
            "text": "For an container to be able to use SGX, it has to have access to a device (/dev/isgx). This device permits the container to talk to the SGX driver. This driver is needed, in particular, to create SGX enclaves.   Some docker commands (like  docker run ) support an option --device (i.e.,  --device /dev/isgx ) which allows us to give a container access to the SGX device. We need to point out that some docker commands (like  docker build ) do, however, not yet support the device option. Therefore, we maintain and install a slightly patched docker engine (i.e., a variant of moby): this engine ensures that each container has access to the SGX device (/dev/isgx).  With the help of this patched engine, we can use Dockerfiles to generate container images (see this  Tutorial ).  Right now we provide a patched version of the currently active branch of Moby (a.k.a., the Docker engine): 17.05.0-ce, build 89658be (November 11, 2017).", 
            "title": "Patched Docker Engine (Moby)"
        }, 
        {
            "location": "/SCONE_HOST_SETUP/#patched-sgx-driver", 
            "text": "We also maintain a patched version of the SGX driver. This version adds some additional monitoring like the number of  available  and  free  EPC (Extended Page Cache) pages.  Right now, we provide a patched version of the latest Intel SGX driver (November 11, 2017).", 
            "title": "Patched SGX Driver"
        }, 
        {
            "location": "/SCONE_HOST_SETUP/#note", 
            "text": "We have been updating SCONE such that SCONE does not need access to  /dev/isgx  during cross-compilation - which could, for example, be executed during  docker build . Hence, we will soon switch to an unpatched Docker engine and SGX driver.", 
            "title": "Note"
        }, 
        {
            "location": "/SCONE_HOST_SETUP/#screencast", 
            "text": "This screencast shows the installation of three machines SGX-capable hosts. In this screencast, we show the installation on machines that run older versions of Ubuntu (sgx2 = 14.04, sgx3 = 14.04 with custom kernel, and sgx4 = 16.04). In this case, we will see some warnings since  scone host  depends on  systemd  to start the swarm. In case  systemd  is not available,  scone host  will still be able to install the patched SGX driver and the patched Docker engine.  Another issue that one sometimes faces is that an older SGX drivers is already installed but cannot be offloaded and replaced by the patched driver by  scone host . The reason for that is that typically that some process is still using the  /dev/isgx  device. This needs to be manually fixed by stopping the process or by rebooting the machine. Alternatively, one can use the existing SGX driver. However, the monitoring of the used EPC pages will not be provided in this case.     scontain.com , March 2018.  Questions or Suggestions?", 
            "title": "Screencast"
        }, 
        {
            "location": "/SCONE_toolchain/", 
            "text": "SCONE SGX Toolchain\n\n\nSCONE comes with compiler support for popular languages: C, C++, GO, Rust as well as Fortran.\nThe objective of these (cross-) compilers are to compile applications - generally without source code changes - such that they can run inside of SGX enclaves.\n\n\nTo simplify the use of these (cross-) compilers, SCONE maintains curated container image that includes these cross-compilers.\n\n\nCompiler variants\n\n\nDepending on if you want to generate a \ndynamically-linked\n or a \nstatically-linked\n binary, you\ncan use a standard compiler (dynamic) or you need to use a cross compiler (static). The compiler can run on any system, i.e., does not require SGX to run. To generate a statically-linked binary, you need to run on a SGX-enabled machine (for now).\n\n\nNote\n Independently, if you use a dynamic or static linking, the hash of an enclave (MRENCLAVE) will encompass the whole code, i.e., includes all libraries. Any updates of a library on your host might prevent the execution of a SCONE binary because of a wrong MRENCLAVE. Hence, we recommend to use only statically-linked programs on the host. In containers, which have a more controlled environment, we support both statically as well as dynamically linked binaries. The main advantage of dynamic linking is that for many programs we do not change the build process when moved to SCONE.\n\n\nNote\n also that SCONE supports the loading of dynamic libraries after a program has already started inside of an enclave. This feature is required by modern languages like Java. Enabling general loading of dynamic library introduces the risk that one could load malicious code inside of an enclave. Hence, we switch this feature off by default. For debugging programs, you can enable this feature via an environment variable (\nexport SCONE_ALLOW_DLOPEN=2).\n For production enclaves, you will need to protect the integrity of the shared libraries with the help of the \nSCONE file shield\n.\n\n\nDynamically-Linked Binaries\n\n\nThe easiest to get started is to compile your programs such that\n\n\n\n\n\n\nthe generated code is position independent (\n-fPIC\n),\n\n\n\n\n\n\nthe thread local storage model is global-dynamic (\n-ftls-model=global-dynamic\n),\n\n\n\n\n\n\nyour binary is dynamically linked (i.e., do not use \n-static\n), and\n\n\n\n\n\n\nlink against musl as your libc (i.e., not glibc or any other libc).\n\n\n\n\n\n\n\n\nWhen a program is started, SCONE uses its own dynamic link loader to replace libc by a SCONE libc. The SCONE dynamic linker will load the program inside a new SGX enclave and SCONE libc will enable programs to run inside the SGX enclaves, e.g., execute system calls and protect them from attacks via shields like the \nfile system shield\n.\n\n\nTo simplify the compiling of your programs for scone, we make available a docker image \nsconecuratedimages/muslgcc\n which includes \ngcc\n and \ng++\n support. The options will by default be set as shown above. You need, however, to make sure that your Makefiles will not overwrite these options.\n\n\nStatically-Linked Binaries\n\n\nFor statically linked binaries, we make available a (private) docker image \nsconecuratedimages/crosscompilers:scone\n which can produce statically linked binaries. In the statically linked binaries, we replace the interface to the operating system (i.e., libc) by a variant that enables programs to run inside Intel SGX enclaves.\n\n\n\n\nNote that a statically linked binaries might look like a dynamically-linked binary. For example, if you look at a statically-linked program \nweb-srv-go\n, you will still see dynamic dependencies:\n\n\n$ ldd web-srv-go \n    linux-vdso.so.1 \n=\n  \n(\n0x00007ffe423fd000\n)\n\n    libpthread.so.0 \n=\n /lib/x86_64-linux-gnu/libpthread.so.0 \n(\n0x00007effa344f000\n)\n\n    libc.so.6 \n=\n /lib/x86_64-linux-gnu/libc.so.6 \n(\n0x00007effa3085000\n)\n\n    /lib64/ld-linux-x86-64.so.2 \n(\n0x00007effa366c000\n)\n\n\n\n\n\nThe reason for that is that the statically linked binary that runs inside of an enclave is wrapped in a dynamically linked \nloader\n program. The loader program creates the enclave, moves the program code inside the enclave and starts threads that will enter the enclave. The code that is moved inside the enclave is, however, statically linked.\n\n\nUsing the cross compiler container\n\n\nHow to use the compiler:\n\n\n\n\n\n\nuse this as a base image and build your programs inside of a container we a  \nDockerfile\n), or\n\n\n\n\n\n\nmap volumes such that the compiler can compile files living outside the container (see \nSCONE Tutorial\n).\n\n\n\n\n\n\nFor an example how to use the crosscompilers, see how to compile a programs written in \nGO\n.\n\n\nExample\n\n\nNote\n  on some systems you will need to run \ndocker\n with root permissions, i.e., in this case you should prefix a \n\n\n docker ...\n\n\n\n\ncommand with \nsudo\n, i.e., you execute \n\n\n sudo docker ...\n\n\n\n\nOne can run the above compiler inside of a container while the compiled files reside outside the container. Say, your code is\nin file \nmyapp.c\n in your current directory (\n$PWD\n). You can compile this code as follows:\n\n\n docker run --rm  -v \n$PWD\n:/usr/src/myapp -w /usr/src/myapp sconecuratedimages/muslgcc gcc myapp.c\n\n\n\n\nThis call will generate a binary \na.out\n in your working directory. This binary is dynamically linked against musl:\n\n\n ldd a.out \n    /lib/ld-musl-x86_64.so.1 \n(\n0x7fb0379f9000\n)\n\n    libc.musl-x86_64.so.1 \n=\n /lib/ld-musl-x86_64.so.1 \n(\n0x7fb0379f9000\n)\n\n\n\n\n\nThis binary can run natively only if you have musl installed at the correct position in your development machine (and your development machine runs Linux). Alternatively, you can run the binary in a container:\n\n\n docker run --rm  -v \n$PWD\n:/usr/src/myapp -w /usr/src/myapp sconecuratedimages/muslgcc ./a.out\n\n\n\n\nTo run this inside of SGX enclaves with SCONE, you need access to the SCONE runtime systems. For more details, see our \nhello world\n in Section \nSCONE Tutorial\n. This is not very convenient and hence, we provide a) a simpler version with the help of \nDockerfiles\n. \n\n\nIn most cases, you might just set to use one of our crosscompilers in your \nconfigure\n script or \nMakefile\n. A simple way is to use the Docker image \nsconecuratedimages/crosscompilers:scone\n as a base image and then clone your code inside the container and set one or more of our compilers (\nscone-gcc, scone-g++, scone-gccgo, scone-gfortran, and scone-rustc\n) to be used in your build. For Rust, we support also our variant of \ncargo\n which is \nscone-cargo\n.\n\n\nDebugger support\n\n\nWe also support \ngdb\n to debug applications running inside of enclaves. To get started, we recommend that you first ensure that your program runs natively linked against musl. Most programs will do - after all, the Alpine Linux distribution is completely based on musl. The debugger is available in image \nsconecuratedimages/crosscompilers:scone\n as \nscone-gdb\n.\n\n\nFor example on how to use the debugger, see how to debug a program written in  \nGO\n.\n\n\n \nscontain.com\n, November 2017. \nQuestions or Suggestions?", 
            "title": "SCONE SGX toolchain"
        }, 
        {
            "location": "/SCONE_toolchain/#scone-sgx-toolchain", 
            "text": "SCONE comes with compiler support for popular languages: C, C++, GO, Rust as well as Fortran.\nThe objective of these (cross-) compilers are to compile applications - generally without source code changes - such that they can run inside of SGX enclaves.  To simplify the use of these (cross-) compilers, SCONE maintains curated container image that includes these cross-compilers.", 
            "title": "SCONE SGX Toolchain"
        }, 
        {
            "location": "/SCONE_toolchain/#compiler-variants", 
            "text": "Depending on if you want to generate a  dynamically-linked  or a  statically-linked  binary, you\ncan use a standard compiler (dynamic) or you need to use a cross compiler (static). The compiler can run on any system, i.e., does not require SGX to run. To generate a statically-linked binary, you need to run on a SGX-enabled machine (for now).  Note  Independently, if you use a dynamic or static linking, the hash of an enclave (MRENCLAVE) will encompass the whole code, i.e., includes all libraries. Any updates of a library on your host might prevent the execution of a SCONE binary because of a wrong MRENCLAVE. Hence, we recommend to use only statically-linked programs on the host. In containers, which have a more controlled environment, we support both statically as well as dynamically linked binaries. The main advantage of dynamic linking is that for many programs we do not change the build process when moved to SCONE.  Note  also that SCONE supports the loading of dynamic libraries after a program has already started inside of an enclave. This feature is required by modern languages like Java. Enabling general loading of dynamic library introduces the risk that one could load malicious code inside of an enclave. Hence, we switch this feature off by default. For debugging programs, you can enable this feature via an environment variable ( export SCONE_ALLOW_DLOPEN=2).  For production enclaves, you will need to protect the integrity of the shared libraries with the help of the  SCONE file shield .", 
            "title": "Compiler variants"
        }, 
        {
            "location": "/SCONE_toolchain/#dynamically-linked-binaries", 
            "text": "The easiest to get started is to compile your programs such that    the generated code is position independent ( -fPIC ),    the thread local storage model is global-dynamic ( -ftls-model=global-dynamic ),    your binary is dynamically linked (i.e., do not use  -static ), and    link against musl as your libc (i.e., not glibc or any other libc).     When a program is started, SCONE uses its own dynamic link loader to replace libc by a SCONE libc. The SCONE dynamic linker will load the program inside a new SGX enclave and SCONE libc will enable programs to run inside the SGX enclaves, e.g., execute system calls and protect them from attacks via shields like the  file system shield .  To simplify the compiling of your programs for scone, we make available a docker image  sconecuratedimages/muslgcc  which includes  gcc  and  g++  support. The options will by default be set as shown above. You need, however, to make sure that your Makefiles will not overwrite these options.", 
            "title": "Dynamically-Linked Binaries"
        }, 
        {
            "location": "/SCONE_toolchain/#statically-linked-binaries", 
            "text": "For statically linked binaries, we make available a (private) docker image  sconecuratedimages/crosscompilers:scone  which can produce statically linked binaries. In the statically linked binaries, we replace the interface to the operating system (i.e., libc) by a variant that enables programs to run inside Intel SGX enclaves.   Note that a statically linked binaries might look like a dynamically-linked binary. For example, if you look at a statically-linked program  web-srv-go , you will still see dynamic dependencies:  $ ldd web-srv-go \n    linux-vdso.so.1  =    ( 0x00007ffe423fd000 ) \n    libpthread.so.0  =  /lib/x86_64-linux-gnu/libpthread.so.0  ( 0x00007effa344f000 ) \n    libc.so.6  =  /lib/x86_64-linux-gnu/libc.so.6  ( 0x00007effa3085000 ) \n    /lib64/ld-linux-x86-64.so.2  ( 0x00007effa366c000 )   The reason for that is that the statically linked binary that runs inside of an enclave is wrapped in a dynamically linked  loader  program. The loader program creates the enclave, moves the program code inside the enclave and starts threads that will enter the enclave. The code that is moved inside the enclave is, however, statically linked.", 
            "title": "Statically-Linked Binaries"
        }, 
        {
            "location": "/SCONE_toolchain/#using-the-cross-compiler-container", 
            "text": "How to use the compiler:    use this as a base image and build your programs inside of a container we a   Dockerfile ), or    map volumes such that the compiler can compile files living outside the container (see  SCONE Tutorial ).    For an example how to use the crosscompilers, see how to compile a programs written in  GO .", 
            "title": "Using the cross compiler container"
        }, 
        {
            "location": "/SCONE_toolchain/#example", 
            "text": "Note   on some systems you will need to run  docker  with root permissions, i.e., in this case you should prefix a    docker ...  command with  sudo , i.e., you execute    sudo docker ...  One can run the above compiler inside of a container while the compiled files reside outside the container. Say, your code is\nin file  myapp.c  in your current directory ( $PWD ). You can compile this code as follows:   docker run --rm  -v  $PWD :/usr/src/myapp -w /usr/src/myapp sconecuratedimages/muslgcc gcc myapp.c  This call will generate a binary  a.out  in your working directory. This binary is dynamically linked against musl:   ldd a.out \n    /lib/ld-musl-x86_64.so.1  ( 0x7fb0379f9000 ) \n    libc.musl-x86_64.so.1  =  /lib/ld-musl-x86_64.so.1  ( 0x7fb0379f9000 )   This binary can run natively only if you have musl installed at the correct position in your development machine (and your development machine runs Linux). Alternatively, you can run the binary in a container:   docker run --rm  -v  $PWD :/usr/src/myapp -w /usr/src/myapp sconecuratedimages/muslgcc ./a.out  To run this inside of SGX enclaves with SCONE, you need access to the SCONE runtime systems. For more details, see our  hello world  in Section  SCONE Tutorial . This is not very convenient and hence, we provide a) a simpler version with the help of  Dockerfiles .   In most cases, you might just set to use one of our crosscompilers in your  configure  script or  Makefile . A simple way is to use the Docker image  sconecuratedimages/crosscompilers:scone  as a base image and then clone your code inside the container and set one or more of our compilers ( scone-gcc, scone-g++, scone-gccgo, scone-gfortran, and scone-rustc ) to be used in your build. For Rust, we support also our variant of  cargo  which is  scone-cargo .", 
            "title": "Example"
        }, 
        {
            "location": "/SCONE_toolchain/#debugger-support", 
            "text": "We also support  gdb  to debug applications running inside of enclaves. To get started, we recommend that you first ensure that your program runs natively linked against musl. Most programs will do - after all, the Alpine Linux distribution is completely based on musl. The debugger is available in image  sconecuratedimages/crosscompilers:scone  as  scone-gdb .  For example on how to use the debugger, see how to debug a program written in   GO .    scontain.com , November 2017.  Questions or Suggestions?", 
            "title": "Debugger support"
        }, 
        {
            "location": "/SCONE_Curated_Images/", 
            "text": "SCONE Curated Images\n\n\nWe provide a set of curated SCONE container images on a (partially private) repositories on Docker hub:\n\n\nPrivate images:\n1\n\n\n\n\n\n\n\n\nImage Name\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nsconecuratedimages/crosscompilers:scone\n\n\na container image with all the SCONE crosscompilers.\n\n\n\n\n\n\nsconecuratedimages/crosscompilers:runtime\n\n\na container image that can run dynamically linked applications inside of an enclave.\n\n\n\n\n\n\nsconecuratedimages/crosscompilers:python27\n\n\na container image including a \npython interpreter\n running inside of an enclave.\n\n\n\n\n\n\nsconecuratedimages/crosscompilers:mongodb\n\n\nMongoDB container image.\n\n\n\n\n\n\nsconecuratedimages/crosscompilers:vault\n\n\nVault container image.\n\n\n\n\n\n\nsconecuratedimages/crosscompilers:Memcached\n\n\nMemcached container image.\n\n\n\n\n\n\nsconecuratedimages/apps:node-8.9.4\n\n\na container image for \nnode\n running inside an enclave.\n\n\n\n\n\n\nsconecuratedimages/apps:nginx-1.13-alpine\n\n\na container image for \nnginx\n running inside an enclave.\n\n\n\n\n\n\nsconecuratedimages/apps:openjdk8-alpine\n\n\na container image for \nJava applications\n running inside an enclave.\n\n\n\n\n\n\n\n\nLogin in\n\n\nAccess to some SCONE images is restricted. First, create a new\ndocker hub ID (- in case you do not yet have one).\nSecond, get access\nto the private images for evaluation by sending email to \nscontain.com\n\nwith your  docker hub id and short statement what images you want to evaluate and what you plan to do with the images.\nSecond, log into to docker hub via:\n\n\n docker login\n\n\n\n\nbefore you will be able to pull any of the private curated images.\n\n\nScone Compilers\n\n\nTo run a local copy of the SCONE (cross-)compilers, just pull the appropriate image on your computer.\n\n\nDynamically-Linked Binaries\n\n\nEven if you have no SGX CPU extension / no SGX driver installed on your computer,\nyou can use a standard gcc compiler - as long as the requirements mentioned in \nSGX ToolChain\n are satisfied.\n\n\n docker pull sconecuratedimages/muslgcc\n\n\n\n\nNote that the binaries generated with the above image are just native binaries, i.e., they run \noutside of enclaves\n. To be able to run the binary inside of an enclave, you need to have installed the SCONE runtime library.\n\n\nTo run a dynamically-linked binary, one needs a special runtime environment. We provide this in form of a (private) container image:\n\n\n docker pull sconecuratedimages/crosscompilers:runtime\n\n\n\n\nStatically-Linked Binaries\n\n\nTo generate statically-linked secure binaries you need a cross compiler. You can pull\nthis image from Docker hub (you need to be granted access rights for that):\n\n\n docker pull sconecuratedimages/crosscompilers:scone\n\n\n\n\nScone Hello World\n\n\nYou can pull the following (private) image:\n\n\n docker pull sconecuratedimages/helloworld\n\n\n\n\nIf you installed the patched Docker engine (see \nSCONE Host Setup\n), run the helloworld program inside of an enclave via\n\n\n docker run sconecuratedimages/helloworld\nHello World\n\n\n\n\nThis command will fail in case you have the standard Docker engine installed:\n\n\n docker run sconecuratedimages/helloworld\nerror opening sgx device: No such file or directory\n\n\n\n\nYou can run on the standard Docker engine - if you have the SGX driver installed:\n\n\n docker run --device\n=\n/dev/isgx sconecuratedimages/helloworld\nHello World\n\n\n\n\nIf you do not have the SGX driver installed, you get an error message:\n\n\n docker run --device\n=\n/dev/isgx sconecuratedimages/helloworld\ndocker: Error response from daemon: linux runtime spec devices: error gathering device information \nwhile\n adding custom device \n/dev/isgx\n: no such file or directory.\n\n\n\n\nIn this case, install the SGX driver and the patched Docker engine as described in\n\nSCONE Host Setup\n. This installation will fail in case you disabled SGX in the BIOS or your CPU is not SGX-enabled.\n\n\nScreencast\n\n\n\n\n \nscontain.com\n, November 2017. \nQuestions or Suggestions?\n\n\n\n\n\n\n\n\n\n\nsend email to \n to get access.", 
            "title": "SCONE Curated Images"
        }, 
        {
            "location": "/SCONE_Curated_Images/#scone-curated-images", 
            "text": "We provide a set of curated SCONE container images on a (partially private) repositories on Docker hub:  Private images: 1     Image Name  Description      sconecuratedimages/crosscompilers:scone  a container image with all the SCONE crosscompilers.    sconecuratedimages/crosscompilers:runtime  a container image that can run dynamically linked applications inside of an enclave.    sconecuratedimages/crosscompilers:python27  a container image including a  python interpreter  running inside of an enclave.    sconecuratedimages/crosscompilers:mongodb  MongoDB container image.    sconecuratedimages/crosscompilers:vault  Vault container image.    sconecuratedimages/crosscompilers:Memcached  Memcached container image.    sconecuratedimages/apps:node-8.9.4  a container image for  node  running inside an enclave.    sconecuratedimages/apps:nginx-1.13-alpine  a container image for  nginx  running inside an enclave.    sconecuratedimages/apps:openjdk8-alpine  a container image for  Java applications  running inside an enclave.", 
            "title": "SCONE Curated Images"
        }, 
        {
            "location": "/SCONE_Curated_Images/#login-in", 
            "text": "Access to some SCONE images is restricted. First, create a new\ndocker hub ID (- in case you do not yet have one).\nSecond, get access\nto the private images for evaluation by sending email to  scontain.com \nwith your  docker hub id and short statement what images you want to evaluate and what you plan to do with the images.\nSecond, log into to docker hub via:   docker login  before you will be able to pull any of the private curated images.", 
            "title": "Login in"
        }, 
        {
            "location": "/SCONE_Curated_Images/#scone-compilers", 
            "text": "To run a local copy of the SCONE (cross-)compilers, just pull the appropriate image on your computer.", 
            "title": "Scone Compilers"
        }, 
        {
            "location": "/SCONE_Curated_Images/#dynamically-linked-binaries", 
            "text": "Even if you have no SGX CPU extension / no SGX driver installed on your computer,\nyou can use a standard gcc compiler - as long as the requirements mentioned in  SGX ToolChain  are satisfied.   docker pull sconecuratedimages/muslgcc  Note that the binaries generated with the above image are just native binaries, i.e., they run  outside of enclaves . To be able to run the binary inside of an enclave, you need to have installed the SCONE runtime library.  To run a dynamically-linked binary, one needs a special runtime environment. We provide this in form of a (private) container image:   docker pull sconecuratedimages/crosscompilers:runtime", 
            "title": "Dynamically-Linked Binaries"
        }, 
        {
            "location": "/SCONE_Curated_Images/#statically-linked-binaries", 
            "text": "To generate statically-linked secure binaries you need a cross compiler. You can pull\nthis image from Docker hub (you need to be granted access rights for that):   docker pull sconecuratedimages/crosscompilers:scone", 
            "title": "Statically-Linked Binaries"
        }, 
        {
            "location": "/SCONE_Curated_Images/#scone-hello-world", 
            "text": "You can pull the following (private) image:   docker pull sconecuratedimages/helloworld  If you installed the patched Docker engine (see  SCONE Host Setup ), run the helloworld program inside of an enclave via   docker run sconecuratedimages/helloworld\nHello World  This command will fail in case you have the standard Docker engine installed:   docker run sconecuratedimages/helloworld\nerror opening sgx device: No such file or directory  You can run on the standard Docker engine - if you have the SGX driver installed:   docker run --device = /dev/isgx sconecuratedimages/helloworld\nHello World  If you do not have the SGX driver installed, you get an error message:   docker run --device = /dev/isgx sconecuratedimages/helloworld\ndocker: Error response from daemon: linux runtime spec devices: error gathering device information  while  adding custom device  /dev/isgx : no such file or directory.  In this case, install the SGX driver and the patched Docker engine as described in SCONE Host Setup . This installation will fail in case you disabled SGX in the BIOS or your CPU is not SGX-enabled.", 
            "title": "Scone Hello World"
        }, 
        {
            "location": "/SCONE_Curated_Images/#screencast", 
            "text": "scontain.com , November 2017.  Questions or Suggestions?      send email to   to get access.", 
            "title": "Screencast"
        }, 
        {
            "location": "/SCONE_TUTORIAL/", 
            "text": "SCONE Tutorial\n\n\nPrerequisites\n\n\nEnsure that the SGX driver is installed\n\n\nCheck on the host as well as inside your containers that the SGX device \n/dev/isgx\n is visible:\n\n\n ls /dev/isgx \n/dev/isgx\n\n\n\n\nIf the driver is not installed, read Section \nSCONE Host Setup\n to learn how to install the SGX driver.\n\n\nChecking availability of SGX device inside of containers\n\n\nSometimes, Docker does not automatically map the SGX device inside of containers. We provide a patched Docker engine and a patched SGX driver that together permit to automatically open the sgx device inside of containers.\n\n\nIf your node is part of a Docker swarm, we provide a simple check as part of the \nscone CLI\n. Say, the leader node of your swarm  is called \nbeatrix\n, then you could just execute:\n\n\n$ scone swarm check --verbose --manager beatrix\n\n\n\n\nIn case your node is not part of a Docker swarm, you can run the checks manually. For that, we provide a container image\nthat helps to check if the SGX device can be accessed from inside a container. Just execute the following commands to check if your containers have access to the SGX device:\n\n\n# preferred alternative: required for swarms to work: SGX device is available in all containers by default\n\n\n sudo docker run --rm sconecuratedimages/checksgx \n||\n \necho\n \nSGX device is not automatically mapped inside of container\n\n\n# alternative: use --device option without --privileged flag\n\n\n sudo docker run --device\n=\n/dev/isgx --rm sconecuratedimages/checksgx \n||\n \necho\n \n--device=/dev/isgx: not sufficient to access SGX device inside of container\n\n\n# last alternative: use --device option without --privileged flag\n\n\n sudo docker run -v /dev/isgx:/dev/isgx --privileged  --rm sconecuratedimages/checksgx \n||\n \necho\n \nSGX device NOT available inside of container\n\n\n\n\n\nUse the first alternative that works in your installation to give containers access to the SGX device.\n\n\nInstall sgxmusl cross compiler image\n\n\nEnsure that you installed the various sconecuratedimages/crosscompilers container image:\n\n\n docker image ls sconecuratedimages/*\nREPOSITORY                          TAG                    IMAGE ID            CREATED             SIZE\nsconecuratedimages/crosscompilers   latest                 dff7975b7f32        \n7\n hours ago         \n1\n.57GB\nsconecuratedimages/crosscompilers   scone                  dff7975b7f32        \n7\n hours ago         \n1\n.57GB\n\n\n\n\nIf the cross compiler image is not yet installed, read Section \nSCONE Curated Container Images\n to learn how to install the SCONE cross compiler image.\n\n\nIf the docker command fails, please ensure that docker is indeed installed (see \nSCONE Host Setup\n. Also, on some systems you might need to use \nsodo\n to run docker commands.\n\n\nInstall the tutorial\n\n\nClone the tutorial:\n\n\n git clone https://github.com/christoffetzer/SCONE_TUTORIAL.git\n\n\n\n\nNative Hello World\n\n\nEnsure that \nhello world\n runs natively on your machine:\n\n\n \ncd\n SCONE_TUTORIAL/HelloWorld/\n\n gcc hello_world.c  -o native_hello_world\n\n ./native_hello_world\nHello World\n\n\n\n\nNote that the generated executable, i.e., \nsim_hello_world\n, will only run on Linux.\n\n\nStatically-Linked Hello World\n\n\nThe default cross compiler variant that runs \nhello world\n inside of an enclave is \nscone gcc\n and you can find this in container \nsconecuratedimages/crosscompilers:scone\n.\nThis variant requires access to the SGX device. \nIn Linux, the SGX device is made available as \n/dev/isgx\n and we can give the cross compiler inside of an container access via option \n--device=/dev/isgx\n:\n\n\n docker run --rm --device\n=\n/dev/isgx -v \n$PWD\n:/usr/src/myapp -w /usr/src/myapp sconecuratedimages/crosscompilers:scone scone-gcc hello_world.c  -o sgx_hello_world\n\n\n\n\nThis generates a statically linked binary. However, as we mentioned above, the binary looks like a dynamically\nlinked binary since it is wrapped in a dynamically linked loader program:\n\n\n ldd ./sgx_hello_world \n    linux-vdso.so.1 \n=\n  \n(\n0x00007ffcf73ad000\n)\n\n    libpthread.so.0 \n=\n /lib/x86_64-linux-gnu/libpthread.so.0 \n(\n0x00007f7c2a0e9000\n)\n\n    libc.so.6 \n=\n /lib/x86_64-linux-gnu/libc.so.6 \n(\n0x00007f7c29d1f000\n)\n\n    /lib64/ld-linux-x86-64.so.2 \n(\n0x00007f7c2a306000\n)\n\n\n\n\n\nEnsure that file \n/etc/sgx-musl.conf\n exists. If not, store some default file like:\n\n\n \nprintf\n \nQ 1\\ne 0 0 0\\ns 1 0 0\\n\n \n|\n sudo tee /etc/sgx-musl.conf\n\n\n\n\nTo run \nsgx_hello_world\n, in an enclave, just execute:\n\n\n ./sgx_hello_world\nHello World\n\n\n\n\nTo see some more debug messages, set environment variable \nSCONE_VERSION=1\n:\n\n\n \nSCONE_VERSION\n=\n1\n ./sgx_hello_world\n\nexport\n \nSCONE_QUEUES\n=\n4\n\n\nexport\n \nSCONE_SLOTS\n=\n256\n\n\nexport\n \nSCONE_SIGPIPE\n=\n0\n\n\nexport\n \nSCONE_MMAP32BIT\n=\n0\n\n\nexport\n \nSCONE_SSPINS\n=\n100\n\n\nexport\n \nSCONE_SSLEEP\n=\n4000\n\n\nexport\n \nSCONE_KERNEL\n=\n0\n\n\nexport\n \nSCONE_HEAP\n=\n67108864\n\n\nexport\n \nSCONE_CONFIG\n=\n/etc/sgx-musl.conf\n\nexport\n \nSCONE_MODE\n=\nhw\n\nexport\n \nSCONE_SGXBOUNDS\n=\nno\n\nexport\n \nSCONE_ALLOW_DLOPEN\n=\nno\nRevision: 9b355b99170ad434010353bb9f4dca24e532b1b7\nBranch: master\nConfigure options: --enable-file-prot --enable-shared --enable-debug --prefix\n=\n/scone/src/built/cross-compiler/x86_64-linux-musl\n\nHello World\n\n\n\n\nThe debug outputs \nSCONE_MODE=hw\n shows that \nsgx_hello_world\n runs in hardware mode, i.e., inside an SGX enclave.\n\n\nNote.\n The compilation as well as the hello world program will fail in case you do not have an SGX driver installed.\n\n\nDynamically-Linked Hello World\n\n\n docker run --rm  -v \n$PWD\n:/usr/src/myapp -w /usr/src/myapp sconecuratedimages/muslgcc gcc  hello_world.c -o dyn_hello_world\n\n\n\n\nTo run this natively, just execute the following:\n\n\n docker run --rm  -v \n$PWD\n:/usr/src/myapp -w /usr/src/myapp sconecuratedimages/muslgcc ./dyn_hello_world\n\n\n\n\nTo run a dynamically-linked binary in an enclave, you need to run this in a special runtime environment. In this environment you can ask binaries to run inside of enclaves by setting environment \nSCONE_ALPINE=1\n. To indicate that we are indeed running inside an enclave, we ask to issue some debug messages from inside the enclave by setting environment variable \nSCONE_VERSION=1\n:\n\n\nHardware Mode vs Simulation Mode\n\n\nFor debugging, we support three different modes for execution: \nhardware, simulation, and automatic\n:\n\n\n\n\n\n\nhardware\n: by setting environment variable to \nSCONE_MODE=HW\n, SCONE will enforce running this application inside an SGX enclave.\n\n\n\n\n\n\nsimulation\n: by setting environment variable to \nSCONE_MODE=SIM\n, SCONE will enforce running this application in native mode (i.e., outside of an enclave). This will run all SCONE functionality but outside enclaves. This is intended for development and debugging on machines that are not SGX-capable.\n\n\n\n\n\n\nautomatic\n: by setting environment variable to \nSCONE_MODE=AUTO\n, SCONE will run the application inside of an SGX enclave if available and otherwise in simulation mode. (This is the default mode)\n\n\n\n\n\n\nNOTE\n: In production mode, you must only permit running in hardware mode. Scone ensures this with the help of \nremote attestation\n: the SCONE configuration and attestation service (CAS) will only provide configuration information and secrets to an application only after it has proven (with the help of SGX CPU extensions) that it is indeed running inside an SGX enclave.\n\n\nExecution on a SGX-capable machine\n\n\n docker run --rm  -v \n$PWD\n:/usr/src/myapp -e \nSCONE_MODE\n=\nHW -e \nSCONE_ALPINE\n=\n1\n -e \nSCONE_VERSION\n=\n1\n sconecuratedimages/crosscompilers:runtime /usr/src/myapp/dyn_hello_world\n\nexport\n \nSCONE_QUEUES\n=\n4\n\n\nexport\n \nSCONE_SLOTS\n=\n256\n\n\nexport\n \nSCONE_SIGPIPE\n=\n0\n\n\nexport\n \nSCONE_MMAP32BIT\n=\n0\n\n\nexport\n \nSCONE_SSPINS\n=\n100\n\n\nexport\n \nSCONE_SSLEEP\n=\n4000\n\n\nexport\n \nSCONE_KERNEL\n=\n0\n\n\nexport\n \nSCONE_HEAP\n=\n67108864\n\n\nexport\n \nSCONE_CONFIG\n=\n/etc/sgx-musl.conf\n\nexport\n \nSCONE_MODE\n=\nhw\nConfigure parameters: \n\n1\n.1.15\nHello World\n\n\n\n\nExecution on a non-SGX machine\n\n\nIf you run this inside a container without access to SGX (/dev/isgx), for example, when running on a Mac, you will see the following error message:\n\n\n docker run --rm  -v \n$PWD\n:/usr/src/myapp -e \nSCONE_MODE\n=\nHW -e \nSCONE_ALPINE\n=\n1\n -e \nSCONE_VERSION\n=\n1\n sconecuratedimages/crosscompilers:runtime /usr/src/myapp/dyn_hello_world\n\n[\nError\n]\n Could not create enclave: Error opening SGX device\n\n\n\n\nYou could run this in simulation mode as follows:\n\n\n docker run --rm  -v \n$PWD\n:/usr/src/myapp -e \nSCONE_MODE\n=\nSIM -e \nSCONE_ALPINE\n=\n1\n -e \nSCONE_VERSION\n=\n1\n sconecuratedimages/crosscompilers:runtime /usr/src/myapp/dyn_hello_world\n\nexport\n \nSCONE_QUEUES\n=\n4\n\n\nexport\n \nSCONE_SLOTS\n=\n256\n\n\nexport\n \nSCONE_SIGPIPE\n=\n0\n\n\nexport\n \nSCONE_MMAP32BIT\n=\n0\n\n\nexport\n \nSCONE_SSPINS\n=\n100\n\n\nexport\n \nSCONE_SSLEEP\n=\n4000\n\n\nexport\n \nSCONE_KERNEL\n=\n0\n\n\nexport\n \nSCONE_HEAP\n=\n67108864\n\n\nexport\n \nSCONE_CONFIG\n=\n/etc/sgx-musl.conf\n\nexport\n \nSCONE_MODE\n=\nsim\nConfigure parameters: \n\n1\n.1.15\nHello World\n\n\n\n\nAlternatively, you could run this program in automatic mode:\n\n\n docker run --rm  -v \n$PWD\n:/usr/src/myapp -e \nSCONE_MODE\n=\nAUTO -e \nSCONE_ALPINE\n=\n1\n -e \nSCONE_VERSION\n=\n1\n sconecuratedimages/crosscompilers:runtime \n\nexport\n \nSCONE_QUEUES\n=\n4\n\n\nexport\n \nSCONE_SLOTS\n=\n256\n\n\nexport\n \nSCONE_SIGPIPE\n=\n0\n\n\nexport\n \nSCONE_MMAP32BIT\n=\n0\n\n\nexport\n \nSCONE_SSPINS\n=\n100\n\n\nexport\n \nSCONE_SSLEEP\n=\n4000\n\n\nexport\n \nSCONE_KERNEL\n=\n0\n\n\nexport\n \nSCONE_HEAP\n=\n67108864\n\n\nexport\n \nSCONE_CONFIG\n=\n/etc/sgx-musl.conf\n\nexport\n \nSCONE_MODE\n=\nsim\nConfigure parameters:\n\n1\n.1.15\nHelloWorld\n\n\n\n\nRun STRACE\n\n\nLets see how we can trace the program. Say, you have compile the program as shown above. After that you enter a cross compiler container and strace hello world as follows:\n\n\n docker run --cap-add SYS_PTRACE -it --rm --device\n=\n/dev/isgx -v \n$PWD\n:/usr/src/myapp -w /usr/src/myapp sconecuratedimages/crosscompilers strace  -f /usr/src/myapp/sgx_hello_world \n strace.log\nHello World\nhead strace.log\nexecve\n(\n/usr/src/myapp/sgx_hello_world\n, \n[\n/usr/src/myapp/sgx_hello_world\n]\n, \n[\n/* \n10\n vars */\n])\n \n=\n \n0\n\nbrk\n(\nNULL\n)\n                               \n=\n 0x10e8000\naccess\n(\n/etc/ld.so.nohwcap\n, F_OK\n)\n      \n=\n -1 ENOENT \n(\nNo such file or directory\n)\n\nmmap\n(\nNULL, \n8192\n, PROT_READ\n|\nPROT_WRITE, MAP_PRIVATE\n|\nMAP_ANONYMOUS, -1, \n0\n)\n \n=\n 0x7f17f07f1000\naccess\n(\n/etc/ld.so.preload\n, R_OK\n)\n      \n=\n -1 ENOENT \n(\nNo such file or directory\n)\n\nopen\n(\n/etc/ld.so.cache\n, O_RDONLY\n|\nO_CLOEXEC\n)\n \n=\n \n3\n\nfstat\n(\n3\n, \n{\nst_mode\n=\nS_IFREG\n|\n0644\n, \nst_size\n=\n18506\n, ...\n})\n \n=\n \n0\n\nmmap\n(\nNULL, \n18506\n, PROT_READ, MAP_PRIVATE, \n3\n, \n0\n)\n \n=\n 0x7f17f07ec000\nclose\n(\n3\n)\n                                \n=\n \n0\n\naccess\n(\n/etc/ld.so.nohwcap\n, F_OK\n)\n      \n=\n -1 ENOENT \n(\nNo such file or directory\n)\n\n\n\n\n\nScreencast\n\n\n\n\n \nscontain.com\n, November 2017. \nQuestions or Suggestions?", 
            "title": "SCONE Tutorial"
        }, 
        {
            "location": "/SCONE_TUTORIAL/#scone-tutorial", 
            "text": "", 
            "title": "SCONE Tutorial"
        }, 
        {
            "location": "/SCONE_TUTORIAL/#prerequisites", 
            "text": "", 
            "title": "Prerequisites"
        }, 
        {
            "location": "/SCONE_TUTORIAL/#ensure-that-the-sgx-driver-is-installed", 
            "text": "Check on the host as well as inside your containers that the SGX device  /dev/isgx  is visible:   ls /dev/isgx \n/dev/isgx  If the driver is not installed, read Section  SCONE Host Setup  to learn how to install the SGX driver.", 
            "title": "Ensure that the SGX driver is installed"
        }, 
        {
            "location": "/SCONE_TUTORIAL/#checking-availability-of-sgx-device-inside-of-containers", 
            "text": "Sometimes, Docker does not automatically map the SGX device inside of containers. We provide a patched Docker engine and a patched SGX driver that together permit to automatically open the sgx device inside of containers.  If your node is part of a Docker swarm, we provide a simple check as part of the  scone CLI . Say, the leader node of your swarm  is called  beatrix , then you could just execute:  $ scone swarm check --verbose --manager beatrix  In case your node is not part of a Docker swarm, you can run the checks manually. For that, we provide a container image\nthat helps to check if the SGX device can be accessed from inside a container. Just execute the following commands to check if your containers have access to the SGX device:  # preferred alternative: required for swarms to work: SGX device is available in all containers by default   sudo docker run --rm sconecuratedimages/checksgx  ||   echo   SGX device is not automatically mapped inside of container  # alternative: use --device option without --privileged flag   sudo docker run --device = /dev/isgx --rm sconecuratedimages/checksgx  ||   echo   --device=/dev/isgx: not sufficient to access SGX device inside of container  # last alternative: use --device option without --privileged flag   sudo docker run -v /dev/isgx:/dev/isgx --privileged  --rm sconecuratedimages/checksgx  ||   echo   SGX device NOT available inside of container   Use the first alternative that works in your installation to give containers access to the SGX device.", 
            "title": "Checking availability of SGX device inside of containers"
        }, 
        {
            "location": "/SCONE_TUTORIAL/#install-sgxmusl-cross-compiler-image", 
            "text": "Ensure that you installed the various sconecuratedimages/crosscompilers container image:   docker image ls sconecuratedimages/*\nREPOSITORY                          TAG                    IMAGE ID            CREATED             SIZE\nsconecuratedimages/crosscompilers   latest                 dff7975b7f32         7  hours ago          1 .57GB\nsconecuratedimages/crosscompilers   scone                  dff7975b7f32         7  hours ago          1 .57GB  If the cross compiler image is not yet installed, read Section  SCONE Curated Container Images  to learn how to install the SCONE cross compiler image.  If the docker command fails, please ensure that docker is indeed installed (see  SCONE Host Setup . Also, on some systems you might need to use  sodo  to run docker commands.", 
            "title": "Install sgxmusl cross compiler image"
        }, 
        {
            "location": "/SCONE_TUTORIAL/#install-the-tutorial", 
            "text": "Clone the tutorial:   git clone https://github.com/christoffetzer/SCONE_TUTORIAL.git", 
            "title": "Install the tutorial"
        }, 
        {
            "location": "/SCONE_TUTORIAL/#native-hello-world", 
            "text": "Ensure that  hello world  runs natively on your machine:    cd  SCONE_TUTORIAL/HelloWorld/  gcc hello_world.c  -o native_hello_world  ./native_hello_world\nHello World  Note that the generated executable, i.e.,  sim_hello_world , will only run on Linux.", 
            "title": "Native Hello World"
        }, 
        {
            "location": "/SCONE_TUTORIAL/#statically-linked-hello-world", 
            "text": "The default cross compiler variant that runs  hello world  inside of an enclave is  scone gcc  and you can find this in container  sconecuratedimages/crosscompilers:scone .\nThis variant requires access to the SGX device. \nIn Linux, the SGX device is made available as  /dev/isgx  and we can give the cross compiler inside of an container access via option  --device=/dev/isgx :   docker run --rm --device = /dev/isgx -v  $PWD :/usr/src/myapp -w /usr/src/myapp sconecuratedimages/crosscompilers:scone scone-gcc hello_world.c  -o sgx_hello_world  This generates a statically linked binary. However, as we mentioned above, the binary looks like a dynamically\nlinked binary since it is wrapped in a dynamically linked loader program:   ldd ./sgx_hello_world \n    linux-vdso.so.1  =    ( 0x00007ffcf73ad000 ) \n    libpthread.so.0  =  /lib/x86_64-linux-gnu/libpthread.so.0  ( 0x00007f7c2a0e9000 ) \n    libc.so.6  =  /lib/x86_64-linux-gnu/libc.so.6  ( 0x00007f7c29d1f000 ) \n    /lib64/ld-linux-x86-64.so.2  ( 0x00007f7c2a306000 )   Ensure that file  /etc/sgx-musl.conf  exists. If not, store some default file like:    printf   Q 1\\ne 0 0 0\\ns 1 0 0\\n   |  sudo tee /etc/sgx-musl.conf  To run  sgx_hello_world , in an enclave, just execute:   ./sgx_hello_world\nHello World  To see some more debug messages, set environment variable  SCONE_VERSION=1 :    SCONE_VERSION = 1  ./sgx_hello_world export   SCONE_QUEUES = 4  export   SCONE_SLOTS = 256  export   SCONE_SIGPIPE = 0  export   SCONE_MMAP32BIT = 0  export   SCONE_SSPINS = 100  export   SCONE_SSLEEP = 4000  export   SCONE_KERNEL = 0  export   SCONE_HEAP = 67108864  export   SCONE_CONFIG = /etc/sgx-musl.conf export   SCONE_MODE = hw export   SCONE_SGXBOUNDS = no export   SCONE_ALLOW_DLOPEN = no\nRevision: 9b355b99170ad434010353bb9f4dca24e532b1b7\nBranch: master\nConfigure options: --enable-file-prot --enable-shared --enable-debug --prefix = /scone/src/built/cross-compiler/x86_64-linux-musl\n\nHello World  The debug outputs  SCONE_MODE=hw  shows that  sgx_hello_world  runs in hardware mode, i.e., inside an SGX enclave.  Note.  The compilation as well as the hello world program will fail in case you do not have an SGX driver installed.", 
            "title": "Statically-Linked Hello World"
        }, 
        {
            "location": "/SCONE_TUTORIAL/#dynamically-linked-hello-world", 
            "text": "docker run --rm  -v  $PWD :/usr/src/myapp -w /usr/src/myapp sconecuratedimages/muslgcc gcc  hello_world.c -o dyn_hello_world  To run this natively, just execute the following:   docker run --rm  -v  $PWD :/usr/src/myapp -w /usr/src/myapp sconecuratedimages/muslgcc ./dyn_hello_world  To run a dynamically-linked binary in an enclave, you need to run this in a special runtime environment. In this environment you can ask binaries to run inside of enclaves by setting environment  SCONE_ALPINE=1 . To indicate that we are indeed running inside an enclave, we ask to issue some debug messages from inside the enclave by setting environment variable  SCONE_VERSION=1 :", 
            "title": "Dynamically-Linked Hello World"
        }, 
        {
            "location": "/SCONE_TUTORIAL/#hardware-mode-vs-simulation-mode", 
            "text": "For debugging, we support three different modes for execution:  hardware, simulation, and automatic :    hardware : by setting environment variable to  SCONE_MODE=HW , SCONE will enforce running this application inside an SGX enclave.    simulation : by setting environment variable to  SCONE_MODE=SIM , SCONE will enforce running this application in native mode (i.e., outside of an enclave). This will run all SCONE functionality but outside enclaves. This is intended for development and debugging on machines that are not SGX-capable.    automatic : by setting environment variable to  SCONE_MODE=AUTO , SCONE will run the application inside of an SGX enclave if available and otherwise in simulation mode. (This is the default mode)    NOTE : In production mode, you must only permit running in hardware mode. Scone ensures this with the help of  remote attestation : the SCONE configuration and attestation service (CAS) will only provide configuration information and secrets to an application only after it has proven (with the help of SGX CPU extensions) that it is indeed running inside an SGX enclave.", 
            "title": "Hardware Mode vs Simulation Mode"
        }, 
        {
            "location": "/SCONE_TUTORIAL/#execution-on-a-sgx-capable-machine", 
            "text": "docker run --rm  -v  $PWD :/usr/src/myapp -e  SCONE_MODE = HW -e  SCONE_ALPINE = 1  -e  SCONE_VERSION = 1  sconecuratedimages/crosscompilers:runtime /usr/src/myapp/dyn_hello_world export   SCONE_QUEUES = 4  export   SCONE_SLOTS = 256  export   SCONE_SIGPIPE = 0  export   SCONE_MMAP32BIT = 0  export   SCONE_SSPINS = 100  export   SCONE_SSLEEP = 4000  export   SCONE_KERNEL = 0  export   SCONE_HEAP = 67108864  export   SCONE_CONFIG = /etc/sgx-musl.conf export   SCONE_MODE = hw\nConfigure parameters:  1 .1.15\nHello World", 
            "title": "Execution on a SGX-capable machine"
        }, 
        {
            "location": "/SCONE_TUTORIAL/#execution-on-a-non-sgx-machine", 
            "text": "If you run this inside a container without access to SGX (/dev/isgx), for example, when running on a Mac, you will see the following error message:   docker run --rm  -v  $PWD :/usr/src/myapp -e  SCONE_MODE = HW -e  SCONE_ALPINE = 1  -e  SCONE_VERSION = 1  sconecuratedimages/crosscompilers:runtime /usr/src/myapp/dyn_hello_world [ Error ]  Could not create enclave: Error opening SGX device  You could run this in simulation mode as follows:   docker run --rm  -v  $PWD :/usr/src/myapp -e  SCONE_MODE = SIM -e  SCONE_ALPINE = 1  -e  SCONE_VERSION = 1  sconecuratedimages/crosscompilers:runtime /usr/src/myapp/dyn_hello_world export   SCONE_QUEUES = 4  export   SCONE_SLOTS = 256  export   SCONE_SIGPIPE = 0  export   SCONE_MMAP32BIT = 0  export   SCONE_SSPINS = 100  export   SCONE_SSLEEP = 4000  export   SCONE_KERNEL = 0  export   SCONE_HEAP = 67108864  export   SCONE_CONFIG = /etc/sgx-musl.conf export   SCONE_MODE = sim\nConfigure parameters:  1 .1.15\nHello World  Alternatively, you could run this program in automatic mode:   docker run --rm  -v  $PWD :/usr/src/myapp -e  SCONE_MODE = AUTO -e  SCONE_ALPINE = 1  -e  SCONE_VERSION = 1  sconecuratedimages/crosscompilers:runtime  export   SCONE_QUEUES = 4  export   SCONE_SLOTS = 256  export   SCONE_SIGPIPE = 0  export   SCONE_MMAP32BIT = 0  export   SCONE_SSPINS = 100  export   SCONE_SSLEEP = 4000  export   SCONE_KERNEL = 0  export   SCONE_HEAP = 67108864  export   SCONE_CONFIG = /etc/sgx-musl.conf export   SCONE_MODE = sim\nConfigure parameters: 1 .1.15\nHelloWorld", 
            "title": "Execution on a non-SGX machine"
        }, 
        {
            "location": "/SCONE_TUTORIAL/#run-strace", 
            "text": "Lets see how we can trace the program. Say, you have compile the program as shown above. After that you enter a cross compiler container and strace hello world as follows:   docker run --cap-add SYS_PTRACE -it --rm --device = /dev/isgx -v  $PWD :/usr/src/myapp -w /usr/src/myapp sconecuratedimages/crosscompilers strace  -f /usr/src/myapp/sgx_hello_world   strace.log\nHello World\nhead strace.log\nexecve ( /usr/src/myapp/sgx_hello_world ,  [ /usr/src/myapp/sgx_hello_world ] ,  [ /*  10  vars */ ])   =   0 \nbrk ( NULL )                                 =  0x10e8000\naccess ( /etc/ld.so.nohwcap , F_OK )        =  -1 ENOENT  ( No such file or directory ) \nmmap ( NULL,  8192 , PROT_READ | PROT_WRITE, MAP_PRIVATE | MAP_ANONYMOUS, -1,  0 )   =  0x7f17f07f1000\naccess ( /etc/ld.so.preload , R_OK )        =  -1 ENOENT  ( No such file or directory ) \nopen ( /etc/ld.so.cache , O_RDONLY | O_CLOEXEC )   =   3 \nfstat ( 3 ,  { st_mode = S_IFREG | 0644 ,  st_size = 18506 , ... })   =   0 \nmmap ( NULL,  18506 , PROT_READ, MAP_PRIVATE,  3 ,  0 )   =  0x7f17f07ec000\nclose ( 3 )                                  =   0 \naccess ( /etc/ld.so.nohwcap , F_OK )        =  -1 ENOENT  ( No such file or directory )", 
            "title": "Run STRACE"
        }, 
        {
            "location": "/SCONE_TUTORIAL/#screencast", 
            "text": "scontain.com , November 2017.  Questions or Suggestions?", 
            "title": "Screencast"
        }, 
        {
            "location": "/SCONE_GENERATE_IMAGE/", 
            "text": "Generating Container Image with SCONE\n\n\nWe show how to generate a Docker image that contains our \nhello world\n running inside of an enclave and pushing this to docker hub. We only show this for the statically-linked binary. You can see that this code is quite awkward. It is much easier to generate images with a Dockerfile - which we show in the next section.\n\n\nPrerequisites\n\n\nCheck that all prerequisites from \nSCONE Tutorial\n are satisfied. \nClone the SCONE_TUTORIAL before you start creating a \nhello world\n image.\n\n\nGenerate HelloWorld image\n\n\nWe generate a \nhello world\n container image. \n\n\n \ncd\n SCONE_TUTORIAL/CreateImage\n\n\n\n\nYou can either execute all step manually by copy\npasting all instructions or you can just execute\n\n docker login\n\n sudo ./Dockerfile.sh\n\n\nand watch the outputs.\n\n\nPlease change the image name to a repository on docker hub to which you can write:\n\n\n \nexport\n \nTAG\n=\nlatest\n\n\n \nexport\n \nIMAGE_NAME\n=\nsconecuratedimages/helloworld\n\n\n\n\n\nWe generate container and compile hello world inside of this container with the help of our standard SCONE cross compiler:\n\n\n \nCONTAINER_ID\n=\n`\ndocker run -d -it --device\n=\n/dev/isgx  -v \n$(\npwd\n)\n:/mnt sconecuratedimages/crosscompilers:scone bash -c \n\n\nset -e\n\n\nprintf \nQ 1\\ne 0 0 0\\ns 1 0 0\\n\n \n /etc/sgx-musl.conf\n\n\nsgxmusl-hw-async-gcc /mnt/hello_world.c  -o /usr/local/bin/sgx_hello_world\n\n\n`\n\n\n\n\n\nNote that above will fail if you do not have access to the SGX device \n/dev/isgx\n.\n\n\nTurn the container into an image:\n\n\n \nIMAGE_ID\n=\n$(\ndocker commit -p -c \nCMD sgx_hello_world\n \n$CONTAINER_ID\n \n$IMAGE_NAME\n:\n$TAG\n)\n\n\n\n\n\nYou can run this image by executing:\n\n\n sudo docker run --device\n=\n/dev/isgx \n$IMAGE_NAME\n:\n$TAG\n\n\n\n\n\nYou can push this image to Docker. However, ensure that you first login to docker:\n\n\n sudo docker login\n\n\n\n\nbefore you push the image to docker hub:\n\n\n sudo docker push \n$IMAGE_NAME\n:\n$TAG\n\n\n\n\n\nNote: this will fail in case you do not have the permission to push to this repository. \n\n\nScreencast\n\n\n\n\n \nscontain.com\n, November 2017. \nQuestions or Suggestions?", 
            "title": "SCONE Create Image"
        }, 
        {
            "location": "/SCONE_GENERATE_IMAGE/#generating-container-image-with-scone", 
            "text": "We show how to generate a Docker image that contains our  hello world  running inside of an enclave and pushing this to docker hub. We only show this for the statically-linked binary. You can see that this code is quite awkward. It is much easier to generate images with a Dockerfile - which we show in the next section.", 
            "title": "Generating Container Image with SCONE"
        }, 
        {
            "location": "/SCONE_GENERATE_IMAGE/#prerequisites", 
            "text": "Check that all prerequisites from  SCONE Tutorial  are satisfied. \nClone the SCONE_TUTORIAL before you start creating a  hello world  image.", 
            "title": "Prerequisites"
        }, 
        {
            "location": "/SCONE_GENERATE_IMAGE/#generate-helloworld-image", 
            "text": "We generate a  hello world  container image.     cd  SCONE_TUTORIAL/CreateImage  You can either execute all step manually by copy pasting all instructions or you can just execute  docker login  sudo ./Dockerfile.sh \nand watch the outputs.  Please change the image name to a repository on docker hub to which you can write:    export   TAG = latest    export   IMAGE_NAME = sconecuratedimages/helloworld   We generate container and compile hello world inside of this container with the help of our standard SCONE cross compiler:    CONTAINER_ID = ` docker run -d -it --device = /dev/isgx  -v  $( pwd ) :/mnt sconecuratedimages/crosscompilers:scone bash -c   set -e  printf  Q 1\\ne 0 0 0\\ns 1 0 0\\n    /etc/sgx-musl.conf  sgxmusl-hw-async-gcc /mnt/hello_world.c  -o /usr/local/bin/sgx_hello_world  `   Note that above will fail if you do not have access to the SGX device  /dev/isgx .  Turn the container into an image:    IMAGE_ID = $( docker commit -p -c  CMD sgx_hello_world   $CONTAINER_ID   $IMAGE_NAME : $TAG )   You can run this image by executing:   sudo docker run --device = /dev/isgx  $IMAGE_NAME : $TAG   You can push this image to Docker. However, ensure that you first login to docker:   sudo docker login  before you push the image to docker hub:   sudo docker push  $IMAGE_NAME : $TAG   Note: this will fail in case you do not have the permission to push to this repository.", 
            "title": "Generate HelloWorld image"
        }, 
        {
            "location": "/SCONE_GENERATE_IMAGE/#screencast", 
            "text": "scontain.com , November 2017.  Questions or Suggestions?", 
            "title": "Screencast"
        }, 
        {
            "location": "/SCONE_Dockerfile/", 
            "text": "Dockerfile\n\n\nWe show how to generate a first secure container image with the help of a Dockerfile.\n\n\nPrerequisites\n\n\nEnsure that the sgx driver is installed\n\n\n ls /dev/isgx \n/dev/isgx\n\n\n\n\nIf the driver is not installed, read Section \nSCONE Host Setup\n to learn how to install the SGX driver.\n\n\nEnsure that the patched docker engine is installed\n\n\nWe need \ndocker build\n in this example. This command does not permit to map devices in the newly created containers. Hence, we provide a patched Docker engine \nSCONE Host Setup\n.\n\n\nInstall the tutorial\n\n\nClone the tutorial: \n\n\n git clone https://github.com/christoffetzer/SCONE_TUTORIAL.git\n\n\n\n\nAccess to SCONE Curated Images\n\n\nRight now, access to the curated images is still restricted. Please, send email to \n to request access.\n\n\nGenerate HelloAgain image (dynamically-linked)\n\n\nWe first generate a \nhello again\n container image with a dynamically-linked secure program:\n\n\n \ncd\n SCONE_TUTORIAL/DLDockerFile\n\n\n\n\nThe Dockerfile to generate the new image looks like this:\n\n\nFROM\n sconecuratedimages/crosscompilers:runtime\n\n\n\nMAINTAINER\n Christof Fetzer \nchristof.fetzer@gmail.com\n\n\n\nRUN\n mkdir /hello\n\nCOPY dyn_hello_again /hello/\n\n\n\nCMD\n SCONE_MODE=HW SCONE_ALPINE=1 SCONE_VERSION=1 /hello/dyn_hello_again\n\n\n\n\n\nThis assumes that we already generated the dynamically linked binary with\nan appropriately configured gcc. We generate this with the provided gcc image:\n\n\n docker run --rm  -v \n$PWD\n:/usr/src/myapp -w /usr/src/myapp sconecuratedimages/muslgcc gcc  hello_again.c -o dyn_hello_again\n\n\n\n\nWe provide a little script that generates the image and pushes it to Docker hub (which should fail since you should not have the credentials):\n\n\n ./generate.sh\n\n\n\n\nYou can run this program inside of enclave (with the output of debug messages):\n\n\n docker run -it sconecuratedimages/helloworld:dynamic\n\nexport\n \nSCONE_QUEUES\n=\n4\n\n\nexport\n \nSCONE_SLOTS\n=\n256\n\n\nexport\n \nSCONE_SIGPIPE\n=\n0\n\n\nexport\n \nSCONE_MMAP32BIT\n=\n0\n\n\nexport\n \nSCONE_SSPINS\n=\n100\n\n\nexport\n \nSCONE_SSLEEP\n=\n4000\n\n\nexport\n \nSCONE_KERNEL\n=\n0\n\n\nexport\n \nSCONE_HEAP\n=\n67108864\n\n\nexport\n \nSCONE_CONFIG\n=\n/etc/sgx-musl.conf\n\nexport\n \nSCONE_MODE\n=\nhw\nConfigure parameters: \n\n1\n.1.15\nHello Again\n\n\n\n\nThis image is nicely small (only 11MB) since it only contains the runtime environment and no development environment.\n\n\nRunning on a docker engine without access to SGX, we get an error message:\n\n\n docker run -it sconecuratedimages/helloworld:dynamic\n\n[\nError\n]\n Could not create enclave: Error opening SGX device \n\n\n\n\nScreencast\n\n\n\n\nGenerate HelloAgain image (statically-linked)\n\n\nWe generate a \nhello again\n container image. \n\n\n \ncd\n SCONE_TUTORIAL/DockerFile\n\n\n\n\nThe Dockerfile is quite straight forward:\n\n\nFROM\n sconecuratedimages/crosscompilers:scone\n\n\n\nMAINTAINER\n Christof Fetzer \nchristof.fetzer@gmail.com\n\n\n\nRUN\n mkdir /hello\n\nCOPY hello_again.c /hello/\n\n\nRUN\n \ncd\n /hello \n scone-gcc hello_again.c -o again\n\n\nCMD\n [\n/hello/again\n]\n\n\n\n\n\nYou can either execute all step manually (see below) or you can just execute\n\n\n docker login\n./generate.sh\n\n\n\n\nand watch the outputs. The push of the image should fail since you should not have the access rights to push the image to Docker hub.\n\n\nWe define the image name and tag that we want to generate:\n\n\nexport\n \nTAG\n=\nagain\n\n\nexport\n \nFULLTAG\n=\nsconecuratedimages/helloworld:\n$TAG\n\n\n\n\n\nWe build the image:\n\n\n docker build --pull -t \n$FULLTAG\n .\n\n\n\n\n docker run -it \n$FULLTAG\n\n\n\n\n\nWe push it to docker hub (will fail unless you have the right to push \n$FULLTAG\n):\n\n\n docker push \n$FULLTAG\n\n\n\n\n\nPlease change the image name to a repository on docker hub to which you can write:\n\n\n \nexport\n \nTAG\n=\nlatest\n\n\n \nexport\n \nIMAGE_NAME\n=\nsconecuratedimages/helloAgain\n\n\n\n\n\nScreencast\n\n\n\n\n \nscontain.com\n, November 2017. \nQuestions or Suggestions?", 
            "title": "SCONE Dockerfile"
        }, 
        {
            "location": "/SCONE_Dockerfile/#dockerfile", 
            "text": "We show how to generate a first secure container image with the help of a Dockerfile.", 
            "title": "Dockerfile"
        }, 
        {
            "location": "/SCONE_Dockerfile/#prerequisites", 
            "text": "", 
            "title": "Prerequisites"
        }, 
        {
            "location": "/SCONE_Dockerfile/#ensure-that-the-sgx-driver-is-installed", 
            "text": "ls /dev/isgx \n/dev/isgx  If the driver is not installed, read Section  SCONE Host Setup  to learn how to install the SGX driver.", 
            "title": "Ensure that the sgx driver is installed"
        }, 
        {
            "location": "/SCONE_Dockerfile/#ensure-that-the-patched-docker-engine-is-installed", 
            "text": "We need  docker build  in this example. This command does not permit to map devices in the newly created containers. Hence, we provide a patched Docker engine  SCONE Host Setup .", 
            "title": "Ensure that the patched docker engine is installed"
        }, 
        {
            "location": "/SCONE_Dockerfile/#install-the-tutorial", 
            "text": "Clone the tutorial:    git clone https://github.com/christoffetzer/SCONE_TUTORIAL.git", 
            "title": "Install the tutorial"
        }, 
        {
            "location": "/SCONE_Dockerfile/#access-to-scone-curated-images", 
            "text": "Right now, access to the curated images is still restricted. Please, send email to   to request access.", 
            "title": "Access to SCONE Curated Images"
        }, 
        {
            "location": "/SCONE_Dockerfile/#generate-helloagain-image-dynamically-linked", 
            "text": "We first generate a  hello again  container image with a dynamically-linked secure program:    cd  SCONE_TUTORIAL/DLDockerFile  The Dockerfile to generate the new image looks like this:  FROM  sconecuratedimages/crosscompilers:runtime  MAINTAINER  Christof Fetzer  christof.fetzer@gmail.com  RUN  mkdir /hello\n\nCOPY dyn_hello_again /hello/ CMD  SCONE_MODE=HW SCONE_ALPINE=1 SCONE_VERSION=1 /hello/dyn_hello_again   This assumes that we already generated the dynamically linked binary with\nan appropriately configured gcc. We generate this with the provided gcc image:   docker run --rm  -v  $PWD :/usr/src/myapp -w /usr/src/myapp sconecuratedimages/muslgcc gcc  hello_again.c -o dyn_hello_again  We provide a little script that generates the image and pushes it to Docker hub (which should fail since you should not have the credentials):   ./generate.sh  You can run this program inside of enclave (with the output of debug messages):   docker run -it sconecuratedimages/helloworld:dynamic export   SCONE_QUEUES = 4  export   SCONE_SLOTS = 256  export   SCONE_SIGPIPE = 0  export   SCONE_MMAP32BIT = 0  export   SCONE_SSPINS = 100  export   SCONE_SSLEEP = 4000  export   SCONE_KERNEL = 0  export   SCONE_HEAP = 67108864  export   SCONE_CONFIG = /etc/sgx-musl.conf export   SCONE_MODE = hw\nConfigure parameters:  1 .1.15\nHello Again  This image is nicely small (only 11MB) since it only contains the runtime environment and no development environment.  Running on a docker engine without access to SGX, we get an error message:   docker run -it sconecuratedimages/helloworld:dynamic [ Error ]  Could not create enclave: Error opening SGX device", 
            "title": "Generate HelloAgain image (dynamically-linked)"
        }, 
        {
            "location": "/SCONE_Dockerfile/#screencast", 
            "text": "", 
            "title": "Screencast"
        }, 
        {
            "location": "/SCONE_Dockerfile/#generate-helloagain-image-statically-linked", 
            "text": "We generate a  hello again  container image.     cd  SCONE_TUTORIAL/DockerFile  The Dockerfile is quite straight forward:  FROM  sconecuratedimages/crosscompilers:scone  MAINTAINER  Christof Fetzer  christof.fetzer@gmail.com  RUN  mkdir /hello\n\nCOPY hello_again.c /hello/ RUN   cd  /hello   scone-gcc hello_again.c -o again CMD  [ /hello/again ]   You can either execute all step manually (see below) or you can just execute   docker login\n./generate.sh  and watch the outputs. The push of the image should fail since you should not have the access rights to push the image to Docker hub.  We define the image name and tag that we want to generate:  export   TAG = again  export   FULLTAG = sconecuratedimages/helloworld: $TAG   We build the image:   docker build --pull -t  $FULLTAG  .   docker run -it  $FULLTAG   We push it to docker hub (will fail unless you have the right to push  $FULLTAG ):   docker push  $FULLTAG   Please change the image name to a repository on docker hub to which you can write:    export   TAG = latest    export   IMAGE_NAME = sconecuratedimages/helloAgain", 
            "title": "Generate HelloAgain image (statically-linked)"
        }, 
        {
            "location": "/SCONE_Dockerfile/#screencast_1", 
            "text": "scontain.com , November 2017.  Questions or Suggestions?", 
            "title": "Screencast"
        }, 
        {
            "location": "/SCONE_Swarm_Example/", 
            "text": "Starting a SCONE Application on a Swarm\n\n\nWe show how to run a \nsecure nginx\n version, i.e., one that runs inside an enclave in a\ndocker swarm with automatic restarts. To simplify the running of services, we provide \na simple wrapper around the \ndocker service\n command: the \nscone service\n executes\n\ndocker service\n commands on the manager of a swarm. The manager is either specified via\nan option \n--manager \n or via environment variable \nSCONE_MANAGER\n. This is \ndone in the same way as for command \nscone swarm\n.\n\n\nIn what follows, we assume that \nSCONE_MANAGER\n is set to the leader of the swarm.\nThe \nscone\n commands are typically executed in a container running at the \ndeveloper\nsite\n.\n\n\nPrerequisites\n\n\nRegistry support\n\n\nFor running an application in a Docker Swarm, you need to set up a local registry to\nensure that all nodes get access to the same container image. The scone CLI expects the\nregistry to be available at \nlocalhost:5000\n. You can start a default registry with the\nhelp of \nscone\n:\n\n\n$ scone service registry --verbose\nRegistry is already running in swarm beatrix\n\n\n\n\nTo simplify pushing images to the local registry, the scone CLI includes a \nscone service pull\n command to pull\nan image from docker hub and then to push this image to the local registry. For example, to pull image\n\nsconecuratedimages/sconetainer:noshielding\n and store it as \nlocalhost:5000/sconetainer:noshielding\n,\njust execute:\n\n\n$ scone service pull sconecuratedimages/sconetainer:noshielding\nnew tag: localhost:5000/sconetainer:noshielding\n\n\n\n\nSGX Support\n\n\nServices are automatically restarted. In case, there is a persistent failure in some service \nha\n, we would see\nrepeated restarts like: \n\n\n$ scone service ps ha\nID                  NAME                IMAGE                      NODE                DESIRED STATE       CURRENT STATE           ERROR                       PORTS\nf65id6ow5n6w        ha.1                sconecuratedimages/nginx   beatrix             Ready               Ready \n1\n second ago                                  \njt6wj5e3lso4         \n\\_\n ha.1            sconecuratedimages/nginx   beatrix             Shutdown            Failed \n3\n seconds ago    \ntask: non-zero exit (1)\n   \nsspou3mcis8m         \n\\_\n ha.1            sconecuratedimages/nginx   beatrix             Shutdown            Failed \n9\n seconds ago    \ntask: non-zero exit (1)\n   \np3bw780pu63b         \n\\_\n ha.1            sconecuratedimages/nginx   beatrix             Shutdown            Failed \n15\n seconds ago   \ntask: non-zero exit (1)\n   \n75zjsesil5k4         \n\\_\n ha.1            sconecuratedimages/nginx   beatrix             Shutdown            Failed \n22\n seconds ago   \ntask: non-zero exit (1)\n   \n\n\n\n\nReasons for such failures might be that that the containers might not have access to the sgx device. \n\n\nThere are multiple reasons why the driver might not be accessible inside of a container: Did you indeed install the patched docker version? Did you indeed label the nodes correctly? To automatically diagnoses and in some cases, to perform some automatic corrections, just execute \nscone swarm check\n:\n\n\n$ scone swarm check\nwarning:  \nsgx device is not automatically mapped inside of container on host beatrix (stack=198 434 0)\n  \n(\nLine numer: \n198\n)\n\nwarning:  \n--device=/dev/isgx: device mapper does not work inside of container on host beatrix (stack=199 434 0)\n  \n(\nLine numer: \n199\n)\n\n\n\n\n\nTo get a summary view of a swarm after you performed a check, just executed:\n\n\n$ scone swarm ls\nNODENO        SGX VERSION   DOCKER-ENGINE SGX-DRIVER    HOST                 STATUS     AVAILABILITY  MANAGER   \n\n2\n             \n1\n             SCONE         SCONE         caroline             Ready      Active        Reachable \n\n3\n             \n1\n             SCONE         SCONE         dorothy              Ready      Active                  \n\n4\n             \n1\n             SCONE         SCONE         edna                 Ready      Active        Reachable \n\n1\n             \n1\n             SCONE         SCONE         beatrix              Ready      Active        Leader    \n\n\n\n\nStarting a Service\n\n\nAfter pulling an image into the local registry (see above), we can\nstart a service in the swarm via \nscone service create\n.\nDocker swarm will start the image and it also takes care of failures by restarting failed services.\n\n\nFor the next steps, make sure that all nodes have access to image \nsconecuratedimages/sconetainer:noshielding\n and pull this image via:\n\n\n$ scone service pull sconecuratedimages/sconetainer:noshielding\nnew tag: localhost:5000/sconetainer:noshielding\n\n\n\n\nWe start a nginx service including a version of the scontain.com website. We start two replicas running inside separate enclaves - most likely on two different nodes:\n\n\n$ scone service create --name sconeweb --detach\n=\ntrue\n  --publish \n80\n:80 --publish \n443\n:443 --replicas\n=\n2\n localhost:5000/sconetainer:noshielding\n\n\n\n\nIn case the service starts up correctly, you will see a status like this:\n\n\n$ scone service ps sconeweb\nID                  NAME           IMAGE                              NODE                DESIRED STATE       CURRENT STATE            ERROR                              PORTS\nba3odjkz6mx2        sconeweb.1     localhost:5000/nginx:noshielding   alice               Running             Running \n8\n minutes ago         \nx2xq1c3aede7        sconeweb.2     localhost:5000/nginx:noshielding   beatrix               Running             Running \n8\n minutes ago         \n\n\n\n\nIf, for example, an image is not available on all nodes, you might see the following status:\n\n\n$ scone service ps sconeweb\nID                  NAME                IMAGE                                        NODE                DESIRED STATE       CURRENT STATE          ERROR                              PORTS\no79714pw2fpn        sconeweb.1          sconecuratedimages/sconetainer:noshielding   alice               Running             Running \n4\n hours ago                                       \nt0byepte0fzj         \n\\_\n sconeweb.1      sconecuratedimages/sconetainer:noshielding   beatrix             Shutdown            Rejected \n4\n hours ago   \nNo such image: sconecuratedim\u2026\n   \nmg4xdq868syq         \n\\_\n sconeweb.1      sconecuratedimages/sconetainer:noshielding   beatrix             Shutdown            Rejected \n4\n hours ago   \nNo such image: sconecuratedim\u2026\n   \nry1pqen9jgan         \n\\_\n sconeweb.1      sconecuratedimages/sconetainer:noshielding   beatrix             Shutdown            Rejected \n4\n hours ago   \nNo such image: sconecuratedim\u2026\n   \nq05ti7gkxc7r         \n\\_\n sconeweb.1      sconecuratedimages/sconetainer:noshielding   beatrix             Shutdown            Rejected \n4\n hours ago   \nNo such image: sconecuratedim\u2026\n   \nzxj74inh2zdf        sconeweb.2          sconecuratedimages/sconetainer:noshielding   alice               Running             Running \n4\n hours ago                                       \n\n\n\n\nStopping the service\n\n\nStop the service via:\n\n\n$ scone service rm sconeweb\n\n\n\n\nUpdating the image of a service\n\n\nSay, there is a new version of the sconetainer image available. We can update this image in our local registry\nas follows:\n\n\n$ scone pull sconecuratedimages/sconetainer:noshielding\n\n\n\n\nWe can now update the service as follows:\n\n\n$ scone service update --image  localhost:5000/sconetainer sconeweb\n\n\n\n\nDraining a node\n\n\nTo be able to drain all containers from a node, we need to figure out the node's id. We can do this manually by executing the following command on the leader node:\n\n\n sudo docker node ls\nID                            HOSTNAME            STATUS              AVAILABILITY        MANAGER STATUS\n91a1vvex4dgozfrzy1y136gmg *   alice               Ready               Active              Leader\njhrayos9ylu02egwvkxpqtbwb     beatrix             Ready               Active              \n\n\n\n\nYou can now take node \nalice\n out of service by executing:\n\n\n sudo docker node update --availability drain 91a1vvex4dgozfrzy1y136gmg\n\n\n\n\nTo put the node back in service by executing:\n\n\n sudo docker node update --availability active 91a1vvex4dgozfrzy1y136gmg\n\n\n\n\n \nscontain.com\n, November 2017. \nQuestions or Suggestions?", 
            "title": "SCONE Swarm Example"
        }, 
        {
            "location": "/SCONE_Swarm_Example/#starting-a-scone-application-on-a-swarm", 
            "text": "We show how to run a  secure nginx  version, i.e., one that runs inside an enclave in a\ndocker swarm with automatic restarts. To simplify the running of services, we provide \na simple wrapper around the  docker service  command: the  scone service  executes docker service  commands on the manager of a swarm. The manager is either specified via\nan option  --manager   or via environment variable  SCONE_MANAGER . This is \ndone in the same way as for command  scone swarm .  In what follows, we assume that  SCONE_MANAGER  is set to the leader of the swarm.\nThe  scone  commands are typically executed in a container running at the  developer\nsite .", 
            "title": "Starting a SCONE Application on a Swarm"
        }, 
        {
            "location": "/SCONE_Swarm_Example/#prerequisites", 
            "text": "", 
            "title": "Prerequisites"
        }, 
        {
            "location": "/SCONE_Swarm_Example/#registry-support", 
            "text": "For running an application in a Docker Swarm, you need to set up a local registry to\nensure that all nodes get access to the same container image. The scone CLI expects the\nregistry to be available at  localhost:5000 . You can start a default registry with the\nhelp of  scone :  $ scone service registry --verbose\nRegistry is already running in swarm beatrix  To simplify pushing images to the local registry, the scone CLI includes a  scone service pull  command to pull\nan image from docker hub and then to push this image to the local registry. For example, to pull image sconecuratedimages/sconetainer:noshielding  and store it as  localhost:5000/sconetainer:noshielding ,\njust execute:  $ scone service pull sconecuratedimages/sconetainer:noshielding\nnew tag: localhost:5000/sconetainer:noshielding", 
            "title": "Registry support"
        }, 
        {
            "location": "/SCONE_Swarm_Example/#sgx-support", 
            "text": "Services are automatically restarted. In case, there is a persistent failure in some service  ha , we would see\nrepeated restarts like:   $ scone service ps ha\nID                  NAME                IMAGE                      NODE                DESIRED STATE       CURRENT STATE           ERROR                       PORTS\nf65id6ow5n6w        ha.1                sconecuratedimages/nginx   beatrix             Ready               Ready  1  second ago                                  \njt6wj5e3lso4          \\_  ha.1            sconecuratedimages/nginx   beatrix             Shutdown            Failed  3  seconds ago     task: non-zero exit (1)    \nsspou3mcis8m          \\_  ha.1            sconecuratedimages/nginx   beatrix             Shutdown            Failed  9  seconds ago     task: non-zero exit (1)    \np3bw780pu63b          \\_  ha.1            sconecuratedimages/nginx   beatrix             Shutdown            Failed  15  seconds ago    task: non-zero exit (1)    \n75zjsesil5k4          \\_  ha.1            sconecuratedimages/nginx   beatrix             Shutdown            Failed  22  seconds ago    task: non-zero exit (1)      Reasons for such failures might be that that the containers might not have access to the sgx device.   There are multiple reasons why the driver might not be accessible inside of a container: Did you indeed install the patched docker version? Did you indeed label the nodes correctly? To automatically diagnoses and in some cases, to perform some automatic corrections, just execute  scone swarm check :  $ scone swarm check\nwarning:   sgx device is not automatically mapped inside of container on host beatrix (stack=198 434 0)    ( Line numer:  198 ) \nwarning:   --device=/dev/isgx: device mapper does not work inside of container on host beatrix (stack=199 434 0)    ( Line numer:  199 )   To get a summary view of a swarm after you performed a check, just executed:  $ scone swarm ls\nNODENO        SGX VERSION   DOCKER-ENGINE SGX-DRIVER    HOST                 STATUS     AVAILABILITY  MANAGER    2               1              SCONE         SCONE         caroline             Ready      Active        Reachable  3               1              SCONE         SCONE         dorothy              Ready      Active                   4               1              SCONE         SCONE         edna                 Ready      Active        Reachable  1               1              SCONE         SCONE         beatrix              Ready      Active        Leader", 
            "title": "SGX Support"
        }, 
        {
            "location": "/SCONE_Swarm_Example/#starting-a-service", 
            "text": "After pulling an image into the local registry (see above), we can\nstart a service in the swarm via  scone service create .\nDocker swarm will start the image and it also takes care of failures by restarting failed services.  For the next steps, make sure that all nodes have access to image  sconecuratedimages/sconetainer:noshielding  and pull this image via:  $ scone service pull sconecuratedimages/sconetainer:noshielding\nnew tag: localhost:5000/sconetainer:noshielding  We start a nginx service including a version of the scontain.com website. We start two replicas running inside separate enclaves - most likely on two different nodes:  $ scone service create --name sconeweb --detach = true   --publish  80 :80 --publish  443 :443 --replicas = 2  localhost:5000/sconetainer:noshielding  In case the service starts up correctly, you will see a status like this:  $ scone service ps sconeweb\nID                  NAME           IMAGE                              NODE                DESIRED STATE       CURRENT STATE            ERROR                              PORTS\nba3odjkz6mx2        sconeweb.1     localhost:5000/nginx:noshielding   alice               Running             Running  8  minutes ago         \nx2xq1c3aede7        sconeweb.2     localhost:5000/nginx:noshielding   beatrix               Running             Running  8  minutes ago           If, for example, an image is not available on all nodes, you might see the following status:  $ scone service ps sconeweb\nID                  NAME                IMAGE                                        NODE                DESIRED STATE       CURRENT STATE          ERROR                              PORTS\no79714pw2fpn        sconeweb.1          sconecuratedimages/sconetainer:noshielding   alice               Running             Running  4  hours ago                                       \nt0byepte0fzj          \\_  sconeweb.1      sconecuratedimages/sconetainer:noshielding   beatrix             Shutdown            Rejected  4  hours ago    No such image: sconecuratedim\u2026    \nmg4xdq868syq          \\_  sconeweb.1      sconecuratedimages/sconetainer:noshielding   beatrix             Shutdown            Rejected  4  hours ago    No such image: sconecuratedim\u2026    \nry1pqen9jgan          \\_  sconeweb.1      sconecuratedimages/sconetainer:noshielding   beatrix             Shutdown            Rejected  4  hours ago    No such image: sconecuratedim\u2026    \nq05ti7gkxc7r          \\_  sconeweb.1      sconecuratedimages/sconetainer:noshielding   beatrix             Shutdown            Rejected  4  hours ago    No such image: sconecuratedim\u2026    \nzxj74inh2zdf        sconeweb.2          sconecuratedimages/sconetainer:noshielding   alice               Running             Running  4  hours ago", 
            "title": "Starting a Service"
        }, 
        {
            "location": "/SCONE_Swarm_Example/#stopping-the-service", 
            "text": "Stop the service via:  $ scone service rm sconeweb", 
            "title": "Stopping the service"
        }, 
        {
            "location": "/SCONE_Swarm_Example/#updating-the-image-of-a-service", 
            "text": "Say, there is a new version of the sconetainer image available. We can update this image in our local registry\nas follows:  $ scone pull sconecuratedimages/sconetainer:noshielding  We can now update the service as follows:  $ scone service update --image  localhost:5000/sconetainer sconeweb", 
            "title": "Updating the image of a service"
        }, 
        {
            "location": "/SCONE_Swarm_Example/#draining-a-node", 
            "text": "To be able to drain all containers from a node, we need to figure out the node's id. We can do this manually by executing the following command on the leader node:   sudo docker node ls\nID                            HOSTNAME            STATUS              AVAILABILITY        MANAGER STATUS\n91a1vvex4dgozfrzy1y136gmg *   alice               Ready               Active              Leader\njhrayos9ylu02egwvkxpqtbwb     beatrix             Ready               Active                You can now take node  alice  out of service by executing:   sudo docker node update --availability drain 91a1vvex4dgozfrzy1y136gmg  To put the node back in service by executing:   sudo docker node update --availability active 91a1vvex4dgozfrzy1y136gmg    scontain.com , November 2017.  Questions or Suggestions?", 
            "title": "Draining a node"
        }, 
        {
            "location": "/SCONE_Compose/", 
            "text": "Compose / Stack Files\n\n\nSCONE supports to run secure applications  consisting of multiple secure containers. To do so, SCONE introduces slightly extended Docker \ncompose\n file. Such an extended compose file defines for each process that runs inside of an enclave, a unique hash value (\nMRENCLAVE\n). During startup, SCONE performs an \nattestation\n for all these \nsecure processes\n to ensure that the hash of the started program is as expected, i.e., is equal to \nMRENCLAVE\n. Only if it is equal, the arguments and the environment variables are passed to the process. In this way, we can pass secrets as arguments or environment variables in a secure fashion to a secure process.\n\n\nBy default, all containers are started with SCONE and the arguments are passed in a secure fashion to the started processes. However, you might not want to run all processes inside of enclaves. For containers that should be directly started with Docker Swarm, you need to set field \nnot_scone: \"true\"\n. In this case, all arguments and the environment variables are directly passed to the container with Docker Swarm (instead of SCONE).\n\n\nLet's consider an example, that consists of a \nhaproxy\n that runs in native mode and is directly started with Docker Swarm (indicated by line \nnot_scone: \"true\"\n) Moreover, we start \nmyapp\n (defined in container image \nmyapp-image\n) and specify \nMRENCLAVE\n. Only after ensuring that the program started in the enclave has the expected \nMRENCLAVE\n the arguments and the environment variables are passed to \nmyapp\n.\n\n\nversion\n:\n \n3.1.scone\n\n\nservices\n:\n\n    \nprimary-service\n:\n\n        \nimage\n:\n \nmyapp-image:latest\n\n        \ncommand\n:\n \n/myapp arg1 arg2 arg3 $my_password\n\n        \nmrenclave\n:\n \n5764436f08dd4cdb526f082be1a07a3422f79ef2b01a5e24f78f9034a838c335\n\n        \nenvironment\n:\n\n            \n-\n \nSECURE_ENV=value\n\n            \n-\n \nMY_PIN=$my_pin\n\n            \n-\n \nMY_PASSWORD=$my_password\n\n        \nworking_dir\n:\n \n/\n\n    \nproxy\n:\n\n        \nimage\n:\n \nhaproxy\n\n        \ncommand\n:\n \nhaproxy --read_config_from_environ\n\n        \nnot_scone\n:\n \ntrue\n\n        \nenvironment\n:\n\n            \n-\n \nHAPROXY_CONFIG=a=b,c=d,3=4\n\n\nsecrets\n:\n\n    \nmy_pin\n:\n\n        \nkind\n:\n \nnumeric\n\n        \nlength\n:\n \n4\n\n    \nmy_password\n:\n\n        \nkind\n:\n \nascii\n\n        \nlength\n:\n \n8\n\n\n\n\n\nThe extended compose file is split by SCONE into a standard compose file and a configuration information that is stored in the SCONE \nConfigruation and Attestation Service\n (\nCAS\n):\n\n\n\n\nIn the current version of scone, we assume that the compose file is stored on a trusted host. (We plan to support compose files stored on untrusted host in the near future.)\n\n\nThe \nsplit\n functionality is part of the \nscone CLI\n, i.e., you can split a compose file via\n\n\n$ scone cas split \nCOMPOSE_FILE\n --stack \nSTACK_ID\n\n\n\n\n\nThe stack ID is a randomly chose unique ID.\n\n\nThis creates a stack file that can be used to start a set of services in your Swarm. Read \nnginx example\n for some more details how to do this.\n\n\nTo be able to use a CAS, you are required to login to a CAS. We assume that your CAS is available at IP address \n$IP\n. If you have set up the CAS in the way specified in \nCAS Setup\n, set environment variable IP to the external IP address of the swarm manager node.\n\n\nYou can login into CAS via:\n\n\n$ scone cas login -c CA.PEM YOURID CASALIAS --host \n$IP\n:8081:18765\n\n\n\n\nReplace \nYOURID\n by your desired user id and \nCASALIAS\n by a name that you want to use to refer to this cas (see \nexample\n).\n\n\nSee \nnginx example\n for some more details and read \nCAS Setup\n to learn how to run a CAS service for development.\n\n\n \nscontain.com\n, March 2018. \nQuestions or Suggestions?", 
            "title": "SCONE Compose"
        }, 
        {
            "location": "/SCONE_Compose/#compose-stack-files", 
            "text": "SCONE supports to run secure applications  consisting of multiple secure containers. To do so, SCONE introduces slightly extended Docker  compose  file. Such an extended compose file defines for each process that runs inside of an enclave, a unique hash value ( MRENCLAVE ). During startup, SCONE performs an  attestation  for all these  secure processes  to ensure that the hash of the started program is as expected, i.e., is equal to  MRENCLAVE . Only if it is equal, the arguments and the environment variables are passed to the process. In this way, we can pass secrets as arguments or environment variables in a secure fashion to a secure process.  By default, all containers are started with SCONE and the arguments are passed in a secure fashion to the started processes. However, you might not want to run all processes inside of enclaves. For containers that should be directly started with Docker Swarm, you need to set field  not_scone: \"true\" . In this case, all arguments and the environment variables are directly passed to the container with Docker Swarm (instead of SCONE).  Let's consider an example, that consists of a  haproxy  that runs in native mode and is directly started with Docker Swarm (indicated by line  not_scone: \"true\" ) Moreover, we start  myapp  (defined in container image  myapp-image ) and specify  MRENCLAVE . Only after ensuring that the program started in the enclave has the expected  MRENCLAVE  the arguments and the environment variables are passed to  myapp .  version :   3.1.scone  services : \n     primary-service : \n         image :   myapp-image:latest \n         command :   /myapp arg1 arg2 arg3 $my_password \n         mrenclave :   5764436f08dd4cdb526f082be1a07a3422f79ef2b01a5e24f78f9034a838c335 \n         environment : \n             -   SECURE_ENV=value \n             -   MY_PIN=$my_pin \n             -   MY_PASSWORD=$my_password \n         working_dir :   / \n     proxy : \n         image :   haproxy \n         command :   haproxy --read_config_from_environ \n         not_scone :   true \n         environment : \n             -   HAPROXY_CONFIG=a=b,c=d,3=4  secrets : \n     my_pin : \n         kind :   numeric \n         length :   4 \n     my_password : \n         kind :   ascii \n         length :   8   The extended compose file is split by SCONE into a standard compose file and a configuration information that is stored in the SCONE  Configruation and Attestation Service  ( CAS ):   In the current version of scone, we assume that the compose file is stored on a trusted host. (We plan to support compose files stored on untrusted host in the near future.)  The  split  functionality is part of the  scone CLI , i.e., you can split a compose file via  $ scone cas split  COMPOSE_FILE  --stack  STACK_ID   The stack ID is a randomly chose unique ID.  This creates a stack file that can be used to start a set of services in your Swarm. Read  nginx example  for some more details how to do this.  To be able to use a CAS, you are required to login to a CAS. We assume that your CAS is available at IP address  $IP . If you have set up the CAS in the way specified in  CAS Setup , set environment variable IP to the external IP address of the swarm manager node.  You can login into CAS via:  $ scone cas login -c CA.PEM YOURID CASALIAS --host  $IP :8081:18765  Replace  YOURID  by your desired user id and  CASALIAS  by a name that you want to use to refer to this cas (see  example ).  See  nginx example  for some more details and read  CAS Setup  to learn how to run a CAS service for development.    scontain.com , March 2018.  Questions or Suggestions?", 
            "title": "Compose / Stack Files"
        }, 
        {
            "location": "/SCONE_CAS/", 
            "text": "SCONE CAS\n\n\nSCONE CAS (\nConfiguration and Attestation Service\n) helps to securely configure secure services. A CAS helps to provide services running inside of enclaves with\n\n\n\n\ntheir command line arguments\n\n\ntheir environment variables\n\n\n\n\nafter the service was attested by the CAS. \n\n\nTo attest a service in a swarm, the CAS requires the help of a local attestation service (LAS).\n\n\nAs part of the SCONE Enterprise Version, you can run your own CAS and LAS infrastructure.\n To do so, we provide you with container images to simplify the execution of CAS and LAS. For other SCONE versions, we can provide you with access to a global CAS.\n\n\nCAS Development Environment\n\n\nTo set up a development environment with CAS, you can perform the following steps.\n\n\nWe assume that you run the following commands inside of a container with the scone CLI like \nsconecuratedimages/sconecli\n. Also, we assume that you have access to a Docker swarm managed by some node called \nfaye\n. \n\n\nFirst, check that the swarm is properly installed:\n\n\n$ \nexport\n \nSCONE_MANAGER\n=\nfaye\n$ scone swarm ls\nNODENO        SGX VERSION   DOCKER-ENGINE SGX-DRIVER    HOST                 STATUS     AVAILABILITY  MANAGER   \n\n1\n             \n1\n             SCONE         SCONE         edna                 Ready      Active        Reachable \n\n1\n             \n1\n             SCONE         SCONE         faye                 Ready      Active        Leader    \n\n\n\n\nIf \nscone swarm ls\n issues some warnings or there are no SGX-capable machines listed, you might want to run \nscone swarm check\n to update the labels of the swarm nodes.\n\n\nTo run LAS on all nodes of a cluster managed by node \nfaye\n, pull the newest LAS image and then execute on all nodes of a SWARM:\n\n\n$ scone service pull sconecuratedimages/sconetainer:las\n$ scone service create --detach\n=\ntrue\n --mode global --publish \nmode\n=\nhost,target\n=\n18766\n,published\n=\n18766\n --name\n=\nlas localhost:5000/sconetainer:las\n\n\n\n\nCheck that the service \nlas\n is indeed running, check with command \nps\n:\n\n$ scone service ps las\nID                  NAME                            IMAGE                            NODE                DESIRED STATE       CURRENT STATE           ERROR               PORTS\nxvnsruar64qx        las.q26sp44pp12uf81zlyhb5pnxf   localhost:5000/sconetainer:las   edna                Running             Running \n3\n minutes ago                       *:18766-\n18766/tcp\nzfgx4t292ew6        las.cr3bxhy0nqmg77goxaih5sw8d   localhost:5000/sconetainer:las   dorothy             Running             Running \n3\n minutes ago                       *:18766-\n18766/tcp\nn441o9qqmnwa        las.os7ihjrel2jb4bplcn81h7f0i   localhost:5000/sconetainer:las   faye                Running             Running \n3\n minutes ago                       *:18766-\n18766/tcp\n\n\n\nNow, we can start the CAS service as follows. We first pull the newest\nCAS image and run it on one node of the swarm.\n\n\n$ scone service pull sconecuratedimages/sconetainer:cas\n$ scone service create --name cas --detach\n=\ntrue\n  --publish \n8081\n:8081 --publish \n18765\n:18765 localhost:5000/sconetainer:cas\n\n\n\n\nLet's check that CAS is indeed running:\n\n\n$ scone service ps cas\nID                  NAME                IMAGE                            NODE                DESIRED STATE       CURRENT STATE         ERROR               PORTS\ne6b0skuph9ve        cas.1               localhost:5000/sconetainer:cas   edna                Running             Running \n3\n hours ago                       \n\n\n\n\n \nscontain.com\n, January 2018. \nQuestions or Suggestions?", 
            "title": "SCONE CAS"
        }, 
        {
            "location": "/SCONE_CAS/#scone-cas", 
            "text": "SCONE CAS ( Configuration and Attestation Service ) helps to securely configure secure services. A CAS helps to provide services running inside of enclaves with   their command line arguments  their environment variables   after the service was attested by the CAS.   To attest a service in a swarm, the CAS requires the help of a local attestation service (LAS).  As part of the SCONE Enterprise Version, you can run your own CAS and LAS infrastructure.  To do so, we provide you with container images to simplify the execution of CAS and LAS. For other SCONE versions, we can provide you with access to a global CAS.", 
            "title": "SCONE CAS"
        }, 
        {
            "location": "/SCONE_CAS/#cas-development-environment", 
            "text": "To set up a development environment with CAS, you can perform the following steps.  We assume that you run the following commands inside of a container with the scone CLI like  sconecuratedimages/sconecli . Also, we assume that you have access to a Docker swarm managed by some node called  faye .   First, check that the swarm is properly installed:  $  export   SCONE_MANAGER = faye\n$ scone swarm ls\nNODENO        SGX VERSION   DOCKER-ENGINE SGX-DRIVER    HOST                 STATUS     AVAILABILITY  MANAGER    1               1              SCONE         SCONE         edna                 Ready      Active        Reachable  1               1              SCONE         SCONE         faye                 Ready      Active        Leader      If  scone swarm ls  issues some warnings or there are no SGX-capable machines listed, you might want to run  scone swarm check  to update the labels of the swarm nodes.  To run LAS on all nodes of a cluster managed by node  faye , pull the newest LAS image and then execute on all nodes of a SWARM:  $ scone service pull sconecuratedimages/sconetainer:las\n$ scone service create --detach = true  --mode global --publish  mode = host,target = 18766 ,published = 18766  --name = las localhost:5000/sconetainer:las  Check that the service  las  is indeed running, check with command  ps : $ scone service ps las\nID                  NAME                            IMAGE                            NODE                DESIRED STATE       CURRENT STATE           ERROR               PORTS\nxvnsruar64qx        las.q26sp44pp12uf81zlyhb5pnxf   localhost:5000/sconetainer:las   edna                Running             Running  3  minutes ago                       *:18766- 18766/tcp\nzfgx4t292ew6        las.cr3bxhy0nqmg77goxaih5sw8d   localhost:5000/sconetainer:las   dorothy             Running             Running  3  minutes ago                       *:18766- 18766/tcp\nn441o9qqmnwa        las.os7ihjrel2jb4bplcn81h7f0i   localhost:5000/sconetainer:las   faye                Running             Running  3  minutes ago                       *:18766- 18766/tcp  Now, we can start the CAS service as follows. We first pull the newest\nCAS image and run it on one node of the swarm.  $ scone service pull sconecuratedimages/sconetainer:cas\n$ scone service create --name cas --detach = true   --publish  8081 :8081 --publish  18765 :18765 localhost:5000/sconetainer:cas  Let's check that CAS is indeed running:  $ scone service ps cas\nID                  NAME                IMAGE                            NODE                DESIRED STATE       CURRENT STATE         ERROR               PORTS\ne6b0skuph9ve        cas.1               localhost:5000/sconetainer:cas   edna                Running             Running  3  hours ago                           scontain.com , January 2018.  Questions or Suggestions?", 
            "title": "CAS Development Environment"
        }, 
        {
            "location": "/SCONE_EE2EE/", 
            "text": "Example: End-to-End Encryption\n\n\nWe show how to encrypt an application end-to-end, i.e., all the data in the file system is encrypted, all data in main memory is encrypted and all data on the wire is encrypted.\n\n\nIn what follows, we only interact with one swarm. Hence, we set environment variable \nSCONE_MANAGER\n to point to the manager node of the swarm. Say, in our case the manage is called \nfaye\n:\n\n\n$ \nexport\n \nSCONE_MANAGER\n=\nfaye\n\n\n\n\nWe have prepared an nginx image that contains this website as encrypted files and nginx is running inside of an enclave reading the encrypted files\nfrom the file system and decrypting these files inside the enclave.\n\n\nLet's assume that we have the newest image in our local swarm registry:\n\n\n$ scone service pull sconecuratedimages/sconetainer:fss\n...\nfss: digest: sha256:2e76b4a0b090cc75c2e56594e20c7ec36bc1abd13e36e2f00d36e2f67b13c1d5 size: \n1783\n\nnew tag: localhost:5000/sconetainer:fss\n\n\n\n\nWe will start this service as a stack. Hence, we define a simple stack/compose file:\n\n\n$ cat \n compose.yml \n EOF\n\n\nversion: \n3.1.scone\n\n\nservices:\n\n\n    nginx:\n\n\n        image: 127.0.0.1:5000/sconetainer:fss\n\n\n        command: nginx -p /nginx -c nginx.conf\n\n\n        mrenclave: 1516e3d41590cf3842282c6f037535af64336138a6b5eff7e8754e97b4c64ecb\n\n\n        fspf_path: /nginx/fspf.pb\n\n\n        fspf_key: 970f4925bb7b221461f3d1a3f17450aa42844539de24f5acc1b45b8c140f9467\n\n\n        fspf_tag: 5930bffbd9ea2f1317e6872b032334db\n\n\n        working_dir: /\n\n\n        ports:\n\n\n          - 8190:8080\n\n\n          - 8192:8082\n\n\nEOF\n\n\n\n\n\nWe explain in section \nMrEnclave\n how to determine mrenclave.\n\n\nFor now, this file contains some metadata related to the encrypted files:\n\n\n\n\nfspf_path\n the path of the \nfile system protection file\n\n\nfspf_key\n the key used to encrypt the \nfile system protection file\n\n\nfspf_tag\n the tag (i.e., MAC) of the \nfile system protection file\n\n\n\n\nWe provide a low level for encrypting files with the help of \nscone fspf\n. (We will soon release a higher level support that will simplify the encryption of files). \n\n\nIn this swarm, we have a CAS (Configuration and Attestation Service running): the CAS helps\nus to to pass some secrets like the \nfspf_key\n to an enclave and protecting both\nthe confidentiality as well as the integrity of this secret.\n\n\nClients can identify the CAS via its certificate. In this example,\nwe explicitly specify the certificate of the CAS:\n\n\n$ cat \n ca.pem \n EOF\n\n\n-----BEGIN CERTIFICATE-----\n\n\nMIICKDCCAc2gAwIBAgIJAMnmUBMT+gOGMAoGCCqGSM49BAMCMHAxCzAJBgNVBAYT\n\n\nAlVTMQ8wDQYDVQQIDAZPcmVnb24xETAPBgNVBAcMCFBvcnRsYW5kMRUwEwYDVQQK\n\n\nDAxDb21wYW55IE5hbWUxDDAKBgNVBAsMA09yZzEYMBYGA1UEAwwPd3d3LmV4YW1w\n\n\nbGUuY29tMB4XDTE3MDQwNzEzNTIyMloXDTE4MDQwNzEzNTIyMlowcDELMAkGA1UE\n\n\nBhMCVVMxDzANBgNVBAgMBk9yZWdvbjERMA8GA1UEBwwIUG9ydGxhbmQxFTATBgNV\n\n\nBAoMDENvbXBhbnkgTmFtZTEMMAoGA1UECwwDT3JnMRgwFgYDVQQDDA93d3cuZXhh\n\n\nbXBsZS5jb20wWTATBgcqhkjOPQIBBggqhkjOPQMBBwNCAARvkkSOfCGXIyWCCdfj\n\n\nMil6dhw/a2ZsPaBKOl1RS5MZuoN+7gtSqgnaT5KrT4S/Wj7flP+PJqkUf0M6p1gg\n\n\nE5wBo1AwTjAdBgNVHQ4EFgQUzPP33ZM7ZMQ7UDb9THNM5Yoma3YwHwYDVR0jBBgw\n\n\nFoAUzPP33ZM7ZMQ7UDb9THNM5Yoma3YwDAYDVR0TBAUwAwEB/zAKBggqhkjOPQQD\n\n\nAgNJADBGAiEA34tSQI8V+4UaQG1w7nVJEDLWA17zw5Ls8NZBYVi4lpcCIQDgzln/\n\n\n1r25fhGzrVbRgAcbmfLsJxuDYpsfAaNL5/5leg==\n\n\n-----END CERTIFICATE-----\n\n\nEOF\n\n\n\n\n\nand now log into the CAS:\n\n\n$ \nexport\n \nIP\n=\nx.y.z.u\n$ mkdir -p ~/.config\n$ rm -f ~/.config/scone_cmd.conf\n$ scone cas login -c ca.pem christof cas --host \n$IP\n:8081:18765\n\n\n\n\nWe split our stack file into two parts: public part sent to the docker engine and a secret part that is directly sent to the CAS:\n\n\n$ scone cas split compose.yml  --stack \n283299\n\n\n\n\n\nFor now, the stack ID is a randomly chose unique ID. If you want to modify this stack later, you need to use the same stack ID.  \n\n\nShow public part that is sent to \ndocker stack\n (which is the original stack file name appended with \"docker.yml\"):\n\n\n$ cat compose.yml.docker.yml\n---\nservices: \n  nginx: \n    command: nginx\n    environment: \n      SCONE_CAS_ADDR: \nx.y.z.u:18765\n\n      SCONE_CONFIG_ID: christof/283299/nginx\n      SCONE_LAS_ADDR: \n172.17.0.1:18766\n\n    image: \n127.0.0.1:5000/sconetainer:fss\n\n    ports: \n      - \n8190:8080\n\n      - \n8192:8082\n\nversion: \n3.1\n\n\n\n\n\nNote: in the above example we assume that the host on which nginx is running is available at address  \n172.17.0.1\n. Hence, the \nLAS\n is available at  address \n\"172.17.0.1:18766\n. In case you run a non-standard configuration, you might need to change  \nSCONE_LAS_ADDR\n to the ip address and port where the LAS  is available. The LAS must run on the same host for the attestation to work.\n\n\nWe can now deploy the service with the help of docker/scone stack:\n\n\n$ scone stack deploy --compose-file compose.yml  nginx\nCreating network nginx_default\nCreating service nginx_nginx\n\n\n\n\nShow running stack:\n\n\n$ scone stack ls\nNAME        SERVICES\nnginx       \n1\n\n\n\n\n\nWe can get some more information about this stack:\n\n\n$ scone stack ps nginx\nID                  NAME                IMAGE                            NODE                DESIRED STATE       CURRENT STATE             ERROR                       PORTS\nh08r5hmeu9bt        nginx_nginx.1       \n127\n.0.0.1:5000/sconetainer:fss   dorothy             Running             Starting \n19\n seconds ago       \n\n\n\n\nFile Encryption\n\n\nLet's check that the files inside of nginx container are indeed encrypted. To do so, we ssh to node  \ndorothy\n and execute the following commands:\n\n\n sudo docker ps\nCONTAINER ID        IMAGE                                    COMMAND                  CREATED             STATUS              PORTS                      NAMES\nb01ab65e1579        \n127\n.0.0.1:5000/sconetainer:fss           \nnginx\n                  \n6\n minutes ago       Up \n6\n minutes        \n8080\n/tcp, \n8082\n/tcp         nginx_nginx.1.0wh80346mpmm5f3l1rc8dipix\n...\n\n\n\n\nLet's look inside the container:\n\n\n sudo docker \nexec\n -it b01ab65e1579 sh\n$ ls\nbin             media           proc            srv\ndev             mnt             root            sys\netc             nginx           run             tmp\nhome            nginx-etc       sbin            usr\nlib             opt             scone-test.key  var\n\n\n\n\nThe nginx configuration file as well as the html files should be encrypted. Let's verify this:\n\n\n$ \ncd\n nginx\n$ ls\ncertificate.pem  key.pem          nginx.conf       www_root\nfspf.pb          mime.types       nginx.pid\n$ head -c \n80\n nginx.conf\n\n]\nP?\u025f\n4???zS????\u01b5?kj\u0198?Ec?!S^!??????8j-?e;?t\n?2?L????????y??\u02f2?\u07cb   \n\n\n\n\n\nOk, the nginx configuration file seems to be encrypted. Now, look at the html files too:\n\n\n$ \ncd\n www_root\n$ ls \n4K                               SCONE_TUTORIAL\nGO                               aboutScone\nPython                           appsecurity\n...\n$ head -c \n80\n index.html \n????V?\u036a?rhk?\nk????\n$?\n    .???k?\u042c?y\n??\u0660n?+????P2?G;o_\n.i?);?I??xFg[???[f?\n\n\n\n\n\n \nscontain.com\n, December 2017. \nQuestions or Suggestions?", 
            "title": "Example E2E encryption"
        }, 
        {
            "location": "/SCONE_EE2EE/#example-end-to-end-encryption", 
            "text": "We show how to encrypt an application end-to-end, i.e., all the data in the file system is encrypted, all data in main memory is encrypted and all data on the wire is encrypted.  In what follows, we only interact with one swarm. Hence, we set environment variable  SCONE_MANAGER  to point to the manager node of the swarm. Say, in our case the manage is called  faye :  $  export   SCONE_MANAGER = faye  We have prepared an nginx image that contains this website as encrypted files and nginx is running inside of an enclave reading the encrypted files\nfrom the file system and decrypting these files inside the enclave.  Let's assume that we have the newest image in our local swarm registry:  $ scone service pull sconecuratedimages/sconetainer:fss\n...\nfss: digest: sha256:2e76b4a0b090cc75c2e56594e20c7ec36bc1abd13e36e2f00d36e2f67b13c1d5 size:  1783 \nnew tag: localhost:5000/sconetainer:fss  We will start this service as a stack. Hence, we define a simple stack/compose file:  $ cat   compose.yml   EOF  version:  3.1.scone  services:      nginx:          image: 127.0.0.1:5000/sconetainer:fss          command: nginx -p /nginx -c nginx.conf          mrenclave: 1516e3d41590cf3842282c6f037535af64336138a6b5eff7e8754e97b4c64ecb          fspf_path: /nginx/fspf.pb          fspf_key: 970f4925bb7b221461f3d1a3f17450aa42844539de24f5acc1b45b8c140f9467          fspf_tag: 5930bffbd9ea2f1317e6872b032334db          working_dir: /          ports:            - 8190:8080            - 8192:8082  EOF   We explain in section  MrEnclave  how to determine mrenclave.  For now, this file contains some metadata related to the encrypted files:   fspf_path  the path of the  file system protection file  fspf_key  the key used to encrypt the  file system protection file  fspf_tag  the tag (i.e., MAC) of the  file system protection file   We provide a low level for encrypting files with the help of  scone fspf . (We will soon release a higher level support that will simplify the encryption of files).   In this swarm, we have a CAS (Configuration and Attestation Service running): the CAS helps\nus to to pass some secrets like the  fspf_key  to an enclave and protecting both\nthe confidentiality as well as the integrity of this secret.  Clients can identify the CAS via its certificate. In this example,\nwe explicitly specify the certificate of the CAS:  $ cat   ca.pem   EOF  -----BEGIN CERTIFICATE-----  MIICKDCCAc2gAwIBAgIJAMnmUBMT+gOGMAoGCCqGSM49BAMCMHAxCzAJBgNVBAYT  AlVTMQ8wDQYDVQQIDAZPcmVnb24xETAPBgNVBAcMCFBvcnRsYW5kMRUwEwYDVQQK  DAxDb21wYW55IE5hbWUxDDAKBgNVBAsMA09yZzEYMBYGA1UEAwwPd3d3LmV4YW1w  bGUuY29tMB4XDTE3MDQwNzEzNTIyMloXDTE4MDQwNzEzNTIyMlowcDELMAkGA1UE  BhMCVVMxDzANBgNVBAgMBk9yZWdvbjERMA8GA1UEBwwIUG9ydGxhbmQxFTATBgNV  BAoMDENvbXBhbnkgTmFtZTEMMAoGA1UECwwDT3JnMRgwFgYDVQQDDA93d3cuZXhh  bXBsZS5jb20wWTATBgcqhkjOPQIBBggqhkjOPQMBBwNCAARvkkSOfCGXIyWCCdfj  Mil6dhw/a2ZsPaBKOl1RS5MZuoN+7gtSqgnaT5KrT4S/Wj7flP+PJqkUf0M6p1gg  E5wBo1AwTjAdBgNVHQ4EFgQUzPP33ZM7ZMQ7UDb9THNM5Yoma3YwHwYDVR0jBBgw  FoAUzPP33ZM7ZMQ7UDb9THNM5Yoma3YwDAYDVR0TBAUwAwEB/zAKBggqhkjOPQQD  AgNJADBGAiEA34tSQI8V+4UaQG1w7nVJEDLWA17zw5Ls8NZBYVi4lpcCIQDgzln/  1r25fhGzrVbRgAcbmfLsJxuDYpsfAaNL5/5leg==  -----END CERTIFICATE-----  EOF   and now log into the CAS:  $  export   IP = x.y.z.u\n$ mkdir -p ~/.config\n$ rm -f ~/.config/scone_cmd.conf\n$ scone cas login -c ca.pem christof cas --host  $IP :8081:18765  We split our stack file into two parts: public part sent to the docker engine and a secret part that is directly sent to the CAS:  $ scone cas split compose.yml  --stack  283299   For now, the stack ID is a randomly chose unique ID. If you want to modify this stack later, you need to use the same stack ID.    Show public part that is sent to  docker stack  (which is the original stack file name appended with \"docker.yml\"):  $ cat compose.yml.docker.yml\n---\nservices: \n  nginx: \n    command: nginx\n    environment: \n      SCONE_CAS_ADDR:  x.y.z.u:18765 \n      SCONE_CONFIG_ID: christof/283299/nginx\n      SCONE_LAS_ADDR:  172.17.0.1:18766 \n    image:  127.0.0.1:5000/sconetainer:fss \n    ports: \n      -  8190:8080 \n      -  8192:8082 \nversion:  3.1   Note: in the above example we assume that the host on which nginx is running is available at address   172.17.0.1 . Hence, the  LAS  is available at  address  \"172.17.0.1:18766 . In case you run a non-standard configuration, you might need to change   SCONE_LAS_ADDR  to the ip address and port where the LAS  is available. The LAS must run on the same host for the attestation to work.  We can now deploy the service with the help of docker/scone stack:  $ scone stack deploy --compose-file compose.yml  nginx\nCreating network nginx_default\nCreating service nginx_nginx  Show running stack:  $ scone stack ls\nNAME        SERVICES\nnginx        1   We can get some more information about this stack:  $ scone stack ps nginx\nID                  NAME                IMAGE                            NODE                DESIRED STATE       CURRENT STATE             ERROR                       PORTS\nh08r5hmeu9bt        nginx_nginx.1        127 .0.0.1:5000/sconetainer:fss   dorothy             Running             Starting  19  seconds ago", 
            "title": "Example: End-to-End Encryption"
        }, 
        {
            "location": "/SCONE_EE2EE/#file-encryption", 
            "text": "Let's check that the files inside of nginx container are indeed encrypted. To do so, we ssh to node   dorothy  and execute the following commands:   sudo docker ps\nCONTAINER ID        IMAGE                                    COMMAND                  CREATED             STATUS              PORTS                      NAMES\nb01ab65e1579         127 .0.0.1:5000/sconetainer:fss            nginx                    6  minutes ago       Up  6  minutes         8080 /tcp,  8082 /tcp         nginx_nginx.1.0wh80346mpmm5f3l1rc8dipix\n...  Let's look inside the container:   sudo docker  exec  -it b01ab65e1579 sh\n$ ls\nbin             media           proc            srv\ndev             mnt             root            sys\netc             nginx           run             tmp\nhome            nginx-etc       sbin            usr\nlib             opt             scone-test.key  var  The nginx configuration file as well as the html files should be encrypted. Let's verify this:  $  cd  nginx\n$ ls\ncertificate.pem  key.pem          nginx.conf       www_root\nfspf.pb          mime.types       nginx.pid\n$ head -c  80  nginx.conf ] P?\u025f 4???zS????\u01b5?kj\u0198?Ec?!S^!??????8j-?e;?t ?2?L????????y??\u02f2?\u07cb      Ok, the nginx configuration file seems to be encrypted. Now, look at the html files too:  $  cd  www_root\n$ ls \n4K                               SCONE_TUTORIAL\nGO                               aboutScone\nPython                           appsecurity\n...\n$ head -c  80  index.html \n????V?\u036a?rhk? k???? $?     .???k?\u042c?y ??\u0660n?+????P2?G;o_ .i?);?I??xFg[???[f?     scontain.com , December 2017.  Questions or Suggestions?", 
            "title": "File Encryption"
        }, 
        {
            "location": "/C/", 
            "text": "C Program Language Support\n\n\nSCONE supports native compilation combined with dynamic linking as well as cross-compilation to support, in particular, statically linked binaries.\n\n\nThis page focuses on the SCONE cross compiler. This cross compiler is based on gcc and hence, the command line options are the same as gcc.\n\n\nImage\n\n\nEnsure that you have the newest SCONE cross compiler image:\n\n\n docker pull sconecuratedimages/crosscompilers:scone\n\n docker run -it sconecuratedimages/crosscompilers:scone\n$\n\n\n\n\nHelp\n\n\nIf you need some help, just execute in the container:\n\n\n$ scone gcc --help\nUsage: x86_64-linux-musl-gcc \n[\noptions\n]\n file...\nOptions:\n...\n\n\n\n\nExample\n\n\nLet's try to compile a simple program:\n\n\n$ cat \n fib.c \n EOF\n\n\n#include \nstdio.h\n\n\n#include \nstdlib.h\n\n\n\nint main(int argc, char** argv) {\n\n\n   int n=0, first = 0, second = 1, next = 0, c;\n\n\n\n   if (argc \n 1)\n\n\n        n=atoi(argv[1]);\n\n\n   printf(\nfib(%d)= 1\n,n);\n\n\n   for ( c = 1 ; c \n n ; c++ ) {\n\n\n        next = first + second;\n\n\n        first = second;\n\n\n        second = next;\n\n\n        printf(\n, %d\n,next);\n\n\n   }\n\n\n   printf(\n\\n\n);\n\n\n}\n\n\nEOF\n\n\n\n\n\nWe compile the program with \nscone gcc\n or \nscone-gcc\n:\n\n\n$ scone gcc fib.c -o fib\n\n\n\n\nTo compute fib(23), execute:\n\n\n$ \nSCONE_VERSION\n=\n1\n ./fib \n23\n\nfib\n(\n23\n)=\n \n1\n, \n1\n, \n2\n, \n3\n, \n5\n, \n8\n, \n13\n, \n21\n, \n34\n, \n55\n, \n89\n, \n144\n, \n233\n, \n377\n, \n610\n, \n987\n, \n1597\n, \n2584\n, \n4181\n, \n6765\n, \n10946\n, \n17711\n, \n28657\n\n\n\n\n\n \nscontain.com\n, December 2017. \nQuestions or Suggestions?", 
            "title": "C"
        }, 
        {
            "location": "/C/#c-program-language-support", 
            "text": "SCONE supports native compilation combined with dynamic linking as well as cross-compilation to support, in particular, statically linked binaries.  This page focuses on the SCONE cross compiler. This cross compiler is based on gcc and hence, the command line options are the same as gcc.", 
            "title": "C Program Language Support"
        }, 
        {
            "location": "/C/#image", 
            "text": "Ensure that you have the newest SCONE cross compiler image:   docker pull sconecuratedimages/crosscompilers:scone  docker run -it sconecuratedimages/crosscompilers:scone\n$", 
            "title": "Image"
        }, 
        {
            "location": "/C/#help", 
            "text": "If you need some help, just execute in the container:  $ scone gcc --help\nUsage: x86_64-linux-musl-gcc  [ options ]  file...\nOptions:\n...", 
            "title": "Help"
        }, 
        {
            "location": "/C/#example", 
            "text": "Let's try to compile a simple program:  $ cat   fib.c   EOF  #include  stdio.h  #include  stdlib.h  int main(int argc, char** argv) {     int n=0, first = 0, second = 1, next = 0, c;     if (argc   1)          n=atoi(argv[1]);     printf( fib(%d)= 1 ,n);     for ( c = 1 ; c   n ; c++ ) {          next = first + second;          first = second;          second = next;          printf( , %d ,next);     }     printf( \\n );  }  EOF   We compile the program with  scone gcc  or  scone-gcc :  $ scone gcc fib.c -o fib  To compute fib(23), execute:  $  SCONE_VERSION = 1  ./fib  23 \nfib ( 23 )=   1 ,  1 ,  2 ,  3 ,  5 ,  8 ,  13 ,  21 ,  34 ,  55 ,  89 ,  144 ,  233 ,  377 ,  610 ,  987 ,  1597 ,  2584 ,  4181 ,  6765 ,  10946 ,  17711 ,  28657     scontain.com , December 2017.  Questions or Suggestions?", 
            "title": "Example"
        }, 
        {
            "location": "/C++/", 
            "text": "C++ Program Language Support\n\n\nSCONE supports native compilation of C++ programs when combined with dynamic linking as well as cross-compilation.  Cross-compilation is required  to support, in particular, statically linked binaries.\n\n\nThis page focuses on the SCONE C++ cross compiler \nscone g++\n (a.k.a. \nscone-g++\n). This cross compiler is based on g++ and hence, the command line options are the same as those of g++.\n\n\nImage\n\n\nEnsure that you have the newest SCONE cross compiler image:\n\n\n docker pull sconecuratedimages/crosscompilers:scone\n\n docker run -it sconecuratedimages/crosscompilers:scone\n$\n\n\n\n\nHelp\n\n\nIf you need some help, just execute in the container:\n\n\n$ scone g++ --help\nUsage: x86_64-linux-musl-g++ \n[\noptions\n]\n file...\nOptions:\n...\n\n\n\n\nExample\n\n\nLet's try to compile a simple program:\n\n\n$ cat \n sqrt.cc \n EOF\n\n\n#include \niostream\n\n\n#include \ncmath\n\n\n\nusing namespace std;\n\n\n\nint main() {\n\n\n    int x = 0;\n\n\n    while(x \n 10) {\n\n\n        double y = sqrt((double)x);\n\n\n        cout \n \nThe square root of \n \n x \n \n is \n \n y \n endl;\n\n\n        x++;\n\n\n    }\n\n\n    return 0;\n\n\n}\n\n\nEOF\n\n\n\n\n\nWe compile the program with \nscone gcc\n or \nscone-gcc\n:\n\n\n$ scone g++ sqrt.cc -o sqrt\n\n\n\n\nLet's execute the binary:\n\n\n$ \nSCONE_VERSION\n=\n1\n ./sqrt\n\nexport\n \nSCONE_QUEUES\n=\n4\n\n\nexport\n \nSCONE_SLOTS\n=\n256\n\n\nexport\n \nSCONE_SIGPIPE\n=\n0\n\n\nexport\n \nSCONE_MMAP32BIT\n=\n0\n\n\nexport\n \nSCONE_SSPINS\n=\n100\n\n\nexport\n \nSCONE_SSLEEP\n=\n4000\n\n\nexport\n \nSCONE_KERNEL\n=\n0\n\n\nexport\n \nSCONE_HEAP\n=\n67108864\n\n\nexport\n \nSCONE_CONFIG\n=\n/etc/sgx-musl.conf\n\nexport\n \nSCONE_MODE\n=\nhw\n\nexport\n \nSCONE_SGXBOUNDS\n=\nno\n\nexport\n \nSCONE_VARYS\n=\nno\n\nexport\n \nSCONE_ALLOW_DLOPEN\n=\nno\n\nexport\n \nSCONE_ALLOW_DLOPEN2\n=\nno\n\nThe square root of \n0\n is \n0\n\nThe square root of \n1\n is \n1\n\nThe square root of \n2\n is \n1\n.41421\nThe square root of \n3\n is \n1\n.73205\nThe square root of \n4\n is \n2\n\nThe square root of \n5\n is \n2\n.23607\nThe square root of \n6\n is \n2\n.44949\nThe square root of \n7\n is \n2\n.64575\nThe square root of \n8\n is \n2\n.82843\nThe square root of \n9\n is \n3\n\n\n\n\n\n \nscontain.com\n, December 2017. \nQuestions or Suggestions?", 
            "title": "C++"
        }, 
        {
            "location": "/C++/#c-program-language-support", 
            "text": "SCONE supports native compilation of C++ programs when combined with dynamic linking as well as cross-compilation.  Cross-compilation is required  to support, in particular, statically linked binaries.  This page focuses on the SCONE C++ cross compiler  scone g++  (a.k.a.  scone-g++ ). This cross compiler is based on g++ and hence, the command line options are the same as those of g++.", 
            "title": "C++ Program Language Support"
        }, 
        {
            "location": "/C++/#image", 
            "text": "Ensure that you have the newest SCONE cross compiler image:   docker pull sconecuratedimages/crosscompilers:scone  docker run -it sconecuratedimages/crosscompilers:scone\n$", 
            "title": "Image"
        }, 
        {
            "location": "/C++/#help", 
            "text": "If you need some help, just execute in the container:  $ scone g++ --help\nUsage: x86_64-linux-musl-g++  [ options ]  file...\nOptions:\n...", 
            "title": "Help"
        }, 
        {
            "location": "/C++/#example", 
            "text": "Let's try to compile a simple program:  $ cat   sqrt.cc   EOF  #include  iostream  #include  cmath  using namespace std;  int main() {      int x = 0;      while(x   10) {          double y = sqrt((double)x);          cout    The square root of     x     is     y   endl;          x++;      }      return 0;  }  EOF   We compile the program with  scone gcc  or  scone-gcc :  $ scone g++ sqrt.cc -o sqrt  Let's execute the binary:  $  SCONE_VERSION = 1  ./sqrt export   SCONE_QUEUES = 4  export   SCONE_SLOTS = 256  export   SCONE_SIGPIPE = 0  export   SCONE_MMAP32BIT = 0  export   SCONE_SSPINS = 100  export   SCONE_SSLEEP = 4000  export   SCONE_KERNEL = 0  export   SCONE_HEAP = 67108864  export   SCONE_CONFIG = /etc/sgx-musl.conf export   SCONE_MODE = hw export   SCONE_SGXBOUNDS = no export   SCONE_VARYS = no export   SCONE_ALLOW_DLOPEN = no export   SCONE_ALLOW_DLOPEN2 = no\n\nThe square root of  0  is  0 \nThe square root of  1  is  1 \nThe square root of  2  is  1 .41421\nThe square root of  3  is  1 .73205\nThe square root of  4  is  2 \nThe square root of  5  is  2 .23607\nThe square root of  6  is  2 .44949\nThe square root of  7  is  2 .64575\nThe square root of  8  is  2 .82843\nThe square root of  9  is  3     scontain.com , December 2017.  Questions or Suggestions?", 
            "title": "Example"
        }, 
        {
            "location": "/Fortran/", 
            "text": "Fortran\n\n\nSCONE supports Fortran with the help of cross-compilation. This page focuses on the SCONE gfortran cross compiler \nscone gfortran\n (a.k.a. \nscone-gfortran\n). This cross compiler is based on gfortran and hence, the command line options are the same as those of gfortran.\n\n\nImage\n\n\nEnsure that you have the newest SCONE cross compiler image:\n\n\n docker pull sconecuratedimages/crosscompilers:scone\n\n docker run -it sconecuratedimages/crosscompilers:scone\n$\n\n\n\n\nHelp\n\n\nIf you need some help, just execute in the container:\n\n\n$ scone gfortran --help\nUsage: x86_64-linux-musl-gfortran \n[\noptions\n]\n file...\nOptions:\n...\n\n\n\n\nLet's try a simple Fortran program.\n\n\n$ cat \n gcd.f \n EOF\n\n\n*     euclid.f (FORTRAN 77)\n\n\n*     Find greatest common divisor using the Euclidean algorithm\n\n\n\n      PROGRAM EUCLID\n\n\n        PRINT *, \nA?\n\n\n        READ *, NA\n\n\n        IF (NA.LE.0) THEN\n\n\n          PRINT *, \nA must be a positive integer.\n\n\n          STOP\n\n\n        END IF\n\n\n        PRINT *, \nB?\n\n\n        READ *, NB\n\n\n        IF (NB.LE.0) THEN\n\n\n          PRINT *, \nB must be a positive integer.\n\n\n          STOP\n\n\n        END IF\n\n\n        PRINT *, \nThe GCD of\n, NA, \n and\n, NB, \n is\n, NGCD(NA, NB), \n.\n\n\n        STOP\n\n\n      END\n\n\n\n      FUNCTION NGCD(NA, NB)\n\n\n        IA = NA\n\n\n        IB = NB\n\n\n    1   IF (IB.NE.0) THEN\n\n\n          ITEMP = IA\n\n\n          IA = IB\n\n\n          IB = MOD(ITEMP, IB)\n\n\n          GOTO 1\n\n\n        END IF\n\n\n        NGCD = IA\n\n\n        RETURN\n\n\n      END\n\n\nEOF\n\n\n\n\n\nWe compile the program with \nscone gfortran\n (a.k.a. \nscone-gfortran\n):\n\n\n$ scone gfortran gcd.f -o gcd\n\n\n\n\nWe can now run this program as follows:\n\n\n$ \nSCONE_VERSION\n=\n1\n ./gcd \n EOF\n\n\n10\n\n\n15\n\n\nEOF\n\n\n\nexport\n \nSCONE_QUEUES\n=\n4\n\n\nexport\n \nSCONE_SLOTS\n=\n256\n\n\nexport\n \nSCONE_SIGPIPE\n=\n0\n\n\nexport\n \nSCONE_MMAP32BIT\n=\n0\n\n\nexport\n \nSCONE_SSPINS\n=\n100\n\n\nexport\n \nSCONE_SSLEEP\n=\n4000\n\n\nexport\n \nSCONE_KERNEL\n=\n0\n\n\nexport\n \nSCONE_HEAP\n=\n67108864\n\n\nexport\n \nSCONE_CONFIG\n=\n/etc/sgx-musl.conf\n\nexport\n \nSCONE_MODE\n=\nhw\n\nexport\n \nSCONE_SGXBOUNDS\n=\nno\n\nexport\n \nSCONE_VARYS\n=\nno\n\nexport\n \nSCONE_ALLOW_DLOPEN\n=\nno\n\nexport\n \nSCONE_ALLOW_DLOPEN2\n=\nno\n\n A?\n B?\n The GCD of          \n10\n  and          \n15\n  is           \n5\n .\n\n\n\n\n \nscontain.com\n, December 2017. \nQuestions or Suggestions?", 
            "title": "Fortran"
        }, 
        {
            "location": "/Fortran/#fortran", 
            "text": "SCONE supports Fortran with the help of cross-compilation. This page focuses on the SCONE gfortran cross compiler  scone gfortran  (a.k.a.  scone-gfortran ). This cross compiler is based on gfortran and hence, the command line options are the same as those of gfortran.", 
            "title": "Fortran"
        }, 
        {
            "location": "/Fortran/#image", 
            "text": "Ensure that you have the newest SCONE cross compiler image:   docker pull sconecuratedimages/crosscompilers:scone  docker run -it sconecuratedimages/crosscompilers:scone\n$", 
            "title": "Image"
        }, 
        {
            "location": "/Fortran/#help", 
            "text": "If you need some help, just execute in the container:  $ scone gfortran --help\nUsage: x86_64-linux-musl-gfortran  [ options ]  file...\nOptions:\n...  Let's try a simple Fortran program.  $ cat   gcd.f   EOF  *     euclid.f (FORTRAN 77)  *     Find greatest common divisor using the Euclidean algorithm        PROGRAM EUCLID          PRINT *,  A?          READ *, NA          IF (NA.LE.0) THEN            PRINT *,  A must be a positive integer.            STOP          END IF          PRINT *,  B?          READ *, NB          IF (NB.LE.0) THEN            PRINT *,  B must be a positive integer.            STOP          END IF          PRINT *,  The GCD of , NA,   and , NB,   is , NGCD(NA, NB),  .          STOP        END        FUNCTION NGCD(NA, NB)          IA = NA          IB = NB      1   IF (IB.NE.0) THEN            ITEMP = IA            IA = IB            IB = MOD(ITEMP, IB)            GOTO 1          END IF          NGCD = IA          RETURN        END  EOF   We compile the program with  scone gfortran  (a.k.a.  scone-gfortran ):  $ scone gfortran gcd.f -o gcd  We can now run this program as follows:  $  SCONE_VERSION = 1  ./gcd   EOF  10  15  EOF  export   SCONE_QUEUES = 4  export   SCONE_SLOTS = 256  export   SCONE_SIGPIPE = 0  export   SCONE_MMAP32BIT = 0  export   SCONE_SSPINS = 100  export   SCONE_SSLEEP = 4000  export   SCONE_KERNEL = 0  export   SCONE_HEAP = 67108864  export   SCONE_CONFIG = /etc/sgx-musl.conf export   SCONE_MODE = hw export   SCONE_SGXBOUNDS = no export   SCONE_VARYS = no export   SCONE_ALLOW_DLOPEN = no export   SCONE_ALLOW_DLOPEN2 = no\n\n A?\n B?\n The GCD of           10   and           15   is            5  .    scontain.com , December 2017.  Questions or Suggestions?", 
            "title": "Help"
        }, 
        {
            "location": "/GO/", 
            "text": "GO\n\n\nSCONE supports cross-compiling GO programs to run these inside of SGX enclaves. The GO cross-compiler is part of image \nsconecuratedimages/crosscompilers:scone\n.\n\n\nExample\n\n\nStart the SCONE crosscompiler container:\n\n\n docker run -it -p \n8080\n:8080 sconecuratedimages/crosscompilers:scone\n$\n\n\n\n\nLets consider a simple GO program (take from a \nGO tutorial\n):\n\n\n$ cat \n web-srv.go \n EOF\n\n\npackage main\n\n\n\nimport (\n\n\n    \nos\n\n\n    \nfmt\n\n\n    \nnet/http\n\n\n)\n\n\n\nfunc handler(w http.ResponseWriter, r *http.Request) {\n\n\n    fmt.Fprintf(w, \nHi there, I love %s!\\n\n, r.URL.Path[1:])\n\n\n    if r.URL.Path[1:] == \nEXIT\n {\n\n\n        os.Exit(0)\n\n\n    }\n\n\n}\n\n\n\nfunc main() {\n\n\n    http.HandleFunc(\n/\n, handler)\n\n\n    http.ListenAndServe(\n:8080\n, nil)\n\n\n}\n\n\nEOF\n\n\n\n\n\nYou can cross-compile this program as follows:\n\n\n$ \nSCONE_HEAP\n=\n1G scone-gccgo web-srv.go -O3 -o web-srv-go -g\n\n\n\n\nYou can start the compiled program (and enable some debug messages) as follows:\n\n\n$ \nSCONE_VERSION\n=\n1\n ./web-srv-go \n\n\nexport\n \nSCONE_QUEUES\n=\n4\n\n\nexport\n \nSCONE_SLOTS\n=\n256\n\n\nexport\n \nSCONE_SIGPIPE\n=\n0\n\n\nexport\n \nSCONE_MMAP32BIT\n=\n0\n\n\nexport\n \nSCONE_SSPINS\n=\n100\n\n\nexport\n \nSCONE_SSLEEP\n=\n4000\n\n\nexport\n \nSCONE_KERNEL\n=\n0\n\n\nexport\n \nSCONE_HEAP\n=\n1073741824\n\n\nexport\n \nSCONE_CONFIG\n=\n/etc/sgx-musl.conf\n\nexport\n \nSCONE_MODE\n=\nhw\n\nexport\n \nSCONE_SGXBOUNDS\n=\nno\n\nexport\n \nSCONE_ALLOW_DLOPEN\n=\nno\nRevision: 9b355b99170ad434010353bb9f4dca24e532b1b7\nBranch: master\nConfigure options: --enable-file-prot --enable-shared --enable-debug --prefix\n=\n/scone/src/built/cross-compiler/x86_64-linux-musl\n\n\n\n\nYou can now connect to port 8080, for example, with \ncurl\n:\n\n\n$ curl localhost:8080/SCONE\nHi there, I love SCONE!\n\n\n\n\nYou can terminate the server with\n\n\n$ curl localhost:8080/EXIT\ncurl: \n(\n52\n)\n Empty reply from server\n\n\n\n\nDebugging\n\n\nSCONE supports debugging of programs running inside of an enclave with the help of gdb.\n\n\nDebugging inside of a container\n\n\nStandard containers have not sufficient rights to use the debugger.  Hence, you must start a container with \nSYS_PTRACE\n capability. For example:\n\n\n$ docker run --cap-add SYS_PTRACE -it -p \n8080\n:8080 -v \n$PWD\n/EXAMPLE:/usr/src/myapp -w /usr/src/myapp  sconecuratedimages/crosscompilers:scone\n\n\n\n\nHandling Illegal instructions\n\n\nSome instructions, like CPUID, are not permitted inside of enclaves. For some of these instructions, like CPUID, we provide an automatic emulation. However, we recommend not to use any illegal instructions inside of enclaves despite having an automatic emulation of these instructions. For example, we provide static replacements of the CPUID instruction. \n\n\nIf your program contains some illegal instructions, you need to ask the debugger to forward the signals,\nthat these illegal instructions cause, to the program via \nhandle SIGILL nostop pass\n: \n\n\n$ scone-gdb ./web-srv-go\nGNU gdb \n(\nUbuntu \n7\n.12.50.20170314-0ubuntu1.1\n)\n \n7\n.12.50.20170314-git\nCopyright \n(\nC\n)\n \n2017\n Free Software Foundation, Inc.\nLicense GPLv3+: GNU GPL version \n3\n or later \nhttp://gnu.org/licenses/gpl.html\n\nThis is free software: you are free to change and redistribute it.\nThere is NO WARRANTY, to the extent permitted by law.  Type \nshow copying\n\nand \nshow warranty\n \nfor\n details.\nThis GDB was configured as \nx86_64-linux-gnu\n.\nType \nshow configuration\n \nfor\n configuration details.\nFor bug reporting instructions, please see:\n\nhttp://www.gnu.org/software/gdb/bugs/\n.\nFind the GDB manual and other documentation resources online at:\n\nhttp://www.gnu.org/software/gdb/documentation/\n.\nFor help, \ntype\n \nhelp\n.\nType \napropos word\n to search \nfor\n commands related to \nword\n...\nSource directories searched: /opt/scone/scone-gdb/gdb-sgxmusl-plugin:\n$cdir\n:\n$cwd\n\nSetting environment variable \nLD_PRELOAD\n to null value.\nReading symbols from ./web-srv-go...done.\n\n[\nSCONE\n]\n Initializing...\n\n(\ngdb\n)\n handle SIGILL nostop pass\nSignal        Stop  Print   Pass to program Description\nSIGILL        No    Yes Yes     Illegal instruction\n\n(\ngdb\n)\n\n\n\n\n\nSince we do not patch the CPUID instructions in this run, you will see something like this:\n\n\n(\ngdb\n)\n run\nStarting program: /usr/src/myapp/web-srv-go \nwarning: Error disabling address space randomization: Operation not permitted\n\n[\nThread debugging using libthread_db enabled\n]\n\nUsing host libthread_db library \n/lib/x86_64-linux-gnu/libthread_db.so.1\n.\n\n[\nSCONE\n]\n Enclave base: \n1000000000\n\n\n[\nSCONE\n]\n Loaded debug symbols\n\n[\nNew Thread 0x7f1786d26700 \n(\nLWP \n105\n)]\n\n\n[\nNew Thread 0x7f1786525700 \n(\nLWP \n106\n)]\n\n\n[\nNew Thread 0x7f1785d24700 \n(\nLWP \n107\n)]\n\n\n[\nNew Thread 0x7f1785523700 \n(\nLWP \n108\n)]\n\n\n[\nNew Thread 0x7f1787502700 \n(\nLWP \n109\n)]\n\n\n[\nNew Thread 0x7f17874fa700 \n(\nLWP \n110\n)]\n\n\n[\nNew Thread 0x7f17874f2700 \n(\nLWP \n111\n)]\n\n\n[\nNew Thread 0x7f17874ea700 \n(\nLWP \n112\n)]\n\n\nThread \n6\n \nweb-srv-go\n received signal SIGILL, Illegal instruction.\n\nThread \n6\n \nweb-srv-go\n received signal SIGILL, Illegal instruction.\n\nThread \n6\n \nweb-srv-go\n received signal SIGILL, Illegal instruction.\n\nThread \n6\n \nweb-srv-go\n received signal SIGILL, Illegal instruction.\n\nThread \n6\n \nweb-srv-go\n received signal SIGILL, Illegal instruction.\n\nThread \n8\n \nweb-srv-go\n received signal SIGILL, Illegal instruction.\n\n\n\n\nYou could interrupt this execution via control c:\n\n\n^C\nThread \n1\n \nweb-srv-go\n received signal SIGINT, Interrupt.\n0x00007f17870f69dd in pthread_join \n(\nthreadid\n=\n139739022911232\n, \nthread_return\n=\n0x7ffe1c807928\n)\n at pthread_join.c:90\n\n90\n  pthread_join.c: No such file or directory.\n\n(\ngdb\n)\n where\n\n#0  0x00007f17870f69dd in pthread_join (threadid=139739022911232, thread_return=0x7ffe1c807928) at pthread_join.c:90\n\n\n#1  0x0000002000004053 in main (argc=1, argv=0x7ffe1c807c18, envp=0x7ffe1c807c28) at ./tools/starter-exec.c:764\n\n\n(\ngdb\n)\n cont\nContinuing.\n\n\n\n\nBreakpoints\n\n\nOf course, you might also set breakpoints. Say, we want to get control in the debugger whenever a request is being\nprocessed by the handler. We would set a breakpoint at function \nmain.handler\n as follows:\n\n\n$ scone-gdb ./web-srv-go\n...\n\n[\nSCONE\n]\n Initializing...\n\n(\ngdb\n)\n handle SIGILL nostop pass\nSignal        Stop      Print   Pass to program Description\nSIGILL        No        Yes     Yes             Illegal instruction\n\n(\ngdb\n)\n \nbreak\n main.handler\nFunction \nmain.handler\n not defined.\nMake breakpoint pending on future shared library load? \n(\ny or \n[\nn\n])\n y\nBreakpoint \n1\n \n(\nmain.handler\n)\n pending.\n\n(\ngdb\n)\n run\nStarting program: /usr/src/myapp/web-srv-go \nwarning: Error disabling address space randomization: Operation not permitted\n\n[\nThread debugging using libthread_db enabled\n]\n\nUsing host libthread_db library \n/lib/x86_64-linux-gnu/libthread_db.so.1\n.\n\n[\nSCONE\n]\n Enclave base: \n1000000000\n\n\n[\nSCONE\n]\n Loaded debug symbols\n\n[\nNew Thread 0x7fb6cad32700 \n(\nLWP \n243\n)]\n\n\n[\nNew Thread 0x7fb6ca531700 \n(\nLWP \n244\n)]\n\n\n[\nNew Thread 0x7fb6c9d30700 \n(\nLWP \n245\n)]\n\n\n[\nNew Thread 0x7fb6c952f700 \n(\nLWP \n246\n)]\n\n\n[\nNew Thread 0x7fb6cb50e700 \n(\nLWP \n247\n)]\n\n\n[\nNew Thread 0x7fb6cb506700 \n(\nLWP \n248\n)]\n\n\n[\nNew Thread 0x7fb6cb4fe700 \n(\nLWP \n249\n)]\n\n\n[\nNew Thread 0x7fb6cb4f6700 \n(\nLWP \n250\n)]\n\n\nThread \n6\n \nweb-srv-go\n received signal SIGILL, Illegal instruction.\n\nThread \n6\n \nweb-srv-go\n received signal SIGILL, Illegal instruction.\n\nThread \n6\n \nweb-srv-go\n received signal SIGILL, Illegal instruction.\n\nThread \n6\n \nweb-srv-go\n received signal SIGILL, Illegal instruction.\n\nThread \n6\n \nweb-srv-go\n received signal SIGILL, Illegal instruction.\n\nThread \n6\n \nweb-srv-go\n received signal SIGILL, Illegal instruction.\n\n\n\n\nNote that at the time when we are setting the breakpoint, the symbols of the code running inside\nof the enclave are not yet known. Hence, we just let gdb know that the symbol will be defined later on.\n\n\nWe are now sending a request with the help of \ncurl\n from a different window. This triggers the breakpoint:\n\n\n[\nSwitching to Thread 0x7fb6cb506700 \n(\nLWP \n248\n)]\n\n\nThread \n7\n \nweb-srv-go\n hit Breakpoint \n1\n, main.handler \n(\nw\n=\n..., \nr\n=\n0x100909e300\n)\n at web-srv.go:8\n\n8\n       func handler\n(\nw http.ResponseWriter, r *http.Request\n)\n \n{\n\n\n(\ngdb\n)\n n\n\n9\n           fmt.Fprintf\n(\nw, \nHi there, I love %s!\n, r.URL.Path\n[\n1\n:\n])\n\n\n(\ngdb\n)\n n\n\n8\n       func handler\n(\nw http.ResponseWriter, r *http.Request\n)\n \n{\n\n\n(\ngdb\n)\n c\nContinuing.\n\n\n\n\n \nscontain.com\n, November 2017. \nQuestions or Suggestions?", 
            "title": "GO"
        }, 
        {
            "location": "/GO/#go", 
            "text": "SCONE supports cross-compiling GO programs to run these inside of SGX enclaves. The GO cross-compiler is part of image  sconecuratedimages/crosscompilers:scone .", 
            "title": "GO"
        }, 
        {
            "location": "/GO/#example", 
            "text": "Start the SCONE crosscompiler container:   docker run -it -p  8080 :8080 sconecuratedimages/crosscompilers:scone\n$  Lets consider a simple GO program (take from a  GO tutorial ):  $ cat   web-srv.go   EOF  package main  import (       os       fmt       net/http  )  func handler(w http.ResponseWriter, r *http.Request) {      fmt.Fprintf(w,  Hi there, I love %s!\\n , r.URL.Path[1:])      if r.URL.Path[1:] ==  EXIT  {          os.Exit(0)      }  }  func main() {      http.HandleFunc( / , handler)      http.ListenAndServe( :8080 , nil)  }  EOF   You can cross-compile this program as follows:  $  SCONE_HEAP = 1G scone-gccgo web-srv.go -O3 -o web-srv-go -g  You can start the compiled program (and enable some debug messages) as follows:  $  SCONE_VERSION = 1  ./web-srv-go   export   SCONE_QUEUES = 4  export   SCONE_SLOTS = 256  export   SCONE_SIGPIPE = 0  export   SCONE_MMAP32BIT = 0  export   SCONE_SSPINS = 100  export   SCONE_SSLEEP = 4000  export   SCONE_KERNEL = 0  export   SCONE_HEAP = 1073741824  export   SCONE_CONFIG = /etc/sgx-musl.conf export   SCONE_MODE = hw export   SCONE_SGXBOUNDS = no export   SCONE_ALLOW_DLOPEN = no\nRevision: 9b355b99170ad434010353bb9f4dca24e532b1b7\nBranch: master\nConfigure options: --enable-file-prot --enable-shared --enable-debug --prefix = /scone/src/built/cross-compiler/x86_64-linux-musl  You can now connect to port 8080, for example, with  curl :  $ curl localhost:8080/SCONE\nHi there, I love SCONE!  You can terminate the server with  $ curl localhost:8080/EXIT\ncurl:  ( 52 )  Empty reply from server", 
            "title": "Example"
        }, 
        {
            "location": "/GO/#debugging", 
            "text": "SCONE supports debugging of programs running inside of an enclave with the help of gdb.", 
            "title": "Debugging"
        }, 
        {
            "location": "/GO/#debugging-inside-of-a-container", 
            "text": "Standard containers have not sufficient rights to use the debugger.  Hence, you must start a container with  SYS_PTRACE  capability. For example:  $ docker run --cap-add SYS_PTRACE -it -p  8080 :8080 -v  $PWD /EXAMPLE:/usr/src/myapp -w /usr/src/myapp  sconecuratedimages/crosscompilers:scone", 
            "title": "Debugging inside of a container"
        }, 
        {
            "location": "/GO/#handling-illegal-instructions", 
            "text": "Some instructions, like CPUID, are not permitted inside of enclaves. For some of these instructions, like CPUID, we provide an automatic emulation. However, we recommend not to use any illegal instructions inside of enclaves despite having an automatic emulation of these instructions. For example, we provide static replacements of the CPUID instruction.   If your program contains some illegal instructions, you need to ask the debugger to forward the signals,\nthat these illegal instructions cause, to the program via  handle SIGILL nostop pass :   $ scone-gdb ./web-srv-go\nGNU gdb  ( Ubuntu  7 .12.50.20170314-0ubuntu1.1 )   7 .12.50.20170314-git\nCopyright  ( C )   2017  Free Software Foundation, Inc.\nLicense GPLv3+: GNU GPL version  3  or later  http://gnu.org/licenses/gpl.html \nThis is free software: you are free to change and redistribute it.\nThere is NO WARRANTY, to the extent permitted by law.  Type  show copying \nand  show warranty   for  details.\nThis GDB was configured as  x86_64-linux-gnu .\nType  show configuration   for  configuration details.\nFor bug reporting instructions, please see: http://www.gnu.org/software/gdb/bugs/ .\nFind the GDB manual and other documentation resources online at: http://www.gnu.org/software/gdb/documentation/ .\nFor help,  type   help .\nType  apropos word  to search  for  commands related to  word ...\nSource directories searched: /opt/scone/scone-gdb/gdb-sgxmusl-plugin: $cdir : $cwd \nSetting environment variable  LD_PRELOAD  to null value.\nReading symbols from ./web-srv-go...done. [ SCONE ]  Initializing... ( gdb )  handle SIGILL nostop pass\nSignal        Stop  Print   Pass to program Description\nSIGILL        No    Yes Yes     Illegal instruction ( gdb )   Since we do not patch the CPUID instructions in this run, you will see something like this:  ( gdb )  run\nStarting program: /usr/src/myapp/web-srv-go \nwarning: Error disabling address space randomization: Operation not permitted [ Thread debugging using libthread_db enabled ] \nUsing host libthread_db library  /lib/x86_64-linux-gnu/libthread_db.so.1 . [ SCONE ]  Enclave base:  1000000000  [ SCONE ]  Loaded debug symbols [ New Thread 0x7f1786d26700  ( LWP  105 )]  [ New Thread 0x7f1786525700  ( LWP  106 )]  [ New Thread 0x7f1785d24700  ( LWP  107 )]  [ New Thread 0x7f1785523700  ( LWP  108 )]  [ New Thread 0x7f1787502700  ( LWP  109 )]  [ New Thread 0x7f17874fa700  ( LWP  110 )]  [ New Thread 0x7f17874f2700  ( LWP  111 )]  [ New Thread 0x7f17874ea700  ( LWP  112 )] \n\nThread  6   web-srv-go  received signal SIGILL, Illegal instruction.\n\nThread  6   web-srv-go  received signal SIGILL, Illegal instruction.\n\nThread  6   web-srv-go  received signal SIGILL, Illegal instruction.\n\nThread  6   web-srv-go  received signal SIGILL, Illegal instruction.\n\nThread  6   web-srv-go  received signal SIGILL, Illegal instruction.\n\nThread  8   web-srv-go  received signal SIGILL, Illegal instruction.  You could interrupt this execution via control c:  ^C\nThread  1   web-srv-go  received signal SIGINT, Interrupt.\n0x00007f17870f69dd in pthread_join  ( threadid = 139739022911232 ,  thread_return = 0x7ffe1c807928 )  at pthread_join.c:90 90   pthread_join.c: No such file or directory. ( gdb )  where #0  0x00007f17870f69dd in pthread_join (threadid=139739022911232, thread_return=0x7ffe1c807928) at pthread_join.c:90  #1  0x0000002000004053 in main (argc=1, argv=0x7ffe1c807c18, envp=0x7ffe1c807c28) at ./tools/starter-exec.c:764  ( gdb )  cont\nContinuing.", 
            "title": "Handling Illegal instructions"
        }, 
        {
            "location": "/GO/#breakpoints", 
            "text": "Of course, you might also set breakpoints. Say, we want to get control in the debugger whenever a request is being\nprocessed by the handler. We would set a breakpoint at function  main.handler  as follows:  $ scone-gdb ./web-srv-go\n... [ SCONE ]  Initializing... ( gdb )  handle SIGILL nostop pass\nSignal        Stop      Print   Pass to program Description\nSIGILL        No        Yes     Yes             Illegal instruction ( gdb )   break  main.handler\nFunction  main.handler  not defined.\nMake breakpoint pending on future shared library load?  ( y or  [ n ])  y\nBreakpoint  1   ( main.handler )  pending. ( gdb )  run\nStarting program: /usr/src/myapp/web-srv-go \nwarning: Error disabling address space randomization: Operation not permitted [ Thread debugging using libthread_db enabled ] \nUsing host libthread_db library  /lib/x86_64-linux-gnu/libthread_db.so.1 . [ SCONE ]  Enclave base:  1000000000  [ SCONE ]  Loaded debug symbols [ New Thread 0x7fb6cad32700  ( LWP  243 )]  [ New Thread 0x7fb6ca531700  ( LWP  244 )]  [ New Thread 0x7fb6c9d30700  ( LWP  245 )]  [ New Thread 0x7fb6c952f700  ( LWP  246 )]  [ New Thread 0x7fb6cb50e700  ( LWP  247 )]  [ New Thread 0x7fb6cb506700  ( LWP  248 )]  [ New Thread 0x7fb6cb4fe700  ( LWP  249 )]  [ New Thread 0x7fb6cb4f6700  ( LWP  250 )] \n\nThread  6   web-srv-go  received signal SIGILL, Illegal instruction.\n\nThread  6   web-srv-go  received signal SIGILL, Illegal instruction.\n\nThread  6   web-srv-go  received signal SIGILL, Illegal instruction.\n\nThread  6   web-srv-go  received signal SIGILL, Illegal instruction.\n\nThread  6   web-srv-go  received signal SIGILL, Illegal instruction.\n\nThread  6   web-srv-go  received signal SIGILL, Illegal instruction.  Note that at the time when we are setting the breakpoint, the symbols of the code running inside\nof the enclave are not yet known. Hence, we just let gdb know that the symbol will be defined later on.  We are now sending a request with the help of  curl  from a different window. This triggers the breakpoint:  [ Switching to Thread 0x7fb6cb506700  ( LWP  248 )] \n\nThread  7   web-srv-go  hit Breakpoint  1 , main.handler  ( w = ...,  r = 0x100909e300 )  at web-srv.go:8 8        func handler ( w http.ResponseWriter, r *http.Request )   {  ( gdb )  n 9            fmt.Fprintf ( w,  Hi there, I love %s! , r.URL.Path [ 1 : ])  ( gdb )  n 8        func handler ( w http.ResponseWriter, r *http.Request )   {  ( gdb )  c\nContinuing.    scontain.com , November 2017.  Questions or Suggestions?", 
            "title": "Breakpoints"
        }, 
        {
            "location": "/Java/", 
            "text": "Java\n\n\nSCONE supports supports Java: we have an image containing OpenJDK8: \nsconecuratedimages/apps:openjdk8-alpine\n.\n\n\nExample\n\n\nLet us start with a simple hello world example:\n\n\ncat \n HelloWorld.java \n EOF\n\n\npublic class HelloWorld {\n\n\n\n    public static void main(String[] args) {\n\n\n        System.out.println(\nHello World\n);\n\n\n    }\n\n\n\n}\n\n\nEOF\n\n\n\n\n\nWe can compile and run this inside of an enclave as follows:\n\n\ndocker run --rm -v \n$(\npwd\n)\n:/test sconecuratedimages/apps:openjdk8-alpine sh -c \ncd /test \n javac HelloWorld.java \n java HelloWorld\n\n\n\n\n\nExample: Zookeeper\n\n\nThe above image runs Zookeeper without any modification. Our Zookeeper image is available here: \nsconecuratedimages/crosscompilers:zookeeper\n.\n\n\nZookeeper inside of an enclave runs  \nzk-smoketest\n without any issues.\n\n\n \nscontain.com\n, March 2018. \nQuestions or Suggestions?", 
            "title": "Java"
        }, 
        {
            "location": "/Java/#java", 
            "text": "SCONE supports supports Java: we have an image containing OpenJDK8:  sconecuratedimages/apps:openjdk8-alpine .", 
            "title": "Java"
        }, 
        {
            "location": "/Java/#example", 
            "text": "Let us start with a simple hello world example:  cat   HelloWorld.java   EOF  public class HelloWorld {      public static void main(String[] args) {          System.out.println( Hello World );      }  }  EOF   We can compile and run this inside of an enclave as follows:  docker run --rm -v  $( pwd ) :/test sconecuratedimages/apps:openjdk8-alpine sh -c  cd /test   javac HelloWorld.java   java HelloWorld", 
            "title": "Example"
        }, 
        {
            "location": "/Java/#example-zookeeper", 
            "text": "The above image runs Zookeeper without any modification. Our Zookeeper image is available here:  sconecuratedimages/crosscompilers:zookeeper .  Zookeeper inside of an enclave runs   zk-smoketest  without any issues.    scontain.com , March 2018.  Questions or Suggestions?", 
            "title": "Example: Zookeeper"
        }, 
        {
            "location": "/Python/", 
            "text": "Python\n\n\nSCONE supports running Python programs inside of SGX enclaves. We maintain a Docker image\n1\n for Python. \n\n\n\n\nPyPy for SCONE\n\n\nWe also support PyPy for SCONE: \nPyPySCONE\n's speed is close to PyPy (\"just in time Python\") and in almost all SpeedCenter benchmarks is \nPyPy\n inside an enclave faster than native Python. Send us email if you want to try this out.\n\n\n\n\nImage\n\n\nCurrently, we provde a simple Python 2.7 image that is based on the standard Python image python:2.7-alpine.\n\n\nYou can pull this image as follows:\n\n\n docker pull sconecuratedimages/crosscompilers:python27\n\n\n\n\nPython Interpreter\n\n\nTo run the Python interpreter inside an enclave in interactive mode, just execute:\n\n\n docker run -it -v \n$PWD\n:/usr/src/myapp -w /usr/src/myapp sconecuratedimages/crosscompilers:python27 sh\n\n$ \nSCONE_HEAP\n=\n1000000000\n \nSCONE_ALLOW_DLOPEN\n=\n2\n \nSCONE_MODE\n=\nAUTO \nSCONE_ALPINE\n=\n1\n \nSCONE_VERSION\n=\n1\n /usr/local/bin/python\n\nexport\n \nSCONE_QUEUES\n=\n4\n\n\nexport\n \nSCONE_SLOTS\n=\n256\n\n\nexport\n \nSCONE_SIGPIPE\n=\n0\n\n\nexport\n \nSCONE_MMAP32BIT\n=\n0\n\n\nexport\n \nSCONE_SSPINS\n=\n100\n\n\nexport\n \nSCONE_SSLEEP\n=\n4000\n\n\nexport\n \nSCONE_KERNEL\n=\n0\n\n\nexport\n \nSCONE_HEAP\n=\n11000000\n\n\nexport\n \nSCONE_CONFIG\n=\n/etc/sgx-musl.conf\n\nexport\n \nSCONE_MODE\n=\nhw\n\nexport\n \nSCONE_SGXBOUNDS\n=\nno\n\nexport\n \nSCONE_ALLOW_DLOPEN\n=\nyes\nRevision: 9b355b99170ad434010353bb9f4dca24e532b1b7\nBranch: master\nConfigure options: --enable-file-prot --enable-shared --enable-debug --prefix\n=\n/scone/src/built/cross-compiler/x86_64-linux-musl\n\nPython \n2\n.7.14 \n(\ndefault, Nov  \n4\n \n2017\n, \n00\n:12:32\n)\n \n\n[\nGCC \n5\n.3.0\n]\n on linux2\nType \nhelp\n, \ncopyright\n, \ncredits\n or \nlicense\n \nfor\n more information.\n\n \n\n\n\n\nPotential error messages:\n\n\n\n\nCould not create enclave: Error opening SGX device\n\n\nYour machine / container does not support SGX. Set mode to automatic via \nSCONE_MODE=AUTO\n:\nin AUTO mode, SCONE will use SGX enclaves when available and emulation mode otherwise. \n\n\n\n\n\n\nKilled\n\n\nYour machine / container has most likely too little memory: the Linux OOM (Out\nOf Memory) killer, terminated your program. Try to reduce memory size by reducing\nenvironment variable \nSCONE_HEAP\n appropriately.\n\n\n\n\nRunning an application\n\n\nSay, you have a Python application called \nmyapp.py\n in your current directory.\nTo execute this with Pyhton 2.7 inside an enclave, you need to set some environment variables. \n\n\n\n\n\n\nTo run Python inside of an enclave, you can set the environment variable \nSCONE_MODE=HW\n and \nSCONE_ALPINE=1\n.\n\n\n\n\n\n\nTo issue some debug messages that show that we are running inside an enclave, set \nSCONE_VERSION=1\n\n\n\n\n\n\nIn general, we only permit the loading of dynamic libraries during the startup of a program - these libraries are part of \nMRENCLAVE\n, i.e., the hash of the enclave. To enable the loading of dynamic libraries after startup (and without requiring the authentication of this library via the file shield), one can set \nSCONE_ALLOW_DLOPEN=2\n. For operations, the environment variables are set by the \nCAS\n and you must set \nSCONE_ALLOW_DLOPEN\n either to \nSCONE_ALLOW_DLOPEN=1\n to enable loading of dynamic libraries or must not define \nSCONE_ALLOW_DLOPEN\n.\n\n\n\n\n\n\nPython applications often require large heaps and large stacks. The current SGX CPUs (SGXv1) do not permit to increase the size of enclaves dynamically. This implies that enclaves might run out of memory if the initial enclave size was set to small. Selecting large enclave size by default would result in long startup times for all programs. SCONE permits to set the heap size via environment variable \nSCONE_HEAP\n and the stack size via \nSTACK_SIZE\n at startup.\n\n\n\n\n\n\n\n\nPython program exits\n\n\nPython does not always deal gracefully with out of memory situations: often, it terminates with some misleading error message if Python runs out of heap or stack memory. Please try to give python sufficient stack and heap size if this happens. We recommend to start with a large heap, like, \nSCONE_HEAP=10000000000\n to ensure that Python has sufficient heap. If your program runs without any problem with a large heap, you can try to reduce the heap size to speedup program startup times.**\n\n\n\n\nNote that you can set the environment variable of a process - in our case python - running inside a container with docker option \n-e\n:\n\n\n docker run --rm -v \n$PWD\n:/usr/src/myapp -w /usr/src/myapp -e \nSCONE_HEAP\n=\n10000000000\n \nSCONE_MODE\n=\nHW -e \nSCONE_ALLOW_DLOPEN\n=\n2\n -e \nSCONE_ALPINE\n=\n1\n -e \nSCONE_VERSION\n=\n1\n sconecuratedimages/crosscompilers:python27 python myapp.py\n\n\nexport\n \nSCONE_QUEUES\n=\n4\n\n\nexport\n \nSCONE_SLOTS\n=\n256\n\n\nexport\n \nSCONE_SIGPIPE\n=\n0\n\n\nexport\n \nSCONE_MMAP32BIT\n=\n0\n\n...\n\n\n\n\nNumPy\n\n\nLet's see how we can install some extra packages that your python program might need. Let us focus on \nNumPy\n first, a very popular package for scientific computing. Note that the following steps you would typically perform as part of a \nDockerfile\n.\n\n\nFirst, we start the SCONE Python  image:\n\n\n docker run -it sconecuratedimages/crosscompilers:python27 bash\n\n\n\n\nWe then install \nnumpy\n inside of the container with the help of \npip\n:\n\n\n$ pip install numpy\nCollecting numpy\n  Downloading numpy-1.14.0.zip \n(\n4\n.9MB\n)\n\n    \n100\n% \n|\n\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\n|\n \n4\n.9MB 211kB/s \nBuilding wheels \nfor\n collected packages: numpy\n  Running setup.py bdist_wheel \nfor\n numpy ... \n|\n\n\ndone\n\n  Stored in directory: /root/.cache/pip/wheels/1e/02/97/ba1c91fa21d10f8f96cc6ddd7d9553d429b5af17558fb83280\nSuccessfully built numpy\nInstalling collected packages: numpy\nSuccessfully installed numpy-1.14.0\n\n\n\n\nOk, let's try to execute some examples with \nNumPy\n. Let's  run Python inside an enclave, give it plenty of heap memory and ask SCONE to print some debug messages:\n\n\n$ \nSCONE_HEAP\n=\n10000000000\n \nSCONE_ALLOW_DLOPEN\n=\n2\n \nSCONE_MODE\n=\nAUTO \nSCONE_ALPINE\n=\n1\n \nSCONE_VERSION\n=\n1\n /usr/local/bin/python\n\nexport\n \nSCONE_QUEUES\n=\n4\n\n\nexport\n \nSCONE_SLOTS\n=\n256\n\n\nexport\n \nSCONE_SIGPIPE\n=\n0\n\n\nexport\n \nSCONE_MMAP32BIT\n=\n0\n\n\nexport\n \nSCONE_SSPINS\n=\n100\n\n\nexport\n \nSCONE_SSLEEP\n=\n4000\n\n\nexport\n \nSCONE_KERNEL\n=\n0\n\n\nexport\n \nSCONE_HEAP\n=\n10000000000\n\n\nexport\n \nSCONE_STACK\n=\n0\n\n\nexport\n \nSCONE_CONFIG\n=\n/etc/sgx-musl.conf\n\nexport\n \nSCONE_MODE\n=\nhw\n\nexport\n \nSCONE_SGXBOUNDS\n=\nno\n\nexport\n \nSCONE_VARYS\n=\nno\n\nexport\n \nSCONE_ALLOW_DLOPEN\n=\nyes\n\nexport\n \nSCONE_ALLOW_DLOPEN2\n=\nyes\nRevision: 7950fbd1a699ba15f9382ebaefc3ce0d4090801f\nBranch: master \n(\ndirty\n)\n\nConfigure options: --enable-shared --enable-debug --prefix\n=\n/scone/src/built/cross-compiler/x86_64-linux-musl\n\nPython \n2\n.7.14 \n(\ndefault, Dec \n19\n \n2017\n, \n22\n:29:22\n)\n \n\n[\nGCC \n6\n.4.0\n]\n on linux2\nType \nhelp\n, \ncopyright\n, \ncredits\n or \nlicense\n \nfor\n more information.\n\n \n\n\n\n\nNow, we can import numpy and execute some commands:\n\n\n import numpy as np\n\n \na\n \n=\n np.arange\n(\n15\n)\n.reshape\n(\n3\n, \n5\n)\n\n\n a\narray\n([[\n \n0\n,  \n1\n,  \n2\n,  \n3\n,  \n4\n]\n,\n       \n[\n \n5\n,  \n6\n,  \n7\n,  \n8\n,  \n9\n]\n,\n       \n[\n10\n, \n11\n, \n12\n, \n13\n, \n14\n]])\n\n\n a.shape\n\n(\n3\n, \n5\n)\n\n\n a.ndim\n\n2\n\n\n a.dtype.name\n\nint64\n\n\n a.itemsize\n\n8\n\n\n type\n(\na\n)\n\n\ntype\n \nnumpy.ndarray\n\n\n \n\n\n\n\nCairo\n\n\nLet's look at another popular library: the \ncairo\n graphics library. \ncairo\n  is written in C and has \nPython bindings provided by package \npycairo\n. In this case, we need to install the C-library first:\n\n\nIn Alpine Linux - which is the basis of the SCONE Python image - we can install cairo as follows:\n\n\n$ apk add --no-cache  cairo-dev cairo \nfetch http://dl-cdn.alpinelinux.org/alpine/v3.7/main/x86_64/APKINDEX.tar.gz\nfetch http://dl-cdn.alpinelinux.org/alpine/v3.7/community/x86_64/APKINDEX.tar.gz\n\n(\n1\n/55\n)\n Installing expat-dev \n(\n2\n.2.5-r0\n)\n\n...\n\n(\n55\n/55\n)\n Installing cairo-dev \n(\n1\n.14.10-r0\n)\n\nExecuting busybox-1.27.2-r6.trigger\nExecuting glib-2.54.2-r0.trigger\nNo schema files found: doing nothing.\nOK: \n297\n MiB in \n112\n packages\n$ \n\n\n\n\nNow we can install the Python bindings of cairo with \npip\n:\n\n\n$ pip install pycairo\nCollecting pycairo\n  Downloading pycairo-1.15.4.tar.gz \n(\n178kB\n)\n\n    \n100\n% \n|\n\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\n|\n 184kB \n1\n.7MB/s \nBuilding wheels \nfor\n collected packages: pycairo\n  Running setup.py bdist_wheel \nfor\n pycairo ... \ndone\n\n  Stored in directory: /root/.cache/pip/wheels/99/a6/16/79c5186b0ead4be059ce3102496b1ff776776b31da8e51af8f\nSuccessfully built pycairo\nInstalling collected packages: pycairo\nSuccessfully installed pycairo-1.15.4\n\n\n\n\nWe can now start Python again via \n\n\nSCONE_HEAP\n=\n10000000000\n \nSCONE_ALLOW_DLOPEN\n=\n2\n \nSCONE_MODE\n=\nAUTO \nSCONE_ALPINE\n=\n1\n \nSCONE_VERSION\n=\n1\n /usr/local/bin/python\n\n\n\n\nbefore we execute some cairo examples:\n\n\n import cairo\n\n import math\n\n WIDTH, \nHEIGHT\n \n=\n \n256\n, \n256\n\n\n \n\n \nsurface\n \n=\n cairo.ImageSurface \n(\ncairo.FORMAT_ARGB32, WIDTH, HEIGHT\n)\n\n\n \nctx\n \n=\n cairo.Context \n(\nsurface\n)\n\n\n ctx.scale \n(\nWIDTH, HEIGHT\n)\n \n# Normalizing the canvas\n\n\n \n\n \npat\n \n=\n cairo.LinearGradient \n(\n0\n.0, \n0\n.0, \n0\n.0, \n1\n.0\n)\n\n\n pat.add_color_stop_rgba \n(\n1\n, \n0\n.7, \n0\n, \n0\n, \n0\n.5\n)\n \n# First stop, 50% opacity\n\n\n pat.add_color_stop_rgba \n(\n0\n, \n0\n.9, \n0\n.7, \n0\n.2, \n1\n)\n \n# Last stop, 100% opacity\n\n\n \n\n ctx.rectangle \n(\n0\n, \n0\n, \n1\n, \n1\n)\n \n# Rectangle(x0, y0, x1, y1)\n\n\n ctx.set_source \n(\npat\n)\n\n\n ctx.fill \n()\n\n\n ctx.translate \n(\n0\n.1, \n0\n.1\n)\n \n# Changing the current transformation matrix\n\n\n \n\n ctx.move_to \n(\n0\n, \n0\n)\n\n\n \n# Arc(cx, cy, radius, start_angle, stop_angle)\n\n... ctx.arc \n(\n0\n.2, \n0\n.1, \n0\n.1, -math.pi/2, \n0\n)\n\n\n ctx.line_to \n(\n0\n.5, \n0\n.1\n)\n \n# Line to (x,y)\n\n\n \n# Curve(x1, y1, x2, y2, x3, y3)\n\n... ctx.curve_to \n(\n0\n.5, \n0\n.2, \n0\n.5, \n0\n.4, \n0\n.2, \n0\n.8\n)\n\n\n ctx.close_path \n()\n\n\n \n\n ctx.set_source_rgb \n(\n0\n.3, \n0\n.2, \n0\n.5\n)\n \n# Solid color\n\n\n ctx.set_line_width \n(\n0\n.02\n)\n\n\n ctx.stroke \n()\n\n\n surface.write_to_png \n(\nexample.png\n)\n \n# Output to PNG\n\n\n exit\n()\n\n\n\n\n\nThis generates a file \nexample.png\n in the working directory.\n\n\nExample\n\n\nLet's look at another example:\n\n\n\n\n\n\nWe use \npip\n to install a Python chess library.\n\n\n\n\n\n\nThen we run Python inside of an enclave and  import the chess library. \n\n\n\n\n\n\nWe use the Scholar's mate example from \nhttps://pypi.python.org/pypi/python-chess\n\n\n\n\n\n\n\n\nLimitation\n\n\nWe do not yet support fork, i.e., you spawn new processes from within your Python programs. We are currently working on removing this limitation of SCONE. Until then, we expect you to have an external spawner. \n\n\n \nscontain.com\n, January 2018. \nQuestions or Suggestions?\n\n\n\n\n\n\n\n\n\n\nSend email to \n to get access for evaluation version.", 
            "title": "Python"
        }, 
        {
            "location": "/Python/#python", 
            "text": "SCONE supports running Python programs inside of SGX enclaves. We maintain a Docker image 1  for Python.    PyPy for SCONE  We also support PyPy for SCONE:  PyPySCONE 's speed is close to PyPy (\"just in time Python\") and in almost all SpeedCenter benchmarks is  PyPy  inside an enclave faster than native Python. Send us email if you want to try this out.", 
            "title": "Python"
        }, 
        {
            "location": "/Python/#image", 
            "text": "Currently, we provde a simple Python 2.7 image that is based on the standard Python image python:2.7-alpine.  You can pull this image as follows:   docker pull sconecuratedimages/crosscompilers:python27", 
            "title": "Image"
        }, 
        {
            "location": "/Python/#python-interpreter", 
            "text": "To run the Python interpreter inside an enclave in interactive mode, just execute:   docker run -it -v  $PWD :/usr/src/myapp -w /usr/src/myapp sconecuratedimages/crosscompilers:python27 sh\n\n$  SCONE_HEAP = 1000000000   SCONE_ALLOW_DLOPEN = 2   SCONE_MODE = AUTO  SCONE_ALPINE = 1   SCONE_VERSION = 1  /usr/local/bin/python export   SCONE_QUEUES = 4  export   SCONE_SLOTS = 256  export   SCONE_SIGPIPE = 0  export   SCONE_MMAP32BIT = 0  export   SCONE_SSPINS = 100  export   SCONE_SSLEEP = 4000  export   SCONE_KERNEL = 0  export   SCONE_HEAP = 11000000  export   SCONE_CONFIG = /etc/sgx-musl.conf export   SCONE_MODE = hw export   SCONE_SGXBOUNDS = no export   SCONE_ALLOW_DLOPEN = yes\nRevision: 9b355b99170ad434010353bb9f4dca24e532b1b7\nBranch: master\nConfigure options: --enable-file-prot --enable-shared --enable-debug --prefix = /scone/src/built/cross-compiler/x86_64-linux-musl\n\nPython  2 .7.14  ( default, Nov   4   2017 ,  00 :12:32 )   [ GCC  5 .3.0 ]  on linux2\nType  help ,  copyright ,  credits  or  license   for  more information.    Potential error messages:   Could not create enclave: Error opening SGX device  Your machine / container does not support SGX. Set mode to automatic via  SCONE_MODE=AUTO :\nin AUTO mode, SCONE will use SGX enclaves when available and emulation mode otherwise.     Killed  Your machine / container has most likely too little memory: the Linux OOM (Out\nOf Memory) killer, terminated your program. Try to reduce memory size by reducing\nenvironment variable  SCONE_HEAP  appropriately.", 
            "title": "Python Interpreter"
        }, 
        {
            "location": "/Python/#running-an-application", 
            "text": "Say, you have a Python application called  myapp.py  in your current directory.\nTo execute this with Pyhton 2.7 inside an enclave, you need to set some environment variables.     To run Python inside of an enclave, you can set the environment variable  SCONE_MODE=HW  and  SCONE_ALPINE=1 .    To issue some debug messages that show that we are running inside an enclave, set  SCONE_VERSION=1    In general, we only permit the loading of dynamic libraries during the startup of a program - these libraries are part of  MRENCLAVE , i.e., the hash of the enclave. To enable the loading of dynamic libraries after startup (and without requiring the authentication of this library via the file shield), one can set  SCONE_ALLOW_DLOPEN=2 . For operations, the environment variables are set by the  CAS  and you must set  SCONE_ALLOW_DLOPEN  either to  SCONE_ALLOW_DLOPEN=1  to enable loading of dynamic libraries or must not define  SCONE_ALLOW_DLOPEN .    Python applications often require large heaps and large stacks. The current SGX CPUs (SGXv1) do not permit to increase the size of enclaves dynamically. This implies that enclaves might run out of memory if the initial enclave size was set to small. Selecting large enclave size by default would result in long startup times for all programs. SCONE permits to set the heap size via environment variable  SCONE_HEAP  and the stack size via  STACK_SIZE  at startup.     Python program exits  Python does not always deal gracefully with out of memory situations: often, it terminates with some misleading error message if Python runs out of heap or stack memory. Please try to give python sufficient stack and heap size if this happens. We recommend to start with a large heap, like,  SCONE_HEAP=10000000000  to ensure that Python has sufficient heap. If your program runs without any problem with a large heap, you can try to reduce the heap size to speedup program startup times.**   Note that you can set the environment variable of a process - in our case python - running inside a container with docker option  -e :   docker run --rm -v  $PWD :/usr/src/myapp -w /usr/src/myapp -e  SCONE_HEAP = 10000000000   SCONE_MODE = HW -e  SCONE_ALLOW_DLOPEN = 2  -e  SCONE_ALPINE = 1  -e  SCONE_VERSION = 1  sconecuratedimages/crosscompilers:python27 python myapp.py export   SCONE_QUEUES = 4  export   SCONE_SLOTS = 256  export   SCONE_SIGPIPE = 0  export   SCONE_MMAP32BIT = 0 \n...", 
            "title": "Running an application"
        }, 
        {
            "location": "/Python/#numpy", 
            "text": "Let's see how we can install some extra packages that your python program might need. Let us focus on  NumPy  first, a very popular package for scientific computing. Note that the following steps you would typically perform as part of a  Dockerfile .  First, we start the SCONE Python  image:   docker run -it sconecuratedimages/crosscompilers:python27 bash  We then install  numpy  inside of the container with the help of  pip :  $ pip install numpy\nCollecting numpy\n  Downloading numpy-1.14.0.zip  ( 4 .9MB ) \n     100 %  | \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 |   4 .9MB 211kB/s \nBuilding wheels  for  collected packages: numpy\n  Running setup.py bdist_wheel  for  numpy ...  |  done \n  Stored in directory: /root/.cache/pip/wheels/1e/02/97/ba1c91fa21d10f8f96cc6ddd7d9553d429b5af17558fb83280\nSuccessfully built numpy\nInstalling collected packages: numpy\nSuccessfully installed numpy-1.14.0  Ok, let's try to execute some examples with  NumPy . Let's  run Python inside an enclave, give it plenty of heap memory and ask SCONE to print some debug messages:  $  SCONE_HEAP = 10000000000   SCONE_ALLOW_DLOPEN = 2   SCONE_MODE = AUTO  SCONE_ALPINE = 1   SCONE_VERSION = 1  /usr/local/bin/python export   SCONE_QUEUES = 4  export   SCONE_SLOTS = 256  export   SCONE_SIGPIPE = 0  export   SCONE_MMAP32BIT = 0  export   SCONE_SSPINS = 100  export   SCONE_SSLEEP = 4000  export   SCONE_KERNEL = 0  export   SCONE_HEAP = 10000000000  export   SCONE_STACK = 0  export   SCONE_CONFIG = /etc/sgx-musl.conf export   SCONE_MODE = hw export   SCONE_SGXBOUNDS = no export   SCONE_VARYS = no export   SCONE_ALLOW_DLOPEN = yes export   SCONE_ALLOW_DLOPEN2 = yes\nRevision: 7950fbd1a699ba15f9382ebaefc3ce0d4090801f\nBranch: master  ( dirty ) \nConfigure options: --enable-shared --enable-debug --prefix = /scone/src/built/cross-compiler/x86_64-linux-musl\n\nPython  2 .7.14  ( default, Dec  19   2017 ,  22 :29:22 )   [ GCC  6 .4.0 ]  on linux2\nType  help ,  copyright ,  credits  or  license   for  more information.    Now, we can import numpy and execute some commands:   import numpy as np   a   =  np.arange ( 15 ) .reshape ( 3 ,  5 )   a\narray ([[   0 ,   1 ,   2 ,   3 ,   4 ] ,\n        [   5 ,   6 ,   7 ,   8 ,   9 ] ,\n        [ 10 ,  11 ,  12 ,  13 ,  14 ]])   a.shape ( 3 ,  5 )   a.ndim 2   a.dtype.name int64   a.itemsize 8   type ( a )  type   numpy.ndarray", 
            "title": "NumPy"
        }, 
        {
            "location": "/Python/#cairo", 
            "text": "Let's look at another popular library: the  cairo  graphics library.  cairo   is written in C and has \nPython bindings provided by package  pycairo . In this case, we need to install the C-library first:  In Alpine Linux - which is the basis of the SCONE Python image - we can install cairo as follows:  $ apk add --no-cache  cairo-dev cairo \nfetch http://dl-cdn.alpinelinux.org/alpine/v3.7/main/x86_64/APKINDEX.tar.gz\nfetch http://dl-cdn.alpinelinux.org/alpine/v3.7/community/x86_64/APKINDEX.tar.gz ( 1 /55 )  Installing expat-dev  ( 2 .2.5-r0 ) \n... ( 55 /55 )  Installing cairo-dev  ( 1 .14.10-r0 ) \nExecuting busybox-1.27.2-r6.trigger\nExecuting glib-2.54.2-r0.trigger\nNo schema files found: doing nothing.\nOK:  297  MiB in  112  packages\n$   Now we can install the Python bindings of cairo with  pip :  $ pip install pycairo\nCollecting pycairo\n  Downloading pycairo-1.15.4.tar.gz  ( 178kB ) \n     100 %  | \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 |  184kB  1 .7MB/s \nBuilding wheels  for  collected packages: pycairo\n  Running setup.py bdist_wheel  for  pycairo ...  done \n  Stored in directory: /root/.cache/pip/wheels/99/a6/16/79c5186b0ead4be059ce3102496b1ff776776b31da8e51af8f\nSuccessfully built pycairo\nInstalling collected packages: pycairo\nSuccessfully installed pycairo-1.15.4  We can now start Python again via   SCONE_HEAP = 10000000000   SCONE_ALLOW_DLOPEN = 2   SCONE_MODE = AUTO  SCONE_ALPINE = 1   SCONE_VERSION = 1  /usr/local/bin/python  before we execute some cairo examples:   import cairo  import math  WIDTH,  HEIGHT   =   256 ,  256      surface   =  cairo.ImageSurface  ( cairo.FORMAT_ARGB32, WIDTH, HEIGHT )    ctx   =  cairo.Context  ( surface )   ctx.scale  ( WIDTH, HEIGHT )   # Normalizing the canvas      pat   =  cairo.LinearGradient  ( 0 .0,  0 .0,  0 .0,  1 .0 )   pat.add_color_stop_rgba  ( 1 ,  0 .7,  0 ,  0 ,  0 .5 )   # First stop, 50% opacity   pat.add_color_stop_rgba  ( 0 ,  0 .9,  0 .7,  0 .2,  1 )   # Last stop, 100% opacity     ctx.rectangle  ( 0 ,  0 ,  1 ,  1 )   # Rectangle(x0, y0, x1, y1)   ctx.set_source  ( pat )   ctx.fill  ()   ctx.translate  ( 0 .1,  0 .1 )   # Changing the current transformation matrix     ctx.move_to  ( 0 ,  0 )    # Arc(cx, cy, radius, start_angle, stop_angle) \n... ctx.arc  ( 0 .2,  0 .1,  0 .1, -math.pi/2,  0 )   ctx.line_to  ( 0 .5,  0 .1 )   # Line to (x,y)    # Curve(x1, y1, x2, y2, x3, y3) \n... ctx.curve_to  ( 0 .5,  0 .2,  0 .5,  0 .4,  0 .2,  0 .8 )   ctx.close_path  ()     ctx.set_source_rgb  ( 0 .3,  0 .2,  0 .5 )   # Solid color   ctx.set_line_width  ( 0 .02 )   ctx.stroke  ()   surface.write_to_png  ( example.png )   # Output to PNG   exit ()   This generates a file  example.png  in the working directory.", 
            "title": "Cairo"
        }, 
        {
            "location": "/Python/#example", 
            "text": "Let's look at another example:    We use  pip  to install a Python chess library.    Then we run Python inside of an enclave and  import the chess library.     We use the Scholar's mate example from  https://pypi.python.org/pypi/python-chess", 
            "title": "Example"
        }, 
        {
            "location": "/Python/#limitation", 
            "text": "We do not yet support fork, i.e., you spawn new processes from within your Python programs. We are currently working on removing this limitation of SCONE. Until then, we expect you to have an external spawner.     scontain.com , January 2018.  Questions or Suggestions?      Send email to   to get access for evaluation version.", 
            "title": "Limitation"
        }, 
        {
            "location": "/Nodejs/", 
            "text": "Node\n\n\nWe provide Node 8.9.4 image that runs inside of an enclave:\n\n\n docker pull sconecuratedimages/apps:node-8.9.4\n\n\n\n\nExample\n\n\nLet's look at a little \nhello world\n program. First, we need to start a \nnode\n container: \n\n\n docker run -it --device\n=\n/dev/isgx sconecuratedimages/apps:node-8.9.4 sh\n\n\n\n\nIn case you have no sgx driver installed, you can drop option \n--device=/dev/isgx\n for testing. \nThe programs will then run in simulation mode\n, i.e., the SCONE software runs but in native mode and not inside an enclave.\n\n\nInside of the container, we first add \nnpm\n: \n\n\n# apk add --no-cache nodejs-npm\n\n\n\n\nEnsure we can run even in a resource-constrainted VM by setting the maximum heap size to a reasonable value of 1GB:\n\n\n# export SCONE_HEAP=1G\n\n\n\n\nWe create a new application \nmyapp\n:\n\n\n# mkdir myapp\n\n\n# cd myapp\n\n\n# cat \n package.json \n EOF\n\n\n{\n\n  \nname\n: \nmyapp\n,\n  \nversion\n: \n1.0.0\n,\n  \ndescription\n: \n,\n  \nmain\n: \napp.js\n,\n  \nscripts\n: \n{\n\n    \ntest\n: \necho \\\nError: no test specified\\\n \n exit 1\n\n  \n}\n,\n  \nauthor\n: \n,\n  \nlicense\n: \nISC\n\n\n}\n\nEOF\n\n\n\n\nWe install \nexpress\n with the help of \nnpm\n:\n\n\n# npm install express --save\n\n\n\n\n\nLet's store the \nhello world\n code:\n\n\n# cat \n app.js \n EOF\n\nvar \nexpress\n \n=\n require\n(\nexpress\n)\n;\n\nvar \napp\n \n=\n express\n()\n;\n\napp.get\n(\n/\n, \nfunction\n \n(\nreq, res\n)\n \n{\n\n  res.send\n(\nHello World!\n)\n;\n\n\n})\n;\n\napp.listen\n(\n3000\n, \nfunction\n \n()\n \n{\n\n  console.log\n(\nExample app listening on port 3000!\n)\n;\n\n\n})\n;\n\nEOF\n\n\n\n\nWe can run this application inside of an enclave with \nnode\n. We can also enable some debug\nmessages by setting environment variable \nSCONE_VERSION=1\n to print that we run inside of an enclave:\n\n\n# SCONE_VERSION=1 node app.js\n\n\n\nexport\n \nSCONE_QUEUES\n=\n4\n\n\nexport\n \nSCONE_SLOTS\n=\n256\n\n\nexport\n \nSCONE_SIGPIPE\n=\n0\n\n\nexport\n \nSCONE_MMAP32BIT\n=\n0\n\n\nexport\n \nSCONE_SSPINS\n=\n100\n\n\nexport\n \nSCONE_SSLEEP\n=\n4000\n\n\nexport\n \nSCONE_KERNEL\n=\n0\n\n\nexport\n \nSCONE_HEAP\n=\n4294967296\n\n\nexport\n \nSCONE_STACK\n=\n4194304\n\n\nexport\n \nSCONE_CONFIG\n=\n/etc/sgx-musl.conf\n\nexport\n \nSCONE_MODE\n=\nhw\n\nexport\n \nSCONE_SGXBOUNDS\n=\nno\n\nexport\n \nSCONE_VARYS\n=\nno\n\nexport\n \nSCONE_ALLOW_DLOPEN\n=\nyes \n(\nunprotected\n)\n\n\nexport\n \nSCONE_MPROTECT\n=\nno\nRevision: e349ed6e4821f0cbfe895413c616409848216173 \n(\nWed Feb \n28\n \n19\n:28:04 \n2018\n +0100\n)\n\nBranch: master\nConfigure options: --enable-shared --enable-debug --prefix\n=\n/builds/scone/subtree-scone/built/cross-compiler/x86_64-linux-musl\n\nEnclave hash: 28cf4f0953ba54af02b9d042fa2ec88a832d749ae4e5395cabd50369e72a5dcb\nExample app listening on port \n3000\n!\n\n\n\n\nYou can now try to send a request to \nmyapp\n from another shell in the container.\nAssuming that you did not start a new container in meantime, execute in another\nshell of your host:\n\n\n docker \nexec\n -it \n$(\ndocker ps -l -q\n)\n sh\n\n\n\n\nInside of the container, first install \ncurl\n and then query \nmyapp\n:\n\n\n# apk add \u2014no-cache curl\n\n\n# curl localhost:3000/\n\nHello World!/ \n#\n\n\n\n\n\nPotential error messages:\n\n\n\n\nCould not create enclave: Error opening SGX device\n\n\nYour machine / container does not support SGX. Set mode to automatic via \nSCONE_MODE=AUTO\n:\nin AUTO mode, SCONE will use SGX enclaves when available and emulation mode otherwise. \n\n\n\n\n\n\nKilled\n\n\nYour machine / container has most likely too little memory: the Linux OOM (Out\nOf Memory) killer, terminated your program. Try to reduce memory size by reducing\nenvironment variable \nSCONE_HEAP\n appropriately.\n\n\n\n\n\n\nerrno ENOSYS\n\n\nSCONE does not yet support the \nfork\n system call (- this will happen later this year).\nIf you spawn processes, there will be some error message like:\n\nnpm ERR! spawn ENOSYS\n\n\n\n\nEnvironment variables\n\n\nSGXv1 cannot dynamically increase the memory of an enclave. Hence, we have to determine the maximum heap (and stack) size at program start: you can increase the heap by setting environment variable \nSCONE_HEAP\n, e.g., \nSCONE_HEAP=8G\n.\nIn case you run out of memory inside the enclave, increase the heap size. In case your program gets \nkilled\n by the OS, you might have selected a too large heap that is not supported by your VM or your host.\n\n\nSimilarily, you can increase the stack size of threads running inside of enclaves by setting environment variable \nSCONE_STACK\n.\n\n\nEnvironment variable \nSCONE_VERSION=1\n prints debug messages - to show that the program runs inside of an enclave.\n\n\nSCONE_MODE=hw\n enforce that program runs in hardware enclave. By default, we set \nSCONE_MODE=auto\n which uses hardware enclave if available \nand software emulation otherwise.\n\n\nDockerfile\n\n\nThe above example, you could more easily put the following text in a Dockerfile:\n\n\nFROM sconecuratedimages/apps:node-8.9.4\nENV \nSCONE_HEAP\n=\n1G\nEXPOSE \n3000\n\nRUN apk add --no-cache nodejs-npm \n\\\n\n  \n mkdir myapp \n\\\n\n  \n \ncd\n myapp \n\\\n\n  \n \necho\n \n{\n \n package.json \n\\\n\n  \n \necho\n \nname\n: \nmyapp\n,\n \n package.json \n\\\n\n  \n \necho\n \nversion\n: \n1.0.0\n,\n \n package.json \n\\\n\n  \n \necho\n \ndescription\n: \n,\n \n package.json \n\\\n\n  \n \necho\n \nmain\n: \napp.js\n,\n \n package.json \n\\\n\n  \n \necho\n \nscripts\n:\n \n package.json \n{\n \n\\\n\n  \n \necho\n \n  \ntest\n: \necho \\\nError: no test specified\\\n \n exit 1\n \n package.json \n\\\n\n  \n \necho\n \n},\n \n package.json \n\\\n\n  \n \necho\n \nauthor\n: \n,\n \n package.json \n\\\n\n  \n \necho\n \nlicense\n: \nISC\n \n package.json \n\\\n\n  \n \necho\n \n}\n \n package.json \n\\\n\n  \n npm install express --save \n\\\n\n  \n \necho\n \nvar express = require(\nexpress\n);\n \n app.js \n\\\n\n  \n \necho\n \nvar app = express();\n \n app.js \n\\\n\n  \n \necho\n \napp.get(\n/\n, function (req, res) {\n \n app.js \n\\\n\n  \n \necho\n \n  res.send(\nHello World!\n);\n \n app.js \n\\\n\n  \n \necho\n \n});\n \n app.js \n\\\n\n  \n \necho\n \napp.listen(3000, function () {\n \n app.js \n\\\n\n  \n \necho\n \n  console.log(\nExample app listening on port 3000!\n);\n \n app.js \n\\\n\n  \n \necho\n \n});\n \n app.js \n\nCMD \nSCONE_VERSION\n=\n1\n node /myapp/app.js\n\n\nNow create an image \nmyapp\n as follows:\n\n\n# docker build -t myapp .\n\n\n\n\n\nYou can run a container of this image as a daemon as follows:\n\n\n# docker run -d -p 3000:3000 myapp\n\n\n\n\n\nYou can now query \nmyapp\n as follows:\n\n\n# curl localhost:3000\n\nHello World!\n\n\n\n\n \nscontain.com\n, March 2018. \nQuestions or Suggestions?", 
            "title": "Node"
        }, 
        {
            "location": "/Nodejs/#node", 
            "text": "We provide Node 8.9.4 image that runs inside of an enclave:   docker pull sconecuratedimages/apps:node-8.9.4", 
            "title": "Node"
        }, 
        {
            "location": "/Nodejs/#example", 
            "text": "Let's look at a little  hello world  program. First, we need to start a  node  container:    docker run -it --device = /dev/isgx sconecuratedimages/apps:node-8.9.4 sh  In case you have no sgx driver installed, you can drop option  --device=/dev/isgx  for testing.  The programs will then run in simulation mode , i.e., the SCONE software runs but in native mode and not inside an enclave.  Inside of the container, we first add  npm :   # apk add --no-cache nodejs-npm  Ensure we can run even in a resource-constrainted VM by setting the maximum heap size to a reasonable value of 1GB:  # export SCONE_HEAP=1G  We create a new application  myapp :  # mkdir myapp  # cd myapp  # cat   package.json   EOF  { \n   name :  myapp ,\n   version :  1.0.0 ,\n   description :  ,\n   main :  app.js ,\n   scripts :  { \n     test :  echo \\ Error: no test specified\\    exit 1 \n   } ,\n   author :  ,\n   license :  ISC  } \nEOF  We install  express  with the help of  npm :  # npm install express --save   Let's store the  hello world  code:  # cat   app.js   EOF \nvar  express   =  require ( express ) ; \nvar  app   =  express () ; \napp.get ( / ,  function   ( req, res )   { \n  res.send ( Hello World! ) ;  }) ; \napp.listen ( 3000 ,  function   ()   { \n  console.log ( Example app listening on port 3000! ) ;  }) ; \nEOF  We can run this application inside of an enclave with  node . We can also enable some debug\nmessages by setting environment variable  SCONE_VERSION=1  to print that we run inside of an enclave:  # SCONE_VERSION=1 node app.js  export   SCONE_QUEUES = 4  export   SCONE_SLOTS = 256  export   SCONE_SIGPIPE = 0  export   SCONE_MMAP32BIT = 0  export   SCONE_SSPINS = 100  export   SCONE_SSLEEP = 4000  export   SCONE_KERNEL = 0  export   SCONE_HEAP = 4294967296  export   SCONE_STACK = 4194304  export   SCONE_CONFIG = /etc/sgx-musl.conf export   SCONE_MODE = hw export   SCONE_SGXBOUNDS = no export   SCONE_VARYS = no export   SCONE_ALLOW_DLOPEN = yes  ( unprotected )  export   SCONE_MPROTECT = no\nRevision: e349ed6e4821f0cbfe895413c616409848216173  ( Wed Feb  28   19 :28:04  2018  +0100 ) \nBranch: master\nConfigure options: --enable-shared --enable-debug --prefix = /builds/scone/subtree-scone/built/cross-compiler/x86_64-linux-musl\n\nEnclave hash: 28cf4f0953ba54af02b9d042fa2ec88a832d749ae4e5395cabd50369e72a5dcb\nExample app listening on port  3000 !  You can now try to send a request to  myapp  from another shell in the container.\nAssuming that you did not start a new container in meantime, execute in another\nshell of your host:   docker  exec  -it  $( docker ps -l -q )  sh  Inside of the container, first install  curl  and then query  myapp :  # apk add \u2014no-cache curl  # curl localhost:3000/ \nHello World!/  #   Potential error messages:   Could not create enclave: Error opening SGX device  Your machine / container does not support SGX. Set mode to automatic via  SCONE_MODE=AUTO :\nin AUTO mode, SCONE will use SGX enclaves when available and emulation mode otherwise.     Killed  Your machine / container has most likely too little memory: the Linux OOM (Out\nOf Memory) killer, terminated your program. Try to reduce memory size by reducing\nenvironment variable  SCONE_HEAP  appropriately.    errno ENOSYS  SCONE does not yet support the  fork  system call (- this will happen later this year).\nIf you spawn processes, there will be some error message like: npm ERR! spawn ENOSYS", 
            "title": "Example"
        }, 
        {
            "location": "/Nodejs/#environment-variables", 
            "text": "SGXv1 cannot dynamically increase the memory of an enclave. Hence, we have to determine the maximum heap (and stack) size at program start: you can increase the heap by setting environment variable  SCONE_HEAP , e.g.,  SCONE_HEAP=8G .\nIn case you run out of memory inside the enclave, increase the heap size. In case your program gets  killed  by the OS, you might have selected a too large heap that is not supported by your VM or your host.  Similarily, you can increase the stack size of threads running inside of enclaves by setting environment variable  SCONE_STACK .  Environment variable  SCONE_VERSION=1  prints debug messages - to show that the program runs inside of an enclave.  SCONE_MODE=hw  enforce that program runs in hardware enclave. By default, we set  SCONE_MODE=auto  which uses hardware enclave if available \nand software emulation otherwise.", 
            "title": "Environment variables"
        }, 
        {
            "location": "/Nodejs/#dockerfile", 
            "text": "The above example, you could more easily put the following text in a Dockerfile:  FROM sconecuratedimages/apps:node-8.9.4\nENV  SCONE_HEAP = 1G\nEXPOSE  3000 \nRUN apk add --no-cache nodejs-npm  \\ \n    mkdir myapp  \\ \n     cd  myapp  \\ \n     echo   {    package.json  \\ \n     echo   name :  myapp ,    package.json  \\ \n     echo   version :  1.0.0 ,    package.json  \\ \n     echo   description :  ,    package.json  \\ \n     echo   main :  app.js ,    package.json  \\ \n     echo   scripts :    package.json  {   \\ \n     echo      test :  echo \\ Error: no test specified\\    exit 1    package.json  \\ \n     echo   },    package.json  \\ \n     echo   author :  ,    package.json  \\ \n     echo   license :  ISC    package.json  \\ \n     echo   }    package.json  \\ \n    npm install express --save  \\ \n     echo   var express = require( express );    app.js  \\ \n     echo   var app = express();    app.js  \\ \n     echo   app.get( / , function (req, res) {    app.js  \\ \n     echo     res.send( Hello World! );    app.js  \\ \n     echo   });    app.js  \\ \n     echo   app.listen(3000, function () {    app.js  \\ \n     echo     console.log( Example app listening on port 3000! );    app.js  \\ \n     echo   });    app.js \n\nCMD  SCONE_VERSION = 1  node /myapp/app.js \nNow create an image  myapp  as follows:  # docker build -t myapp .   You can run a container of this image as a daemon as follows:  # docker run -d -p 3000:3000 myapp   You can now query  myapp  as follows:  # curl localhost:3000 \nHello World!    scontain.com , March 2018.  Questions or Suggestions?", 
            "title": "Dockerfile"
        }, 
        {
            "location": "/Rust/", 
            "text": "Rust\n\n\nSCONE supports the Rust programming language. Rust combines speed and strong type safety and it is hence our language of choice for new applications that need to run inside of enclaves.\n\n\nTo build Rust applications, we provide variants of the \nrustc\n and \ncargo\n command line utilities as part of image \nsconecuratedimages/crosscompilers:scone\n: \n\n\nscone-rustc\n /  \nscone rustc\n\n\nYou can compile Rust programs but links against the SCONE libc instead of a standard libc. To print the version of Rust execute (inside container \nsconecuratedimages/crosscompilers:scone\n):\n\n\n docker run -it sconecuratedimages/crosscompilers:scone\n$ scone rustc --version\nrustc \n1\n.20.0 \n(\nf3d6973f4 \n2017\n-08-27\n)\n\n\n\n\n\nLet's try a simple hello world program.\n\n\n$ mkdir ~/projects\n$ \ncd\n ~/projects\n$ mkdir hello_world\n$ \ncd\n hello_world\n\n\n\n\nLet's try our rust program:\n\n\n$ cat \n main.rs \n EOF\n\n\nfn main() {\n\n\n    println!(\nHello, world!\n);\n\n\n}\n\n\nEOF\n\n\n\n\n\nLet's compile the program for running inside of enclaves:\n\n\n$ scone rustc main.rs\n$ ls\nmain  main.rs\n\n\n\n\nLet's run main inside an enclave and print some debug information:\n\n\n$ \nSCONE_MODE\n=\nHW \nSCONE_VERSION\n=\n1\n ./main\n\nexport\n \nSCONE_QUEUES\n=\n4\n\n\nexport\n \nSCONE_SLOTS\n=\n256\n\n\nexport\n \nSCONE_SIGPIPE\n=\n0\n\n\nexport\n \nSCONE_MMAP32BIT\n=\n0\n\n\nexport\n \nSCONE_SSPINS\n=\n100\n\n\nexport\n \nSCONE_SSLEEP\n=\n4000\n\n\nexport\n \nSCONE_KERNEL\n=\n0\n\n\nexport\n \nSCONE_HEAP\n=\n67108864\n\n\nexport\n \nSCONE_CONFIG\n=\n/etc/sgx-musl.conf\n\nexport\n \nSCONE_MODE\n=\nhw\n\nexport\n \nSCONE_SGXBOUNDS\n=\nno\n\nexport\n \nSCONE_VARYS\n=\nno\n\nexport\n \nSCONE_ALLOW_DLOPEN\n=\nno\n\nexport\n \nSCONE_ALLOW_DLOPEN2\n=\nno\n\nHello, world!\n\n\n\n\nscone-cargo\n and \nscone cargo\n:\n\n\nYou can build projects with \nscone cargo\n:\n\n\n$ scone cargo build --target\n=\nscone\n\n\n\n\nAlternatively, you can use \nscone-cargo\n if, for example,  you need a command without a space.\n\n\nscone cargo\n has access to the SCONE-compiled rust standard library and the target file. \n--target=scone\n instructs it to use our target file - essentially triggering a cross-compiler build.\n\n\nDue to the cross-compilation, crates that depend on compiled (C) libraries, such as openssl or error-chain, do not work out of the box. Cargo will not use the system installed libraries because it wrongly assumes that they do not fit the target architecture. To solve this issue, one has to either provide the compiled libraries or deactivate the crate.\n\n\nThe following is an example of how an executable with \nopenssl\n can be compiled: \n\n\n$ \nOPENSSL_LIB_DIR\n=\n/libressl-2.4.5 \nOPENSSL_INCLUDE_DIR\n=\n/libressl-2.4.5/include/ \nOPENSSL_STATIC\n=\n1\n \nPKG_CONFIG_ALLOW_CROSS\n=\n1\n scone-cargo build --target\n=\nscone \n\n\n\n\nIn the case of error-chain, one can just deactivate its optional backtrace feature that actually requires a precompiled library.\n\n\n \nscontain.com\n, December 2017. \nQuestions or Suggestions?", 
            "title": "Rust"
        }, 
        {
            "location": "/Rust/#rust", 
            "text": "SCONE supports the Rust programming language. Rust combines speed and strong type safety and it is hence our language of choice for new applications that need to run inside of enclaves.  To build Rust applications, we provide variants of the  rustc  and  cargo  command line utilities as part of image  sconecuratedimages/crosscompilers:scone :", 
            "title": "Rust"
        }, 
        {
            "location": "/Rust/#scone-rustc-scone-rustc", 
            "text": "You can compile Rust programs but links against the SCONE libc instead of a standard libc. To print the version of Rust execute (inside container  sconecuratedimages/crosscompilers:scone ):   docker run -it sconecuratedimages/crosscompilers:scone\n$ scone rustc --version\nrustc  1 .20.0  ( f3d6973f4  2017 -08-27 )   Let's try a simple hello world program.  $ mkdir ~/projects\n$  cd  ~/projects\n$ mkdir hello_world\n$  cd  hello_world  Let's try our rust program:  $ cat   main.rs   EOF  fn main() {      println!( Hello, world! );  }  EOF   Let's compile the program for running inside of enclaves:  $ scone rustc main.rs\n$ ls\nmain  main.rs  Let's run main inside an enclave and print some debug information:  $  SCONE_MODE = HW  SCONE_VERSION = 1  ./main export   SCONE_QUEUES = 4  export   SCONE_SLOTS = 256  export   SCONE_SIGPIPE = 0  export   SCONE_MMAP32BIT = 0  export   SCONE_SSPINS = 100  export   SCONE_SSLEEP = 4000  export   SCONE_KERNEL = 0  export   SCONE_HEAP = 67108864  export   SCONE_CONFIG = /etc/sgx-musl.conf export   SCONE_MODE = hw export   SCONE_SGXBOUNDS = no export   SCONE_VARYS = no export   SCONE_ALLOW_DLOPEN = no export   SCONE_ALLOW_DLOPEN2 = no\n\nHello, world!", 
            "title": "scone-rustc /  scone rustc"
        }, 
        {
            "location": "/Rust/#scone-cargo-and-scone-cargo", 
            "text": "You can build projects with  scone cargo :  $ scone cargo build --target = scone  Alternatively, you can use  scone-cargo  if, for example,  you need a command without a space.  scone cargo  has access to the SCONE-compiled rust standard library and the target file.  --target=scone  instructs it to use our target file - essentially triggering a cross-compiler build.  Due to the cross-compilation, crates that depend on compiled (C) libraries, such as openssl or error-chain, do not work out of the box. Cargo will not use the system installed libraries because it wrongly assumes that they do not fit the target architecture. To solve this issue, one has to either provide the compiled libraries or deactivate the crate.  The following is an example of how an executable with  openssl  can be compiled:   $  OPENSSL_LIB_DIR = /libressl-2.4.5  OPENSSL_INCLUDE_DIR = /libressl-2.4.5/include/  OPENSSL_STATIC = 1   PKG_CONFIG_ALLOW_CROSS = 1  scone-cargo build --target = scone   In the case of error-chain, one can just deactivate its optional backtrace feature that actually requires a precompiled library.    scontain.com , December 2017.  Questions or Suggestions?", 
            "title": "scone-cargo and scone cargo:"
        }, 
        {
            "location": "/SCONE_HOST/", 
            "text": "scone host\n\n\nThis page describes the CLI \nscone host\n in more details. For an more general introduction on how to install a host and how to set up ssh to this host, please read section \nSCONE Host Setup\n. Read section \nSCONE Command Line Interface\n to see how to install command \nscone\n.\n\n\nCommands\n\n\nscone host\n supports the following commands:\n\n\n\n\n\n\ncheck\n:     checks that host is properly installed (patched docker engine and patched sgx driver)\n\n\n\n\n\n\ninstall\n:   installs the patched SGX driver and patched docker engine\n\n\n\n\n\n\nreboot\n:    reboots a host\n\n\n\n\n\n\nuninstall\n: uninstalls SGX driver and patched docker engine\n\n\n\n\n\n\nswarm\n:    join a new or another swarm\n\n\n\n\n\n\nscone host install\n\n\nCommand \ninstall\n installs a patched docker engine and a patched Intel SGX driver. It also supports that the installed host joins an existing Docker swarm or it becomes the manager of a newly created Docker swarm.\n\n\nNOTE: if a docker engine or an Intel SGX driver is already installed, the installed software is uninstalled and replaced by a patched versions. In this process, all containers that run on this machine and all enclaves that execute on this host are removed. Use command install with care.\n \n\n\nOptions\n\n\n\n\nYou must always specify option \n--name HOST\n, where, \nHOST\n is the name of the host that you want to install. You need to have \nssh access\n to this host - without the need to type in a password.\n\n\n\n\n\nExample:\n To install host \ndorothy\n without joining any swarm, just execute\n\n\n\n\n$ scone host install --name dorothy \n\n\n\n\n\n\nIf you want to install a host and make this host a manager of a new or an existing swarm, you need to add option \n--as-manager\n.\n\n\n\n\n\nExample:\n To install host \nfaye\n and create a new swarm with \nfaye\n as a manager, just execute\n\n\n\n\n$ scone host install --name faye --as-manager\n\n\n\n\nYou can now check your swarm with \nscone swarm\n to see the members of your new swarm:\n\n\n$ scone swarm ls --manager faye\nNODENO        SGX VERSION   DOCKER-ENGINE SGX-DRIVER    HOST                 STATUS     AVAILABILITY  MANAGER   \n\n1\n             \n1\n             SCONE         SCONE         faye                 Ready      Active        Leader    \n\n\n\n\n\n\nIf you want to join an existing swarm as \na worker\n, you have to specify option \n--join MANAGER\n\n\n\n\n\n\nExample:\n To install host \nedna\n and then join the swarm managed by manager \nfaye\n, execute:\n\n\n\n\n$ scone host install --name edna --join faye\n\n\n\n\nYou can now check your swarm with \nscone swarm\n to see the members of your new swarm:\n\n\n$ scone swarm ls --manager faye\n`\n\nNODENO        SGX VERSION   DOCKER-ENGINE SGX-DRIVER    HOST                 STATUS     AVAILABILITY  MANAGER   \n\n1\n             \n1\n             SCONE         SCONE         edna                 Ready      Active                  \n\n2\n             \n1\n             SCONE         SCONE         faye                 Ready      Active        Leader    \n\n\n\n\n\n\nIf you want to join an existing swarm as \na manager\n, you have to specify options \n--join MANAGER --as-manager\n\n\n\n\n\n\nExample:\n To install host \nedna\n and then join the swarm managed by manager \nfaye\n as a manager, execute:\n\n\n\n\n$ scone host install --name edna --join faye --as-manager\n\n\n\n\nscone host swarm\n\n\nIn the above example, we installed host \ndorothy\n without joining any swarm. Sometimes one wants to revise this decision via command \nswarm\n. You can add an existing node to another swarm or you can as a node to become a manager of an existing or a new node. You need to set the options \n--join MANAGER\n and \n--as-manager\n as described for \nscone host install\n.\n\n\nExample:\n To add host \ndorothy\n to the swarm managed by host \nfaye\n, just execute:\n\n\n$ scone host swarm --name dorothy --join faye \n\n\n\n\nExample:\n To add host \ndorothy\n to the swarm managed by host \nfaye\n as a manager, just execute:\n\n\n$ scone host swarm --name dorothy --join faye --as-manager\n\n\n\n\nscone host check\n\n\nTo check if a host is properly installed, you can use command \ncheck\n.\n\n\nExample:\n To check if host \nfaye\n is properly installed, just execute:\n\n\n$ scone host check --name faye\n\n\n\n\nIf the host is not properly installed, warnings or errors are issued. Typically, you can fix errors and warnings be reinstalling the host.\n\n\nscone host reboot\n\n\nSometimes, the installation of an host fails because the existing SGX driver cannot be removed. Most of the time, the issue is an enclave that currently uses the SGX driver. You can find these processes, for example, with Linux utility \nlsof\n:\n\n\n$ sudo lsof \n|\n grep dev/isgx \n\n\n\n\nSometimes removing an existing Intel SGX driver fails, despite the fact that no process seems to use the driver. In case this happens, one last resort is to reboot the machine. You issue the reboot manually or you can perform this with \nscone host reboot\n. \n\n\nYou must specify options \n--name HOST\n to indicate which host to reboot. Moreover,   \nscone host reboot\n will exit with an error unless you specify option \n--force\n.\n\n\nExample:\n To reboot host dorothy, just execute the following:\n\n\n$ scone host reboot --name  dorothy --force \n\n\n\n\nIf you want to wait until the host is again available, you can specify option \n--wait\n.\n\n\nExample:\n To reboot host dorothy and only return after the host has indeed rebooted, just execute the following:\n\n\n$ scone host reboot --name  dorothy --force --wait\n\n\n\n\nscone host uninstall\n\n\nWith the help of \nscone host uninstall\n you can force an host to \n\n\n\n\n\n\nleave any Docker swarm it might be part of \n\n\n\n\n\n\nuninstall the patched Docker engine\n\n\n\n\n\n\nuninstall the patched SGX driver\n\n\n\n\n\n\nNOTE: all containers that might run on this host, will be destroyed.\n\n\nOptions\n\n\n\n\n\n\nYou must define the name of the host to be uninstalled via option \n--name HOST\n. \n\n\n\n\n\n\nYou must always give the \n--force\n option, otherwise, an error is issued.\n\n\n\n\n\n\nIf a node is part of a swarm, you must explicitly specify the option \n--manager MANAGERHOST\n. \n\n\n\n\n\n\nIf the node is not part of a swarm, you must specify the option \n--noswarm\n. \n\n\n\n\n\n\nNote:\n While other objects / commands support the use of environment variable \nSCONE_MANAGER\n\nas an implicit definition of \n--manager $SCONE_MANAGER\n, we decided that users must explicitly specify the manager of the swarm for command \nuninstall\n.\n\n\nExample:\n To uninstall a host \nedna\n that is part of a swarm managed by host \nfaye\n, execute:\n\n\n$ scone host uninstall --name  edna --force --manager faye\n\n\n\n\nExample:\n Let's assume that host \nedna\n is not part of a swarm. To uninstall host \nedna\n, execute:\n\n\n$ scone host uninstall --name  edna --force --noswarm\n\n\n\n\nGeneral options\n\n\n\n\n\n\n--help\n (or, \n-h\n): issue help message for object \nhost\n. If a command is specified, it issues a help message specific to this command.\n\n\n\n\n\n\n--debug\n (or, \n-x\n): display all commands that are executed by \nscone host\n. This can be helpful in case a command fails. When you submit a support request regarding a failed command, please send a copy of the output of the failing command with \n--debug\n set.\n\n\n\n\n\n\n--verbose\n (or, \n-v\n): display all commands that are executed by \nscone host\n. This can be helpful in case commands fail. When you submit a support request regarding a failed command, please send a copy of the log of the output that includes \n\n\n\n\n\n\n \nscontain.com\n, December 2017. \nQuestions or Suggestions?", 
            "title": "scone host"
        }, 
        {
            "location": "/SCONE_HOST/#scone-host", 
            "text": "This page describes the CLI  scone host  in more details. For an more general introduction on how to install a host and how to set up ssh to this host, please read section  SCONE Host Setup . Read section  SCONE Command Line Interface  to see how to install command  scone .", 
            "title": "scone host"
        }, 
        {
            "location": "/SCONE_HOST/#commands", 
            "text": "scone host  supports the following commands:    check :     checks that host is properly installed (patched docker engine and patched sgx driver)    install :   installs the patched SGX driver and patched docker engine    reboot :    reboots a host    uninstall : uninstalls SGX driver and patched docker engine    swarm :    join a new or another swarm", 
            "title": "Commands"
        }, 
        {
            "location": "/SCONE_HOST/#scone-host-install", 
            "text": "Command  install  installs a patched docker engine and a patched Intel SGX driver. It also supports that the installed host joins an existing Docker swarm or it becomes the manager of a newly created Docker swarm.  NOTE: if a docker engine or an Intel SGX driver is already installed, the installed software is uninstalled and replaced by a patched versions. In this process, all containers that run on this machine and all enclaves that execute on this host are removed. Use command install with care.", 
            "title": "scone host install"
        }, 
        {
            "location": "/SCONE_HOST/#options", 
            "text": "You must always specify option  --name HOST , where,  HOST  is the name of the host that you want to install. You need to have  ssh access  to this host - without the need to type in a password.   Example:  To install host  dorothy  without joining any swarm, just execute   $ scone host install --name dorothy    If you want to install a host and make this host a manager of a new or an existing swarm, you need to add option  --as-manager .   Example:  To install host  faye  and create a new swarm with  faye  as a manager, just execute   $ scone host install --name faye --as-manager  You can now check your swarm with  scone swarm  to see the members of your new swarm:  $ scone swarm ls --manager faye\nNODENO        SGX VERSION   DOCKER-ENGINE SGX-DRIVER    HOST                 STATUS     AVAILABILITY  MANAGER    1               1              SCONE         SCONE         faye                 Ready      Active        Leader       If you want to join an existing swarm as  a worker , you have to specify option  --join MANAGER    Example:  To install host  edna  and then join the swarm managed by manager  faye , execute:   $ scone host install --name edna --join faye  You can now check your swarm with  scone swarm  to see the members of your new swarm:  $ scone swarm ls --manager faye ` \nNODENO        SGX VERSION   DOCKER-ENGINE SGX-DRIVER    HOST                 STATUS     AVAILABILITY  MANAGER    1               1              SCONE         SCONE         edna                 Ready      Active                   2               1              SCONE         SCONE         faye                 Ready      Active        Leader       If you want to join an existing swarm as  a manager , you have to specify options  --join MANAGER --as-manager    Example:  To install host  edna  and then join the swarm managed by manager  faye  as a manager, execute:   $ scone host install --name edna --join faye --as-manager", 
            "title": "Options"
        }, 
        {
            "location": "/SCONE_HOST/#scone-host-swarm", 
            "text": "In the above example, we installed host  dorothy  without joining any swarm. Sometimes one wants to revise this decision via command  swarm . You can add an existing node to another swarm or you can as a node to become a manager of an existing or a new node. You need to set the options  --join MANAGER  and  --as-manager  as described for  scone host install .  Example:  To add host  dorothy  to the swarm managed by host  faye , just execute:  $ scone host swarm --name dorothy --join faye   Example:  To add host  dorothy  to the swarm managed by host  faye  as a manager, just execute:  $ scone host swarm --name dorothy --join faye --as-manager", 
            "title": "scone host swarm"
        }, 
        {
            "location": "/SCONE_HOST/#scone-host-check", 
            "text": "To check if a host is properly installed, you can use command  check .  Example:  To check if host  faye  is properly installed, just execute:  $ scone host check --name faye  If the host is not properly installed, warnings or errors are issued. Typically, you can fix errors and warnings be reinstalling the host.", 
            "title": "scone host check"
        }, 
        {
            "location": "/SCONE_HOST/#scone-host-reboot", 
            "text": "Sometimes, the installation of an host fails because the existing SGX driver cannot be removed. Most of the time, the issue is an enclave that currently uses the SGX driver. You can find these processes, for example, with Linux utility  lsof :  $ sudo lsof  |  grep dev/isgx   Sometimes removing an existing Intel SGX driver fails, despite the fact that no process seems to use the driver. In case this happens, one last resort is to reboot the machine. You issue the reboot manually or you can perform this with  scone host reboot .   You must specify options  --name HOST  to indicate which host to reboot. Moreover,    scone host reboot  will exit with an error unless you specify option  --force .  Example:  To reboot host dorothy, just execute the following:  $ scone host reboot --name  dorothy --force   If you want to wait until the host is again available, you can specify option  --wait .  Example:  To reboot host dorothy and only return after the host has indeed rebooted, just execute the following:  $ scone host reboot --name  dorothy --force --wait", 
            "title": "scone host reboot"
        }, 
        {
            "location": "/SCONE_HOST/#scone-host-uninstall", 
            "text": "With the help of  scone host uninstall  you can force an host to     leave any Docker swarm it might be part of     uninstall the patched Docker engine    uninstall the patched SGX driver    NOTE: all containers that might run on this host, will be destroyed.", 
            "title": "scone host uninstall"
        }, 
        {
            "location": "/SCONE_HOST/#options_1", 
            "text": "You must define the name of the host to be uninstalled via option  --name HOST .     You must always give the  --force  option, otherwise, an error is issued.    If a node is part of a swarm, you must explicitly specify the option  --manager MANAGERHOST .     If the node is not part of a swarm, you must specify the option  --noswarm .     Note:  While other objects / commands support the use of environment variable  SCONE_MANAGER \nas an implicit definition of  --manager $SCONE_MANAGER , we decided that users must explicitly specify the manager of the swarm for command  uninstall .  Example:  To uninstall a host  edna  that is part of a swarm managed by host  faye , execute:  $ scone host uninstall --name  edna --force --manager faye  Example:  Let's assume that host  edna  is not part of a swarm. To uninstall host  edna , execute:  $ scone host uninstall --name  edna --force --noswarm", 
            "title": "Options"
        }, 
        {
            "location": "/SCONE_HOST/#general-options", 
            "text": "--help  (or,  -h ): issue help message for object  host . If a command is specified, it issues a help message specific to this command.    --debug  (or,  -x ): display all commands that are executed by  scone host . This can be helpful in case a command fails. When you submit a support request regarding a failed command, please send a copy of the output of the failing command with  --debug  set.    --verbose  (or,  -v ): display all commands that are executed by  scone host . This can be helpful in case commands fail. When you submit a support request regarding a failed command, please send a copy of the log of the output that includes       scontain.com , December 2017.  Questions or Suggestions?", 
            "title": "General options"
        }, 
        {
            "location": "/SCONE_SERVICE/", 
            "text": "scone service\n\n\nscone service\n manages remote docker services. \nscone service\n is mainly a thin wrapper around \ndocker service\n. However, instead of executing commands locally,  it forwards the commands to the manager of a remote swarm.\n\n\nThe remote swarm is specified via command line option \n--manager MANAGER\n. Alternatively, one can export an environment variable \nSCONE_MANAGER\n: this variable defines the default swarm to be used by \nscone service\n in case option \n--manager\n is not given.\n\n\nscone service\n supports all commands and all the options of \ndocker service\n. The semantics of command \ncreate\n is, however, slightly modified: \nscone service create\n only creates services on SGX-enabled nodes. More precisely, it limits the nodes that can run a service to those that have a label \nSGX VERSION\n greater than 0.\n\n\nIn addition to the docker defined commands, \nscone service\n supports two new commands:\n\n\n\n\n\n\n\n\nscone service\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nregistry\n\n\nchecks if a registry service is running in the swarm and if it is not, it starts a new registry service.\n\n\n\n\n\n\npull\n\n\npulls an image from a repository and stores it in the registry of the swarm.\n\n\n\n\n\n\n\n\nscone service commands\n\n\nThe following commands of scone service are implemented by \ndocker service\n:\n\n\n\n\n\n\n\n\nscone service\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\ncreate\n\n\nCreates a new service \non a SGX-enabled machine\n.\n\n\n\n\n\n\ninspect\n\n\nDisplay detailed information on one or more services\n\n\n\n\n\n\nlogs\n\n\nFetch the logs of a service or task\n\n\n\n\n\n\nls\n\n\nList services\n\n\n\n\n\n\nps\n\n\nList the tasks of one or more services\n\n\n\n\n\n\nrm\n\n\nRemove one or more services\n\n\n\n\n\n\nrollback\n\n\nRevert changes to a service's configuration\n\n\n\n\n\n\nscale\n\n\nScale one or multiple replicated services\n\n\n\n\n\n\nupdate\n\n\nUpdate a service\n\n\n\n\n\n\n\n\nThe options of the above commands are the same as for \ndocker service\n with the exception that all commands support\nthe new option \n--manager MANAGER\n.\n\n\nscone service registry [OPTIONS]\n\n\nStarts a local registry service in the swarm managed by node \nMANAGER\n.\nThe identity of \nMANAGER\n is given via option \n--manager=MANAGER\n or via environment variable \nSCONE_MANAGER=MANAGER\n.\n\n\n\n\n\n\n\n\nOptions\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\n--manager MANAGER\n\n\nmanager of swarm (required unless SCONE_MANAGER is defined)\n\n\n\n\n\n\n--verbose\n\n\nprint verbose messages\n\n\n\n\n\n\n--debug\n\n\nprint debug messages\n\n\n\n\n\n\n--help\n\n\nprint usage of this command\n\n\n\n\n\n\n\n\nscone service pull [OPTIONS] REPOSITORY/IMAGE[:TAG]\n\n\nPulls an image from a repository (typically, docker hub) and stores\nthe image in the local registry. First, create this local repository with\n\n\n$  scone service registry --manager MANAGER\n\n\n\n\nIt is expected that the name of the image to be pulled has the \nfollowing format:\n\n\n   REPOSITORY/IMAGE\n[\n:TAG\n]\n\n\n\n\n\nAfter pulling the image, it is locally available in the swarm.\nThe name of the image id now:\n\n\n   localhost:5000/IMAGE\n[\n:TAG\n]\n\n\n\n\n\nThe identity of MANAGER is given via option --manager or via\nenvironment variable SCONE_MANAGER.\n\n\n\n\n\n\n\n\nOptions\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\n--manager MANAGER\n\n\nmanager of swarm (required)\n\n\n\n\n\n\n--verbose\n\n\nprint verbose messages\n\n\n\n\n\n\n--debug\n\n\nprint debug messages\n\n\n\n\n\n\n--help\n\n\nprint usage of this command\n\n\n\n\n\n\n\n\nExample\n\n\nTo ensure that a registry is running on the swarm managed by node \nfaye\n, execute:\n\n\n$ \nexport\n \nSCONE_MANAGER\n=\nfaye\n$ scone service registry\n\n\n\n\nTo pull image \nsconecuratedimages/sconetainer:shielded\n from docker hub and store it in the local registry, execute\n\n\n$ scone service pull sconecuratedimages/sconetainer:shielded\n\n\n\n\nYou can then start this image as a service on the swarm managed by \nfaye\n as follows:\n\n\n$ scone service create --name nginx-shielded --detach\n=\ntrue\n  --publish \n8090\n:8080 --publish \n8092\n:8082 localhost:5000/sconetainer:shielded\n\n\n\n\n \nscontain.com\n, December 2017. \nQuestions or Suggestions?", 
            "title": "scone service"
        }, 
        {
            "location": "/SCONE_SERVICE/#scone-service", 
            "text": "scone service  manages remote docker services.  scone service  is mainly a thin wrapper around  docker service . However, instead of executing commands locally,  it forwards the commands to the manager of a remote swarm.  The remote swarm is specified via command line option  --manager MANAGER . Alternatively, one can export an environment variable  SCONE_MANAGER : this variable defines the default swarm to be used by  scone service  in case option  --manager  is not given.  scone service  supports all commands and all the options of  docker service . The semantics of command  create  is, however, slightly modified:  scone service create  only creates services on SGX-enabled nodes. More precisely, it limits the nodes that can run a service to those that have a label  SGX VERSION  greater than 0.  In addition to the docker defined commands,  scone service  supports two new commands:     scone service  Description      registry  checks if a registry service is running in the swarm and if it is not, it starts a new registry service.    pull  pulls an image from a repository and stores it in the registry of the swarm.", 
            "title": "scone service"
        }, 
        {
            "location": "/SCONE_SERVICE/#scone-service-commands", 
            "text": "The following commands of scone service are implemented by  docker service :     scone service  Description      create  Creates a new service  on a SGX-enabled machine .    inspect  Display detailed information on one or more services    logs  Fetch the logs of a service or task    ls  List services    ps  List the tasks of one or more services    rm  Remove one or more services    rollback  Revert changes to a service's configuration    scale  Scale one or multiple replicated services    update  Update a service     The options of the above commands are the same as for  docker service  with the exception that all commands support\nthe new option  --manager MANAGER .", 
            "title": "scone service commands"
        }, 
        {
            "location": "/SCONE_SERVICE/#scone-service-registry-options", 
            "text": "Starts a local registry service in the swarm managed by node  MANAGER .\nThe identity of  MANAGER  is given via option  --manager=MANAGER  or via environment variable  SCONE_MANAGER=MANAGER .     Options  Description      --manager MANAGER  manager of swarm (required unless SCONE_MANAGER is defined)    --verbose  print verbose messages    --debug  print debug messages    --help  print usage of this command", 
            "title": "scone service registry [OPTIONS]"
        }, 
        {
            "location": "/SCONE_SERVICE/#scone-service-pull-options-repositoryimagetag", 
            "text": "Pulls an image from a repository (typically, docker hub) and stores\nthe image in the local registry. First, create this local repository with  $  scone service registry --manager MANAGER  It is expected that the name of the image to be pulled has the \nfollowing format:     REPOSITORY/IMAGE [ :TAG ]   After pulling the image, it is locally available in the swarm.\nThe name of the image id now:     localhost:5000/IMAGE [ :TAG ]   The identity of MANAGER is given via option --manager or via\nenvironment variable SCONE_MANAGER.     Options  Description      --manager MANAGER  manager of swarm (required)    --verbose  print verbose messages    --debug  print debug messages    --help  print usage of this command", 
            "title": "scone service pull [OPTIONS] REPOSITORY/IMAGE[:TAG]"
        }, 
        {
            "location": "/SCONE_SERVICE/#example", 
            "text": "To ensure that a registry is running on the swarm managed by node  faye , execute:  $  export   SCONE_MANAGER = faye\n$ scone service registry  To pull image  sconecuratedimages/sconetainer:shielded  from docker hub and store it in the local registry, execute  $ scone service pull sconecuratedimages/sconetainer:shielded  You can then start this image as a service on the swarm managed by  faye  as follows:  $ scone service create --name nginx-shielded --detach = true   --publish  8090 :8080 --publish  8092 :8082 localhost:5000/sconetainer:shielded    scontain.com , December 2017.  Questions or Suggestions?", 
            "title": "Example"
        }, 
        {
            "location": "/SCONE_STACK/", 
            "text": "scone stack\n\n\nscone stack\n manages remote docker stacks. \nscone stack\n is mainly a thin wrapper around \ndocker stack\n that  forwards the commands to a remote swarm - instead of executing the commands locally.\n\n\nThe remote swarm is specified via command line option \n--manager MANAGER\n. Alternatively, one can export an environment variable \nSCONE_MANAGER\n - which defines the default swarm to be used by \nscone swarm\n, in case option \n--manager\n is not given.\n\n\nscone stack commands\n\n\n\n\n\n\n\n\nscone stack\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\ndeploy\n\n\ndeploy a new stack or update an existing stack\n\n\n\n\n\n\nls\n\n\nlist stacks\n\n\n\n\n\n\nps\n\n\nlist the tasks in the stack\n\n\n\n\n\n\nrm\n\n\nremove one or more stacks\n\n\n\n\n\n\nservices\n\n\nlist the services in the stack\n\n\n\n\n\n\n\n\nscone stack deploy\n\n\nThe command \ndeploy\n is used to create a set of services specified by a \nstack file\n. \n\n\nscone stack deploy\n expects the \nstack file\n specified via option \n-compose-file\n or \n-c\n. We expect this file to reside on your local machine/container (i.e., where you start  \nscone stack deploy\n).   \n\n\nscone stack deploy\n copies the stack file to the remote swarm manager before executing \ndocker stack deploy\n on the swarm manager.\n\n\nExample\n\n\nTo deploy a stack that is called \nnginx\n on a remote swarm managed by node \nfaye\n, execute the following:\n\n\n$ scone stack deploy --compose-file compose.yml --manager faye nginx\n\n\n\n\nIn case you set the environment variable \nSCONE_MANAGER\n to \nfaye\n, you can drop option \n--manager\n:\n\n\n$ \nexport\n \nSCONE_MANAGER\n=\nfaye\n$ scone stack deploy --compose-file compose.yml nginx\n\n\n\n\n \nscontain.com\n, December 2017. \nQuestions or Suggestions?", 
            "title": "scone stack"
        }, 
        {
            "location": "/SCONE_STACK/#scone-stack", 
            "text": "scone stack  manages remote docker stacks.  scone stack  is mainly a thin wrapper around  docker stack  that  forwards the commands to a remote swarm - instead of executing the commands locally.  The remote swarm is specified via command line option  --manager MANAGER . Alternatively, one can export an environment variable  SCONE_MANAGER  - which defines the default swarm to be used by  scone swarm , in case option  --manager  is not given.", 
            "title": "scone stack"
        }, 
        {
            "location": "/SCONE_STACK/#scone-stack-commands", 
            "text": "scone stack  Description      deploy  deploy a new stack or update an existing stack    ls  list stacks    ps  list the tasks in the stack    rm  remove one or more stacks    services  list the services in the stack", 
            "title": "scone stack commands"
        }, 
        {
            "location": "/SCONE_STACK/#scone-stack-deploy", 
            "text": "The command  deploy  is used to create a set of services specified by a  stack file .   scone stack deploy  expects the  stack file  specified via option  -compose-file  or  -c . We expect this file to reside on your local machine/container (i.e., where you start   scone stack deploy ).     scone stack deploy  copies the stack file to the remote swarm manager before executing  docker stack deploy  on the swarm manager.", 
            "title": "scone stack deploy"
        }, 
        {
            "location": "/SCONE_STACK/#example", 
            "text": "To deploy a stack that is called  nginx  on a remote swarm managed by node  faye , execute the following:  $ scone stack deploy --compose-file compose.yml --manager faye nginx  In case you set the environment variable  SCONE_MANAGER  to  faye , you can drop option  --manager :  $  export   SCONE_MANAGER = faye\n$ scone stack deploy --compose-file compose.yml nginx    scontain.com , December 2017.  Questions or Suggestions?", 
            "title": "Example"
        }, 
        {
            "location": "/SCONE_SWARM/", 
            "text": "scone swarm\n\n\nCommand \nscone swarm ls\n  lists the nodes of a docker swarm. This is an extension of \ndocker node ls\n in the sense that\n\n\n\n\n\n\none can list the nodes of a swarm managed by some remote host, and\n\n\n\n\n\n\nseveral SCONE-related attributes of the nodes of a swarm are also printed.\n\n\n\n\n\n\nThe SCONE-related attributes need to be updated when nodes leave or join a swarm. While the scone commands try to update the labels whenever they might cause a label change, users might add nodes to a swarm with Docker commands.\n\n\nCommands\n\n\nscone swarm\n supports the following commands:\n\n\n\n\n\n\ncheck\n:  checks that the SCONE-related labels of all nodes of a swarm and corrects these if not correct.\n\n\n\n\n\n\nls\n:  lists all nodes of a swarm and their SCONE-related labels.\n\n\n\n\n\n\nAttributes\n\n\nAs we mentioned above, scone introduces multiple new attributes:\n\n\n\n\n\n\nNODENO\n: each host in the swarm has a unique number in the range [1, \nnumber of swarm nodes\n]. The hosts are alphabetically sorted and the node with the smallest hostname gets assigned NODENO 1 and the host with the largest name, gets assigned the largest NODENO.\n\n\n\n\n\n\n\n\n\nSGX VERSION\n: denotes the SGX version of the CPU of a host:\n\n\n\n\n\n0\n: the host does not support SGX (or, does not have a SGX driver installed)\n\n\n1\n: the host supports SGX version 1\n\n\n2\n: the host supports SGX version 2 (CPUs are not yet available)\n\n\n\n\n\n\n\n\n\n\n\nDOCKER-ENGINE\n: shows the version of the Docker engine that is installed. It will show \nSCONE\n if the latest patched Docker engine is installed.\n\n\n\n\n\n\n\n\n\nSGX-DRIVER\n: shows the version of the SGX driver. It will show \nSCONE\n if the latest patched Intel driver is installed.\n\n\n\n\n\n\n\n\n\nscone swarm ls\n\n\nYou can list all nodes of a swarm and their attributes with the help of command \nscone swarm ls\n.\n\n\nThis command requires you to specify a manager of the swarm with the help of option \n--manager MANAGER\n\n\nExample:\n To list all nodes of a swarm managed by host \nfaye\n, execute\n\n\n$ scone swarm ls --manager faye\nNODENO        SGX VERSION   DOCKER-ENGINE SGX-DRIVER    HOST                 STATUS     AVAILABILITY  MANAGER   \n\n1\n             \n1\n             SCONE         SCONE         dorothy              Ready      Active                  \n\n2\n             \n1\n             SCONE         SCONE         edna                 Ready      Active                  \n\n3\n             \n1\n             SCONE         SCONE         faye                 Ready      Active        Leader   \n\n\n\n\nTo list the nodes of the swarm managed by host \nalice\n, execute\n\n\n$ scone swarm ls --manager alice\nNODENO        SGX VERSION   DOCKER-ENGINE SGX-DRIVER    HOST                 STATUS     AVAILABILITY  MANAGER   \n\n1\n             \n1\n             SCONE         SCONE         alice                Ready      Active        Leader    \n\n2\n             \n1\n             SCONE         SCONE         beatrix              Ready      Active                  \n\n3\n             \n1\n             SCONE         SCONE         caroline             Ready      Active                  \n\n\n\n\nEnvironment Variable\n\n\nIn case you mainly work with one swarm, you can set environment variable \nSCONE_MANAGER\n. If option \n--manager\n is not specified and \nSCONE_MANAGER\n is defined, the value stored in \nSCONE_MANAGER\n  is used as the name of the manager.\n\n\nExample:\n To list all nodes of a swarm managed by host \nfaye\n:\n\n\n$ \nexport\n \nSCONE_MANAGER\n=\nfaye\n$ scone swarm ls \nNODENO        SGX VERSION   DOCKER-ENGINE SGX-DRIVER    HOST                 STATUS     AVAILABILITY  MANAGER   \n\n1\n             \n1\n             SCONE         SCONE         dorothy              Ready      Active                  \n\n2\n             \n1\n             SCONE         SCONE         edna                 Ready      Active                  \n\n3\n             \n1\n             SCONE         SCONE         faye                 Ready      Active        Leader   \n\n\n\n\nscone swarm check\n\n\nscone\n stores the attributes of a node using Docker: the attributes of a node are stored as node \nlabels\n. For example, the attributes if a node supports SGX (\nSGX VERSION\n), if the patched docker engine (\nDOCKER-ENGINE\n) and the patched Intel driver (\nSGX-DRIVER\n) is installed are all stored as labels. \n\n\nAttributes might change over time. After a node departs from a swarm or when a node joins a swarm, we need to update the labels. Otherwise, the Docker scheduler might not properly schedule containers on the nodes of a swarm. Also, when nodes of a swarm are listed, warnings might be issued.\n\n\nTo check and update the labels of the swarm nodes, you can execute command \nscone swarm check\n. You must specify the manager of the swarm by defining option \n--manager MANAGER\n or by defining environment variable \nSCONE_MANAGER\n.\n\n\nExample:\n To check the labels of the swarm managed by node \nfaye\n, execute:\n\n\n$ scone swarm check --manager faye --verbose\n\n\n\n\nGeneral options\n\n\n\n\n\n\n--help\n (or, \n-h\n): issue help message for object **host*. If a command is specified, it issues a help message specific to this command.\n\n\n\n\n\n\n--debug\n (or, \n-x\n): display all commands that are executed by \nscone host\n. This can be helpful in case commands fail. When you submit a support request regarding a failed command, please send a copy of the output of the failing command with \n--debug\n set.\n\n\n\n\n\n\n--verbose\n (or, \n-v\n): display all commands that are executed by \nscone host\n. This can be helpful in case commands fail. When you submit a support request regarding a failed command, please send a copy of the log of the output that includes \n\n\n\n\n\n\nScreencast\n\n\n\n\n \nscontain.com\n, December 2017. \nQuestions or Suggestions?", 
            "title": "scone swarm"
        }, 
        {
            "location": "/SCONE_SWARM/#scone-swarm", 
            "text": "Command  scone swarm ls   lists the nodes of a docker swarm. This is an extension of  docker node ls  in the sense that    one can list the nodes of a swarm managed by some remote host, and    several SCONE-related attributes of the nodes of a swarm are also printed.    The SCONE-related attributes need to be updated when nodes leave or join a swarm. While the scone commands try to update the labels whenever they might cause a label change, users might add nodes to a swarm with Docker commands.", 
            "title": "scone swarm"
        }, 
        {
            "location": "/SCONE_SWARM/#commands", 
            "text": "scone swarm  supports the following commands:    check :  checks that the SCONE-related labels of all nodes of a swarm and corrects these if not correct.    ls :  lists all nodes of a swarm and their SCONE-related labels.", 
            "title": "Commands"
        }, 
        {
            "location": "/SCONE_SWARM/#attributes", 
            "text": "As we mentioned above, scone introduces multiple new attributes:    NODENO : each host in the swarm has a unique number in the range [1,  number of swarm nodes ]. The hosts are alphabetically sorted and the node with the smallest hostname gets assigned NODENO 1 and the host with the largest name, gets assigned the largest NODENO.     SGX VERSION : denotes the SGX version of the CPU of a host:   0 : the host does not support SGX (or, does not have a SGX driver installed)  1 : the host supports SGX version 1  2 : the host supports SGX version 2 (CPUs are not yet available)      DOCKER-ENGINE : shows the version of the Docker engine that is installed. It will show  SCONE  if the latest patched Docker engine is installed.     SGX-DRIVER : shows the version of the SGX driver. It will show  SCONE  if the latest patched Intel driver is installed.", 
            "title": "Attributes"
        }, 
        {
            "location": "/SCONE_SWARM/#scone-swarm-ls", 
            "text": "You can list all nodes of a swarm and their attributes with the help of command  scone swarm ls .  This command requires you to specify a manager of the swarm with the help of option  --manager MANAGER  Example:  To list all nodes of a swarm managed by host  faye , execute  $ scone swarm ls --manager faye\nNODENO        SGX VERSION   DOCKER-ENGINE SGX-DRIVER    HOST                 STATUS     AVAILABILITY  MANAGER    1               1              SCONE         SCONE         dorothy              Ready      Active                   2               1              SCONE         SCONE         edna                 Ready      Active                   3               1              SCONE         SCONE         faye                 Ready      Active        Leader     To list the nodes of the swarm managed by host  alice , execute  $ scone swarm ls --manager alice\nNODENO        SGX VERSION   DOCKER-ENGINE SGX-DRIVER    HOST                 STATUS     AVAILABILITY  MANAGER    1               1              SCONE         SCONE         alice                Ready      Active        Leader     2               1              SCONE         SCONE         beatrix              Ready      Active                   3               1              SCONE         SCONE         caroline             Ready      Active", 
            "title": "scone swarm ls"
        }, 
        {
            "location": "/SCONE_SWARM/#environment-variable", 
            "text": "In case you mainly work with one swarm, you can set environment variable  SCONE_MANAGER . If option  --manager  is not specified and  SCONE_MANAGER  is defined, the value stored in  SCONE_MANAGER   is used as the name of the manager.  Example:  To list all nodes of a swarm managed by host  faye :  $  export   SCONE_MANAGER = faye\n$ scone swarm ls \nNODENO        SGX VERSION   DOCKER-ENGINE SGX-DRIVER    HOST                 STATUS     AVAILABILITY  MANAGER    1               1              SCONE         SCONE         dorothy              Ready      Active                   2               1              SCONE         SCONE         edna                 Ready      Active                   3               1              SCONE         SCONE         faye                 Ready      Active        Leader", 
            "title": "Environment Variable"
        }, 
        {
            "location": "/SCONE_SWARM/#scone-swarm-check", 
            "text": "scone  stores the attributes of a node using Docker: the attributes of a node are stored as node  labels . For example, the attributes if a node supports SGX ( SGX VERSION ), if the patched docker engine ( DOCKER-ENGINE ) and the patched Intel driver ( SGX-DRIVER ) is installed are all stored as labels.   Attributes might change over time. After a node departs from a swarm or when a node joins a swarm, we need to update the labels. Otherwise, the Docker scheduler might not properly schedule containers on the nodes of a swarm. Also, when nodes of a swarm are listed, warnings might be issued.  To check and update the labels of the swarm nodes, you can execute command  scone swarm check . You must specify the manager of the swarm by defining option  --manager MANAGER  or by defining environment variable  SCONE_MANAGER .  Example:  To check the labels of the swarm managed by node  faye , execute:  $ scone swarm check --manager faye --verbose", 
            "title": "scone swarm check"
        }, 
        {
            "location": "/SCONE_SWARM/#general-options", 
            "text": "--help  (or,  -h ): issue help message for object **host*. If a command is specified, it issues a help message specific to this command.    --debug  (or,  -x ): display all commands that are executed by  scone host . This can be helpful in case commands fail. When you submit a support request regarding a failed command, please send a copy of the output of the failing command with  --debug  set.    --verbose  (or,  -v ): display all commands that are executed by  scone host . This can be helpful in case commands fail. When you submit a support request regarding a failed command, please send a copy of the log of the output that includes", 
            "title": "General options"
        }, 
        {
            "location": "/SCONE_SWARM/#screencast", 
            "text": "scontain.com , December 2017.  Questions or Suggestions?", 
            "title": "Screencast"
        }, 
        {
            "location": "/SCONE_VOLUME/", 
            "text": "scone volume\n\n\nscone volume\n provides functionality to create \nvolumes\n that are available on all nodes of a swarm. This is handy in case you want to be able to schedule a service on any node of a swarm and still be able to give it access to  volumes.\n\n\nThe underlying technology of \nscone volume\n is \ninfinit\n - a software created by a Docker Inc subsidiary. Please read the \ninfinit\n documentation to understand the infinit concepts of \nuser\n, \nnetwork\n, and \nsilo\n.\n\n\nNOTE: infinit is labeled as alpha software: use this only for development or testing\n. \n\n\nThe performance of \ninfinit\n needs to be improved. Hence, for production, one would most likely install a more mature distributed file system on the swarm nodes.\n\n\nThe remote swarm is specified via command line option \n--manager MANAGER\n. Alternatively, one can export an environment variable \nSCONE_MANAGER\n - which defines the default swarm to be used by \nscone swarm\n, in case option \n--manager\n is not given.\n\n\nscone volume commands\n\n\n\n\n\n\n\n\nscone volume\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\ninstall\n\n\ninstall infinit on all nodes of a swarm.\n\n\n\n\n\n\ncreate\n\n\ncreate new volume and install infinit storage platform if required\n\n\n\n\n\n\ncheck\n\n\ncheck that all volumes are available on all hosts\n\n\n\n\n\n\ndelete\n\n\ndelete a volume\n\n\n\n\n\n\n\n\nscone volume install [OPTIONS]\n\n\nscone volume install\n ensures that infinit is installed on all nodes of a swarm.\n\n\n\n\n\n\n\n\nOptions\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\n--manager MANAGER\n\n\nmanager of swarm (required)\n\n\n\n\n\n\n--as user\n\n\nrun as user (default random id)\n\n\n\n\n\n\n--network name\n\n\nname of network to use/create (default scone-network)\n\n\n\n\n\n\n--silo name\n\n\nname of silo to use/create (default scone-silo)\n\n\n\n\n\n\n--capacity sz\n\n\nsize of silo in GB (default 10)\n\n\n\n\n\n\n--help\n\n\nshow this help message\n\n\n\n\n\n\n\n\nNotes:\n\nIf you do not specify a default \nuser\n via option \n--user USER\n, a random default user is created. \n\n\nIf you do not specify a network name via option \n--network name\n, the default network name is set to \nscone-network\n. \n\n\nYou can specify a silo name via potion \n--silo name\n. The default silo name is \n\"scone-silo\"\n. \n\n\nThe capacity of the silo (in GB) is given via  \n--capacity name\n. The default capacity is 10GB.\n\n\n**Examples: **\n\n\nTo install infinit with a storage capacity of 15 GB on each node, execute the following:\n\n\n$ scone volume  install --verbose --manager alice  --capacity \n15\n\n\n\n\n\nIf you want to keep explicit control over user and network names, execute the following:\n\n\n$ scone volume  install --verbose --manager alice  --as scone --capacity \n5\n --silo  my-silo --network scone-networkg\n\n\n\n\nIf you want to see the user and network names for a given swarm, execute:\n\n\n$ scone volume install --manager alice --help\n\n\n\n\nscone volume create\n\n\nscone volume create\n creates a new infinit volume for a given user (option \n--as USER\n) \nand with a given volume name (via option \n--name VOLUME\n). If required, it (re-)installs the infinit storage platform. If you are sure that the infinit is already properly installed, pass the \n--fast\n option to avoid checks and reinstallation of infinit.\n\n\nThis volume will be available on all hosts of a swarm at location:\n   \n/mnt/infinit/USER/VOLUME\n\n\n\n\n\n\n\n\nOptions\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\n--manager HOST\n\n\nmanager of swarm (required)\n\n\n\n\n\n\n--name volume\n\n\nname of volume to create (required)\n\n\n\n\n\n\n--help\n\n\nshow this help message\n\n\n\n\n\n\n--as user\n\n\nrun as user (default )\n\n\n\n\n\n\n--network name\n\n\nname of network to use/create (default )\n\n\n\n\n\n\n--silo name\n\n\nname of silo to use/create (default )\n\n\n\n\n\n\n--capacity sz\n\n\nsize of silo in GB (default )\n\n\n\n\n\n\n--fast\n\n\ncreate volume without reinstalling all software (default=false)\n\n\n\n\n\n\n--verbose\n\n\nprint location of created volume\n\n\n\n\n\n\n\n\n**Example: **\n\n\nIn case you already installed infinit on your swarm and you just want to create a new volume,\nexecute the following:\n\n\n$ scone volume  create --verbose --fast --name new-volume --verbose\n\n\n\n\nIn case you do not know if infinit is already installed when you create a volume and\nyour want to keep control over the names used by infinit, execute the following:\n\n\n$ scone volume  create --verbose --manager alice  --as scone --name  my-volume --capacity \n5\n --silo  my-silo --network scone-network  --export config\n\n\n\n\nscone volume check\n\n\nChecks if all created volumes are available on all swarm nodes.\n\n\nNote: the current implementation of **scone volume check\n uses the metadata stored in the container/environment in which \nscone volume check\n executes. In case you created some volumes in a different container, the checks will complain.**  Will plan to fix this issue in a future version of \nscone volume check\n.\n\n\n\n\n\n\n\n\nOptions\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\n--manager HOST\n\n\nmanager of swarm (required)\n\n\n\n\n\n\n\n\nExample:\n \n\n\n$ scone volume  check --verbose --manager alice\n\n\n\n\nscone volume delete\n\n\nscone volume delete\n deletes a volume. This removes the volume from all nodes of a swarm.\n\n\n\n\n\n\n\n\nOptions\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\n--manager HOST\n\n\nmanager of swarm (required)\n\n\n\n\n\n\n--name volume\n\n\nname of volume to be deleted (required)\n\n\n\n\n\n\n\n\nExample:\n\n\n$ scone volume  delete --verbose --manager alice --volume new-volume\n\n\n\n\nExample\n\n\nTo install \ninfinit\n on all nodes of a swarm managed by node \nfaye\n and reserve a capacity of 15GB per node, execute the following:\n\n\n$ scone volume  install --verbose --manager faye  --capacity \n15\n \n\n\n\n\nTo create a volume named \ntest_scone_volume\n for some arbitrary infinit user \ntest_scone_user\n, \nexecute the following:\n\n\n$ scone volume create --verbose --manager faye --as test_scone_user --name  test_scone_volume \n\n\n\n\nNote that the infinit user \ntest_scone_user\n is automatically created if it does not yet exists.\n\n\nAfter completion of this command, you will have access to the newly created volume at location \n/mnt/infinit/test_scone_user/test_scone_volume\n on all nodes of the swarm.\n\n\nYou can give a service access to this volume with the help of its stack file.\nSay, the service needs to access this mapped at \n/mnt/vol\n  as follows:\n\n\nmyservice\n:\n\n   \nvolumes\n:\n\n        \n-\n \n/mnt/infinit/test_scone_user/test_scone_volume:/mnt/\nvol\n\n\n\n\n\nNote that \nscone volume create\n will check that \ninfinit\n is properly installed and will reinstall parts that\nare missing. If you are sure that infinit is properly installed, you can add option \n--fast\n to omit the checks that \ninfinit\n is properly installed.\n\n\nTo create a volume \ntest_scone2_volume\n on a swarm managed by \nfaye\n when you know that \ninfinit\n is installed, just executed\n\n\n$ scone volume  create --verbose --fast --manager faye --as test_scone_user  --name  test_scone2_volume \n\n\n\n\nTo check that the volumes on the swarm are properly installed, execute command \ncheck\n:\n\n\n$ scone volume  check --verbose --manager faye \n\n\n\n\nTo delete a volume, we provide command \ndelete\n. For example, to delete volume \ntest_scone2_volume\n, execute:\n\n\n$ scone volume delete --verbose --manager faye  --as test_scone_user --name  test_scone2_volume\n\n\n\n\n \nscontain.com\n, December 2017. \nQuestions or Suggestions?", 
            "title": "scone volume"
        }, 
        {
            "location": "/SCONE_VOLUME/#scone-volume", 
            "text": "scone volume  provides functionality to create  volumes  that are available on all nodes of a swarm. This is handy in case you want to be able to schedule a service on any node of a swarm and still be able to give it access to  volumes.  The underlying technology of  scone volume  is  infinit  - a software created by a Docker Inc subsidiary. Please read the  infinit  documentation to understand the infinit concepts of  user ,  network , and  silo .  NOTE: infinit is labeled as alpha software: use this only for development or testing .   The performance of  infinit  needs to be improved. Hence, for production, one would most likely install a more mature distributed file system on the swarm nodes.  The remote swarm is specified via command line option  --manager MANAGER . Alternatively, one can export an environment variable  SCONE_MANAGER  - which defines the default swarm to be used by  scone swarm , in case option  --manager  is not given.", 
            "title": "scone volume"
        }, 
        {
            "location": "/SCONE_VOLUME/#scone-volume-commands", 
            "text": "scone volume  Description      install  install infinit on all nodes of a swarm.    create  create new volume and install infinit storage platform if required    check  check that all volumes are available on all hosts    delete  delete a volume", 
            "title": "scone volume commands"
        }, 
        {
            "location": "/SCONE_VOLUME/#scone-volume-install-options", 
            "text": "scone volume install  ensures that infinit is installed on all nodes of a swarm.     Options  Description      --manager MANAGER  manager of swarm (required)    --as user  run as user (default random id)    --network name  name of network to use/create (default scone-network)    --silo name  name of silo to use/create (default scone-silo)    --capacity sz  size of silo in GB (default 10)    --help  show this help message     Notes: \nIf you do not specify a default  user  via option  --user USER , a random default user is created.   If you do not specify a network name via option  --network name , the default network name is set to  scone-network .   You can specify a silo name via potion  --silo name . The default silo name is  \"scone-silo\" .   The capacity of the silo (in GB) is given via   --capacity name . The default capacity is 10GB.  **Examples: **  To install infinit with a storage capacity of 15 GB on each node, execute the following:  $ scone volume  install --verbose --manager alice  --capacity  15   If you want to keep explicit control over user and network names, execute the following:  $ scone volume  install --verbose --manager alice  --as scone --capacity  5  --silo  my-silo --network scone-networkg  If you want to see the user and network names for a given swarm, execute:  $ scone volume install --manager alice --help", 
            "title": "scone volume install [OPTIONS]"
        }, 
        {
            "location": "/SCONE_VOLUME/#scone-volume-create", 
            "text": "scone volume create  creates a new infinit volume for a given user (option  --as USER ) \nand with a given volume name (via option  --name VOLUME ). If required, it (re-)installs the infinit storage platform. If you are sure that the infinit is already properly installed, pass the  --fast  option to avoid checks and reinstallation of infinit.  This volume will be available on all hosts of a swarm at location:\n    /mnt/infinit/USER/VOLUME     Options  Description      --manager HOST  manager of swarm (required)    --name volume  name of volume to create (required)    --help  show this help message    --as user  run as user (default )    --network name  name of network to use/create (default )    --silo name  name of silo to use/create (default )    --capacity sz  size of silo in GB (default )    --fast  create volume without reinstalling all software (default=false)    --verbose  print location of created volume     **Example: **  In case you already installed infinit on your swarm and you just want to create a new volume,\nexecute the following:  $ scone volume  create --verbose --fast --name new-volume --verbose  In case you do not know if infinit is already installed when you create a volume and\nyour want to keep control over the names used by infinit, execute the following:  $ scone volume  create --verbose --manager alice  --as scone --name  my-volume --capacity  5  --silo  my-silo --network scone-network  --export config", 
            "title": "scone volume create"
        }, 
        {
            "location": "/SCONE_VOLUME/#scone-volume-check", 
            "text": "Checks if all created volumes are available on all swarm nodes.  Note: the current implementation of **scone volume check  uses the metadata stored in the container/environment in which  scone volume check  executes. In case you created some volumes in a different container, the checks will complain.**  Will plan to fix this issue in a future version of  scone volume check .     Options  Description      --manager HOST  manager of swarm (required)     Example:    $ scone volume  check --verbose --manager alice", 
            "title": "scone volume check"
        }, 
        {
            "location": "/SCONE_VOLUME/#scone-volume-delete", 
            "text": "scone volume delete  deletes a volume. This removes the volume from all nodes of a swarm.     Options  Description      --manager HOST  manager of swarm (required)    --name volume  name of volume to be deleted (required)     Example:  $ scone volume  delete --verbose --manager alice --volume new-volume", 
            "title": "scone volume delete"
        }, 
        {
            "location": "/SCONE_VOLUME/#example", 
            "text": "To install  infinit  on all nodes of a swarm managed by node  faye  and reserve a capacity of 15GB per node, execute the following:  $ scone volume  install --verbose --manager faye  --capacity  15    To create a volume named  test_scone_volume  for some arbitrary infinit user  test_scone_user , \nexecute the following:  $ scone volume create --verbose --manager faye --as test_scone_user --name  test_scone_volume   Note that the infinit user  test_scone_user  is automatically created if it does not yet exists.  After completion of this command, you will have access to the newly created volume at location  /mnt/infinit/test_scone_user/test_scone_volume  on all nodes of the swarm.  You can give a service access to this volume with the help of its stack file.\nSay, the service needs to access this mapped at  /mnt/vol   as follows:  myservice : \n    volumes : \n         -   /mnt/infinit/test_scone_user/test_scone_volume:/mnt/ vol   Note that  scone volume create  will check that  infinit  is properly installed and will reinstall parts that\nare missing. If you are sure that infinit is properly installed, you can add option  --fast  to omit the checks that  infinit  is properly installed.  To create a volume  test_scone2_volume  on a swarm managed by  faye  when you know that  infinit  is installed, just executed  $ scone volume  create --verbose --fast --manager faye --as test_scone_user  --name  test_scone2_volume   To check that the volumes on the swarm are properly installed, execute command  check :  $ scone volume  check --verbose --manager faye   To delete a volume, we provide command  delete . For example, to delete volume  test_scone2_volume , execute:  $ scone volume delete --verbose --manager faye  --as test_scone_user --name  test_scone2_volume    scontain.com , December 2017.  Questions or Suggestions?", 
            "title": "Example"
        }, 
        {
            "location": "/SCONE_Fileshield/", 
            "text": "SCONE File Protection\n\n\nSCONE supports the transparent encryption and/or authentication of files. By \ntransparent\n, we mean that there are no application code changes needed to support this. We support two ways to use the SCONE file protection:\n\n\n\n\na \nlow-level interface\n intended to be used at the developer site. We assumet that the developer machine is sufficiently trust worthy. This is made available via command \nscone fspf\n and described in this document.\n\n\na \nhigh-level interface\n simplifies the use of the file protection and it does and in particular, takes care of key management. (\nThe high-level interface is not yet available\n).\n\n\n\n\nConcepts\n\n\nThe underlying idea of SCONE file protection is that a user specifies that each file is \neither\n: \n\n\n\n\n\n\nauthenticated\n, i.e., SCONE checks that the content was not modified by some unauthorized entity,\n\n\n\n\n\n\nencrypted\n, i.e., the confidentiality is protected by encryption. Encrypted files are always authenticated, or \n\n\n\n\n\n\nnot-protected\n, i.e. SCONE reads and write  the files without any extra protection mechanisms. For example, you might use \nnot-protected\n if your application already encrypts its files or if you need direct access to devices.\n\n\n\n\n\n\nMarking all files individually as either \nauthenticated\n, \nencrypted\n, or \nnot-protected\n would not be very practical. Hence, we support to partition the filesystem into \nregions\n: regions do not overlap and each file belongs to exactly one region.\n\n\nA region is defined by a path. For example, region \n/\n is the root region and you could, for example, specify that all files in region \n/\n must be authenticated. You can define a second region, for example,  region \n/data/db\n and that this region is encrypted.\n\n\nEach file belongs to exactly one region: it belongs to the region that has the longest common path prefix with this file. For example, file \n/etc/db.conf\n would belong, in this case, to region \n/\n and file \n/data/db/table.db\n would belong to region \n/data/db\n.\n\n\nSCONE supports \nephemeral\n regions: files are stored in main memory outside of the enclave. Since the main memory is not protected, we recommend that an ephemeral regions is either authenticated or encrypted.  When a program starts, all its ephemeral regions are empty. The only way to add files to an ephemeral region is by the application writing to this region. \nAll files in an ephemeral region are lost when the application exits.\n\n\nAll files that need to be persistent should be stored in a non-ephemeral region instead. We refer to this as a \nkernel\n  region. For each region, you need to specify if the region is either \nephemeral\n or \nkernel\n.\n\n\nEach region belongs to one of the following six classes:\n   \n{\nephemeral\n | \nkernel\n} X {\nnot-protected\n | \nauthenticated\n | \nencrypted\n }\n\n\nExample\n\n\nSometimes, we might only need to protect the files that are passed to a container via some volume. In this case, it would be sufficient that the volume is either authenticated or encrypted.\n\n\nLet us demonstrate this via a simple example in which we pass an encrypted volume to a container. We create this encrypted volume in our local filesystem (in directory \nvolume\n) and we will later mount this in the container as \n/data\n. The original (non-encrypted) files are stored in directory \ndata-original\n.\n\n\n mkdir -p volume\n\n mkdir -p data-original\n\n\n\n\nLet's write some files in the \ndata-original\n directory:\n\n\ncat \n data-original/hello.txt \n EOF\n\n\nHello World\n\n\nEOF\n\ncat \n data-original/world.py \n EOF\n\n\nf = open(\n/data/hello.txt\n, \nr\n)\n\n\nprint str(f.read())\n\n\nEOF\n\n\n\n\n\nLet's check that \nvolume\n is empty and we print the hash values of the two files in \ndata-original\n:\n\n\n ls volume\n\n shasum data-original/*\n648a6a6ffffdaa0badb23b8baf90b6168dd16b3a  data-original/hello.txt\ndeda99d44e880ea8f2250f45c5c20c15d568d84c  data-original/world.py\n\n\n\n\nNow, we start the SCONE crosscompiler in a container to create the encrypted volume:\n\n\n docker run -it -v \n$PWD\n/volume:/data\n -v \n$PWD\n/data-original:/data-original\n sconecuratedimages/crosscompilers:scone\n\n\n\n\nFile System Protection File\n\n\nAll the metadata required for checking the consistency of the files is stored in a \nfile system protection file\n, or, short \nfspf\n. SCONE supports multiple *fspf*s. \n\n\nLet's start with a simple example with a single \nfspf\n. The \nfspf\n file is created via command \nscone fspf create\n and let us name this file \nfspf.pb\n. We execute the following commands\ninside the container (as indicated by the $ prompt):\n\n\n$ \ncd\n /data\n$ scone fspf create fspf.pb\nCreated empty file system protection file in fspf.pb. AES-GCM tag: 0e3da7ad62f5bc7c7bb08c67b16f2423\n\n\n\n\nWe can now split the file system in \nregions\n, a region is a subtree. You can add regions to a \nfspf\n with the help of command \nscone fspf addr\n.\n\n\nEach region has exactly one of the following properties:\n\n\n\n\nauthenticated\n: the integrity of files is checked, i.e., any unauthorized modification of this file is detected and results in a reading error inside of the enclave. Specify command line option \n--authenticated\n.\n\n\nencrypted\n: the confidentiality and integrity of files is protected, i.e., encrypted always implies that the files are also authenticated. Specify command line option \n--encrypted\n.\n\n\nnot-protected\n: files are neither authenticated nor encrypted. Specify command line option \n--not-protected\n.\n\n\n\n\nFile system changes of containers are typically ephemeral in the sense that file updates are lost when a container terminates. When specifying option \n--ephemeral\n, files in this region are not written to disk, the are written to an in memory file system instead. \n\n\nSay for now, that by default we do not protect files and we want to read files and write  back  changed files to the file system. To do so, we define that the root tree is  \n--kernel\n as well as  \n--not-protected\n:\n\n\n$ scone fspf addr fspf.pb / --kernel / --not-protected\nAdded region / to file system protection file fspf.pb new AES-GCM tag: dd961af10b5aaa5cb1044c35a3f42c84\n\n\n\n\nLet us add another region \n/data\n that should be encrypted and persisted. To encrypt the files,  we specify option \n--encrypted\n. We specify option \n--kernel\n followed by a path (here, also \n/data\n) to request that files in this region are written to directory \n/data\n.\n\n\n$ scone fspf addr fspf.pb /data --encrypted --kernel /data\nAdded region /data to file system protection file fspf.pb new AES-GCM tag: 8481369d3ffdd9b6aeb30d044bf5c1c7\n\n\n\n\nThe encryption key for a file is chosen at random and stored in \nfspf.pb\n. We use the Intel random number generator \nRdRand\n to generate the key. The default key length of a region is 32 bytes. Alternatives are key length of 16 and 24 bytes. These can be selected via option  \n--key-length 16\n and \n--key-length 24\n when creating a region with command \nscone fspf addr\n.\n\n\nNow, that we defined the regions, i.e., \n/\n and \n/data\n, we can add files to region \n/data\n. Let's just add all files in \n/data-original\n and encrypt these and write the encrypted files to \n/data\n:\n\n\n$ scone fspf addf fspf.pb /data /data-original /data\nAdded files to file system protection file fspf.pb new AES-GCM tag: 39a268166e628cf76e3fca80aa2d4f63\n\n\n\n\nNote\n that if we add files to an authenticated file region, \naddf\n will not copy or encrypt any data. It will\nonly add  the file names and the checksums (tags) of the files located in \n/data-original\n to \nfspf.pb\n. Also, you should drop the directory to which encrypted files are written (i.e., in the above case \n/data\n) when adding files with \nscone fspf addf\n. In this case, you would probably mount directory \n/data-original\n as \n/data\n in the container.\n\n\nComing back to the above example, we can now compare the hash values of the original files and the encrypted files:\n\n\n$ shasum /data/*\nshasum /data/*\n87fd97468024e3d2864516ff5840e15d9615340d  /data/fspf.pb\n31732914910f4a08b9832c442074b0932915476c  /data/hello.txt\n8d07f3f576785c373a5e70e8dbcfa8ee06ca6d0c  /data/world.py\n\n\n\n\nThe fspf itself is not yet encrypted. We encrypt this file via command\n\nscone fspf encrypt fspf.pb\n\n\n$ scone fspf encrypt fspf.pb \n /data-original/keytag\n\n\nWe store the random encryption key as well as the tag of file \nfspf.pb\n in\nfile  \n/data-original/keytag\n.\n\n\nWe introduce a very simple program that reads the two files:\n\n\n$ cat \n example.c \n EOF\n\n\n#include \nstdio.h\n\n\n#include \nstdlib.h\n\n\n\nvoid printfile(const char* fn) {\n\n\n    FILE *fp = fopen(fn, \nr\n);\n\n\n    char c;\n\n\n    while((c=fgetc(fp))!=EOF\n){\n\n        printf\n(\n%c\n,c\n)\n;\n\n    \n}\n\n    fclose\n(\nfp\n)\n;\n\n\n}\n\n\nint main\n()\n \n{\n\n    printfile\n(\n/data/hello.txt\n)\n;\n\n    printfile\n(\n/data/world.py\n)\n;\n\n\n}\n\nEOF\n\n\n\n\nLet's crosscompile this program:\n\n\nscone gcc example.c -o example\n\n\n\n\nExecuting this program results in an output like this:\n\n\n$./example\nR??C?\n    q?z??E??\n|\n\u042e?\n}\n\u00fc ?o\n\n$?\n?!rga??\u0387*\n`\n?????????Gw?\n\n\n\n\nWe need to activate the file system shield via environment variables by setting the location of the file system protection file (in \nSCONE_FSPF\n), the encryption key of the file (in \nSCONE_FSPF_KEY\n) and the tag of the fspf (in \nSCONE_FSPF_TAG\n). \nWe can extract the encryption key as well as the tag of \nfspf.pb\n from file \n/data-original/keytag\n:\n\n\n$ \nexport\n \nSCONE_FSPF_KEY\n=\n$(\ncat /data-original/keytag \n|\n awk \n{print $11}\n)\n\n$ \nexport\n \nSCONE_FSPF_TAG\n=\n$(\ncat /data-original/keytag \n|\n awk \n{print $9}\n)\n\n$ \nexport\n \nSCONE_FSPF\n=\n/data/fspf.pb\n\n\n\n\nWe can now execute this program again:\n\n\n$ ./example\nHello World\n\nf\n \n=\n open\n(\n/data/hello.txt\n, \nr\n)\n\nprint str\n(\nf.read\n())\n\n\n\n\n\nVariables \nSCONE_FSPF_KEY\n, \nSCONE_FSPF_TAG\n and \nSCONE_FSPF\n should only be set manually for debugging since they cannot securely be passed in this way to programs running inside enclaves. To securely pass environment variables, please read the section about \nend-to-end encryption\n.\n\n\nPython\n\n\nLet's try a similar approach for Python. \nIn the above example, we encrypted a Python program. Let's try to execute this encrypted program that accesses an encrypted file:\n\n\ndocker run -it -v \n$PWD\n/volume:/data\n sconecuratedimages/crosscompilers:python27 bash\n\n\n\n\nThe files \n/data/world.py\n and  \n/data/hello.txt\n are encrypted:\n\n\n$ cat /data/world.py\n?\n=\n??J??0?6+?Q?nKd?*N,??.?G???????R?cO?t?y??\nf?\n\n\n\n\nLet's activate the file shield:\n\n\n$ \nexport\n \nSCONE_FSPF_KEY\n=\n... extract from data-original/keytag ...\n$ \nexport\n \nSCONE_FSPF_TAG\n=\n... extract from data-original/keytag ...\n$ \nexport\n \nSCONE_FSPF\n=\n/data/fspf.pb\n\n\n\n\nWe can now run the encrypted \nworld.py\n program with the  the Python interpreter:\n\n\nSCONE_HEAP\n=\n100000000\n \nSCONE_ALPINE\n=\n1\n \nSCONE_VERSION\n=\n1\n /usr/local/bin/python /data/world.py\n\nexport\n \nSCONE_QUEUES\n=\n1\n\n...\nHello World\n\n\n\n\nProtecting the Root Region\n\n\nNote that in the above example, Python will not be permitted to load dynamic libraries  outside of the protected directory \n/data\n: a dynamic library must reside in either an authenticated or an encrypted region.  To deal with this, we must define one or more authenticated or encrypted file regions that contain the dynamic libraries.  \n\n\nLet us show how to authenticate all files in region \n/\n\n\n$ scone fspf addr fspf.pb / --kernel / --authenticated\n\n\n\n\nWe need to add all files that our application might access. Often, these files in the root region might be defined in some container image. Let's see how we can add these files to our region \n/\n.\n\n\nAdding files from an existing container image\n\n\nWe show how to add a subset of the files of container image \nsconecuratedimages/crosscompilers:python27\n to our root region.\nTo do so, we ensure that we have the newest images:\n\n\n docker pull sconecuratedimages/crosscompilers:python27\n\n docker pull sconecuratedimages/crosscompilers:scone\n\n\n\n\nHow can we add all files in a container to the \nfspf\n? One way to do so requires to run Docker inside of a Docker container. To be able to do so, we need to permit our outermost docker container to have access to  \n/var/run/docker.sock\n:\n\n\n docker run -it -v /var/run/docker.sock:/var/run/docker.sock -v \n$PWD\n/volume:/data\n -v \n$PWD\n/data-original:/data-original\n sconecuratedimages/crosscompilers:scone\n\n\n\n\nLet us ensure that Docker is installed in this container:\n\n\napt-get update\napt-get install -y docker.io\n\n\n\n\nNow, we want to add all files of some target container. In our example,\nthis is an instance of image \nsconecuratedimages/crosscompilers:python27\n.\nWe ensure that we pulled the latest image before we start the container:\n\n\nCONTAINER_ID\n=\n`\ndocker run -d sconecuratedimages/crosscompilers:python27 \nprintf\n OK\n`\n \n\n\n\n\nWe can now copy all files from this container into a new directory \nrootvol\n:\n\n\n$ \ncd\n\n$ mkdir -p rootvol\n$ docker cp \n$CONTAINER_ID\n:/ ./rootvol\n\n\n\n\nNow that we have a copy of the files, we should not forget to garbage collect this container:\n\n\ndocker rm \n$CONTAINER_ID\n\n\n\n\n\nLet's remove some directories that we do not want our program to access, like for example, \n/dev\n:\n\n\n$ rm -rf rootvol/dev rootvol/proc rootvol/bin rootvol/media rootvol/mnt rootvol/usr/share/X11 rootvol/usr/share/terminfo rootvol/optrootvol/usr/include/c++/ rootvol/usr/lib/tcl8.6 rootvol/usr/lib/gcc rootvol/opt rootvol/sys rootvol/usr/include/c++\n\n\n\n\nNow, we create a root \nfspf\n:\n\n\n$ scone fspf create fspf.pb\n$ scone fspf addr fspf.pb / --kernel / --authenticated\n$ scone fspf addf fspf.pb / ./rootvol /\n$ scone fspf encrypt fspf.pb \n keytag\n\n\n\n\nWe can now create a new container image with this file system protection file\nusing this Dockerfile\n\n\n$ cat \n Dockerfile \n EOF\n\n\nFROM sconecuratedimages/crosscompilers:python27\n\n\nCOPY fspf.pb /\n\n\nEOF\n\n$ docker build -t sconecuratedimages/crosscompilers:python27-authenticated .\n\n\n\n\nWe can run a container as follows:\n\n$ docker run -it sconecuratedimages/crosscompilers:python27-authenticated sh\n\n\n\nLet us activate the file shield:\n\n\n$ \nexport\n \nSCONE_FSPF_KEY\n=\n... extract from data-original/keytag ...\n$ \nexport\n \nSCONE_FSPF_TAG\n=\n... extract from data-original/keytag ...\n$ \nexport\n \nSCONE_FSPF\n=\n/fspf.pb\n\n\n\n\nLet's run python with authenticated file system:\n\n\nSCONE_HEAP\n=\n1000000000\n \nSCONE_ALLOW_DLOPEN\n=\n2\n  \nSCONE_ALPINE\n=\n1\n \nSCONE_VERSION\n=\n1\n /usr/local/bin/python\n\n\n\n\nChecking the File System Shield\n\n\nLet's us check the file shield by creating a new python program (\nhelloworld-manual.py\n) in\nside of a python container:\n\n\n docker run -i sconecuratedimages/crosscompilers:python27-authenticated sh\n$ cat \n helloworld-manual.py \n EOF\n\n\nprint \nHello World\n\n\nEOF\n\n\n\n\n\nWhen we switch on the file shield, the execution of this program inside the enclave will fail: since this file was not part of the original file system, the file system shield will prevent accessing this file.\n\n\n$ \nexport\n \nSCONE_FSPF_KEY\n=\n... extract from data-original/keytag ...\n$ \nexport\n \nSCONE_FSPF_TAG\n=\n... extract from data-original/keytag ...\n$ \nexport\n \nSCONE_FSPF\n=\n/fspf.pb\n$ \nSCONE_HEAP\n=\n1000000000\n \nSCONE_ALLOW_DLOPEN\n=\n2\n  \nSCONE_ALPINE\n=\n1\n \nSCONE_VERSION\n=\n1\n /usr/local/bin/python helloworld-manual.py\n\n(\nfails\n)\n\n\n\n\n\nWe can, however, add a new file via programs that have access to the key of the \nfspf\n. We can, for example, write a python program to add a new python program to the file system.\n\n\nBy default, we disable that the root fspf is updated. We can enable updates by setting environment variable \nSCONE_FSPF_MUTABLE=1\n. We plan to permit updates of the root fspf by default in the near future (i.e., we will remove variable \nSCONE_FSPF_MUTABLE=1\n).\n\n\n$ \nSCONE_HEAP\n=\n1000000000\n \nSCONE_FSPF_MUTABLE\n=\n1\n \nSCONE_ALLOW_DLOPEN\n=\n2\n  \nSCONE_ALPINE\n=\n1\n \nSCONE_VERSION\n=\n1\n /usr/local/bin/python  \n PYTHON\n\n\nf = open(\nhelloworld.py\n, \nw\n)\n\n\nf.write(\nprint \nHello World\n\\n\n)\n\n\nf.close()\n\n\nPYTHON\n```\n\n\n\n\n\nThe tag of the file system protection file is now changed. We can determine the new TAG with the help of command \nscone fspf show\n:\n\n\n$ \nexport\n \nSCONE_FSPF_TAG\n=\n$(\nscone fspf show --tag /fspf.pb\n)\n\n\n\n\n\nNow, we can run the new \nhelloworld.py\n:\n\n\n$ \nSCONE_HEAP\n=\n1000000000\n \nSCONE_ALLOW_DLOPEN\n=\n2\n  \nSCONE_ALPINE\n=\n1\n \nSCONE_VERSION\n=\n1\n /usr/local/bin/python helloworld.py\n...\nHello World\n\n\n\n\nExtended Example\n\n\nTo learn how to use multiple file system protection files, \nplease have a look at the following screencast.\n\n\n\n\nBelow is the script that is executed in the screencast:\n\n\n docker run -it -v \n$PWD\n:/mnt sconecuratedimages/crosscompilers:scone\n\n$ mkdir -p /example\n$ mkdir -p /mnt/authenticated/\n$ mkdir -p /mnt/encrypted/\n$ \ncd\n /example\n$ mkdir -p .original\n\n$ scone fspf create fspf.pb\n$ scone fspf create authenticated.pb\n$ scone fspf create encrypted.pb\n\n\n# add protection regions\n\n$ scone fspf addr fspf.pb / -e --ephemeral\n$ scone fspf addr authenticated.pb /mnt/authenticated -a --kernel /mnt/authenticated\n$ scone fspf addr encrypted.pb /mnt/encrypted -e --kernel /mnt/encrypted\n\n\n# add files\n\n\n\n# enclave program should expect the files (directories) found by the client in ./original in /mnt/authenticated\n\n$ scone fspf addf authenticated.pb /mnt/authenticated ./original\n\n\n# enclave program should expect the files (directories) found by the client in ./original in encrypted form in /mnt/encrypted\n\n\n# the client will write the encrypted files to ./mnt/encrypted\n\n$ scone fspf addf encrypted.pb /mnt/encrypted ./original ./mnt/encrypted\n\nencrypted_key\n=\n`\nscone fspf encrypt encrypted.pb \n|\n awk \n{print $11}\n`\n\n\n$ \necho\n \nencrypted.pb key: \n${\nencrypted_key\n}\n\n$ scone fspf addfspf fspf.pb authenticated.pb\n$ scone fspf addfspf fspf.pb encrypted.pb \n${\nencrypted_key\n}\n\n$ cat \n example.c \n EOF\n\n\n#include \nstdio.h\n\n\n\nint main() {\n\n\n    FILE *fp = fopen(\n/mnt/authenticated/hello\n, \nw\n);\n\n\n    fprintf(fp, \nhello world\\n\n);\n\n\n    fclose(fp);\n\n\n\n    fp = fopen(\n/mnt/encrypted/hello\n, \nw\n);\n\n\n    fprintf(fp, \nhello world\\n\n);\n\n\n    fclose(fp);\n\n\n}\n\n\nEOF\n\n\n$ scone gcc example.c -o sgxex\n$ cat \n /etc/sgx-musl.conf \n EOF\n\n\nQ 4\n\n\ne -1 0 0\n\n\ns -1 0 0\n\n\ne -1 1 0\n\n\ns -1 1 0\n\n\ne -1 2 0\n\n\ns -1 2 0\n\n\ne -1 3 0\n\n\ns -1 3 0\n\n\nEOF\n\n$ \nSCONE_FSPF\n=\nfspf.pb ./sgxex\n$ cat /mnt/authenticated/hello\n$ cat /mnt/encrypted/hello \n$ cat \n cat.c \n EOF\n\n\n#include \nstdio.h\n\n\n\nint main() {\n\n\n    char buf[80];\n\n\n    FILE *fp = fopen(\n/mnt/authenticated/hello\n, \nr\n);\n\n\n    fgets(buf, sizeof(buf), fp);\n\n\n    fclose(fp);\n\n\n    printf(\nread: \n%s\n\\n\n, buf);\n\n\n\n    fp = fopen(\n/mnt/encrypted/hello\n, \nr\n);\n\n\n    fgets(buf, sizeof(buf), fp);\n\n\n    fclose(fp);\n\n\n    printf(\nread: \n%s\n\\n\n, buf);\n\n\n}\n\n\nEOF\n\n\n$ scone gcc cat.c -o native_cat\n$ ./native_cat\n$ scone gcc cat.c -o sgxcat\n$ \nSCONE_FSPF\n=\nfspf.pb ./sgxcat\n\n\n\n\nNotes\n\n\nThe SCONE File Protection documentation is not yet completed and more information will be provided soon.\n\n\n \nscontain.com\n, January 2018. \nQuestions or Suggestions?", 
            "title": "scone fspf"
        }, 
        {
            "location": "/SCONE_Fileshield/#scone-file-protection", 
            "text": "SCONE supports the transparent encryption and/or authentication of files. By  transparent , we mean that there are no application code changes needed to support this. We support two ways to use the SCONE file protection:   a  low-level interface  intended to be used at the developer site. We assumet that the developer machine is sufficiently trust worthy. This is made available via command  scone fspf  and described in this document.  a  high-level interface  simplifies the use of the file protection and it does and in particular, takes care of key management. ( The high-level interface is not yet available ).", 
            "title": "SCONE File Protection"
        }, 
        {
            "location": "/SCONE_Fileshield/#concepts", 
            "text": "The underlying idea of SCONE file protection is that a user specifies that each file is  either :     authenticated , i.e., SCONE checks that the content was not modified by some unauthorized entity,    encrypted , i.e., the confidentiality is protected by encryption. Encrypted files are always authenticated, or     not-protected , i.e. SCONE reads and write  the files without any extra protection mechanisms. For example, you might use  not-protected  if your application already encrypts its files or if you need direct access to devices.    Marking all files individually as either  authenticated ,  encrypted , or  not-protected  would not be very practical. Hence, we support to partition the filesystem into  regions : regions do not overlap and each file belongs to exactly one region.  A region is defined by a path. For example, region  /  is the root region and you could, for example, specify that all files in region  /  must be authenticated. You can define a second region, for example,  region  /data/db  and that this region is encrypted.  Each file belongs to exactly one region: it belongs to the region that has the longest common path prefix with this file. For example, file  /etc/db.conf  would belong, in this case, to region  /  and file  /data/db/table.db  would belong to region  /data/db .  SCONE supports  ephemeral  regions: files are stored in main memory outside of the enclave. Since the main memory is not protected, we recommend that an ephemeral regions is either authenticated or encrypted.  When a program starts, all its ephemeral regions are empty. The only way to add files to an ephemeral region is by the application writing to this region.  All files in an ephemeral region are lost when the application exits.  All files that need to be persistent should be stored in a non-ephemeral region instead. We refer to this as a  kernel   region. For each region, you need to specify if the region is either  ephemeral  or  kernel .  Each region belongs to one of the following six classes:\n    { ephemeral  |  kernel } X { not-protected  |  authenticated  |  encrypted  }", 
            "title": "Concepts"
        }, 
        {
            "location": "/SCONE_Fileshield/#example", 
            "text": "Sometimes, we might only need to protect the files that are passed to a container via some volume. In this case, it would be sufficient that the volume is either authenticated or encrypted.  Let us demonstrate this via a simple example in which we pass an encrypted volume to a container. We create this encrypted volume in our local filesystem (in directory  volume ) and we will later mount this in the container as  /data . The original (non-encrypted) files are stored in directory  data-original .   mkdir -p volume  mkdir -p data-original  Let's write some files in the  data-original  directory:  cat   data-original/hello.txt   EOF  Hello World  EOF \ncat   data-original/world.py   EOF  f = open( /data/hello.txt ,  r )  print str(f.read())  EOF   Let's check that  volume  is empty and we print the hash values of the two files in  data-original :   ls volume  shasum data-original/*\n648a6a6ffffdaa0badb23b8baf90b6168dd16b3a  data-original/hello.txt\ndeda99d44e880ea8f2250f45c5c20c15d568d84c  data-original/world.py  Now, we start the SCONE crosscompiler in a container to create the encrypted volume:   docker run -it -v  $PWD /volume:/data  -v  $PWD /data-original:/data-original  sconecuratedimages/crosscompilers:scone", 
            "title": "Example"
        }, 
        {
            "location": "/SCONE_Fileshield/#file-system-protection-file", 
            "text": "All the metadata required for checking the consistency of the files is stored in a  file system protection file , or, short  fspf . SCONE supports multiple *fspf*s.   Let's start with a simple example with a single  fspf . The  fspf  file is created via command  scone fspf create  and let us name this file  fspf.pb . We execute the following commands\ninside the container (as indicated by the $ prompt):  $  cd  /data\n$ scone fspf create fspf.pb\nCreated empty file system protection file in fspf.pb. AES-GCM tag: 0e3da7ad62f5bc7c7bb08c67b16f2423  We can now split the file system in  regions , a region is a subtree. You can add regions to a  fspf  with the help of command  scone fspf addr .  Each region has exactly one of the following properties:   authenticated : the integrity of files is checked, i.e., any unauthorized modification of this file is detected and results in a reading error inside of the enclave. Specify command line option  --authenticated .  encrypted : the confidentiality and integrity of files is protected, i.e., encrypted always implies that the files are also authenticated. Specify command line option  --encrypted .  not-protected : files are neither authenticated nor encrypted. Specify command line option  --not-protected .   File system changes of containers are typically ephemeral in the sense that file updates are lost when a container terminates. When specifying option  --ephemeral , files in this region are not written to disk, the are written to an in memory file system instead.   Say for now, that by default we do not protect files and we want to read files and write  back  changed files to the file system. To do so, we define that the root tree is   --kernel  as well as   --not-protected :  $ scone fspf addr fspf.pb / --kernel / --not-protected\nAdded region / to file system protection file fspf.pb new AES-GCM tag: dd961af10b5aaa5cb1044c35a3f42c84  Let us add another region  /data  that should be encrypted and persisted. To encrypt the files,  we specify option  --encrypted . We specify option  --kernel  followed by a path (here, also  /data ) to request that files in this region are written to directory  /data .  $ scone fspf addr fspf.pb /data --encrypted --kernel /data\nAdded region /data to file system protection file fspf.pb new AES-GCM tag: 8481369d3ffdd9b6aeb30d044bf5c1c7  The encryption key for a file is chosen at random and stored in  fspf.pb . We use the Intel random number generator  RdRand  to generate the key. The default key length of a region is 32 bytes. Alternatives are key length of 16 and 24 bytes. These can be selected via option   --key-length 16  and  --key-length 24  when creating a region with command  scone fspf addr .  Now, that we defined the regions, i.e.,  /  and  /data , we can add files to region  /data . Let's just add all files in  /data-original  and encrypt these and write the encrypted files to  /data :  $ scone fspf addf fspf.pb /data /data-original /data\nAdded files to file system protection file fspf.pb new AES-GCM tag: 39a268166e628cf76e3fca80aa2d4f63  Note  that if we add files to an authenticated file region,  addf  will not copy or encrypt any data. It will\nonly add  the file names and the checksums (tags) of the files located in  /data-original  to  fspf.pb . Also, you should drop the directory to which encrypted files are written (i.e., in the above case  /data ) when adding files with  scone fspf addf . In this case, you would probably mount directory  /data-original  as  /data  in the container.  Coming back to the above example, we can now compare the hash values of the original files and the encrypted files:  $ shasum /data/*\nshasum /data/*\n87fd97468024e3d2864516ff5840e15d9615340d  /data/fspf.pb\n31732914910f4a08b9832c442074b0932915476c  /data/hello.txt\n8d07f3f576785c373a5e70e8dbcfa8ee06ca6d0c  /data/world.py  The fspf itself is not yet encrypted. We encrypt this file via command scone fspf encrypt fspf.pb  $ scone fspf encrypt fspf.pb   /data-original/keytag \nWe store the random encryption key as well as the tag of file  fspf.pb  in\nfile   /data-original/keytag .  We introduce a very simple program that reads the two files:  $ cat   example.c   EOF  #include  stdio.h  #include  stdlib.h  void printfile(const char* fn) {      FILE *fp = fopen(fn,  r );      char c;      while((c=fgetc(fp))!=EOF ){ \n        printf ( %c ,c ) ; \n     } \n    fclose ( fp ) ;  } \n\nint main ()   { \n    printfile ( /data/hello.txt ) ; \n    printfile ( /data/world.py ) ;  } \nEOF  Let's crosscompile this program:  scone gcc example.c -o example  Executing this program results in an output like this:  $./example\nR??C?\n    q?z??E?? | \u042e? } \u00fc ?o $? ?!rga??\u0387* ` ?????????Gw?  We need to activate the file system shield via environment variables by setting the location of the file system protection file (in  SCONE_FSPF ), the encryption key of the file (in  SCONE_FSPF_KEY ) and the tag of the fspf (in  SCONE_FSPF_TAG ). \nWe can extract the encryption key as well as the tag of  fspf.pb  from file  /data-original/keytag :  $  export   SCONE_FSPF_KEY = $( cat /data-original/keytag  |  awk  {print $11} ) \n$  export   SCONE_FSPF_TAG = $( cat /data-original/keytag  |  awk  {print $9} ) \n$  export   SCONE_FSPF = /data/fspf.pb  We can now execute this program again:  $ ./example\nHello World f   =  open ( /data/hello.txt ,  r ) \nprint str ( f.read ())   Variables  SCONE_FSPF_KEY ,  SCONE_FSPF_TAG  and  SCONE_FSPF  should only be set manually for debugging since they cannot securely be passed in this way to programs running inside enclaves. To securely pass environment variables, please read the section about  end-to-end encryption .", 
            "title": "File System Protection File"
        }, 
        {
            "location": "/SCONE_Fileshield/#python", 
            "text": "Let's try a similar approach for Python. \nIn the above example, we encrypted a Python program. Let's try to execute this encrypted program that accesses an encrypted file:  docker run -it -v  $PWD /volume:/data  sconecuratedimages/crosscompilers:python27 bash  The files  /data/world.py  and   /data/hello.txt  are encrypted:  $ cat /data/world.py\n? = ??J??0?6+?Q?nKd?*N,??.?G???????R?cO?t?y?? f?  Let's activate the file shield:  $  export   SCONE_FSPF_KEY = ... extract from data-original/keytag ...\n$  export   SCONE_FSPF_TAG = ... extract from data-original/keytag ...\n$  export   SCONE_FSPF = /data/fspf.pb  We can now run the encrypted  world.py  program with the  the Python interpreter:  SCONE_HEAP = 100000000   SCONE_ALPINE = 1   SCONE_VERSION = 1  /usr/local/bin/python /data/world.py export   SCONE_QUEUES = 1 \n...\nHello World", 
            "title": "Python"
        }, 
        {
            "location": "/SCONE_Fileshield/#protecting-the-root-region", 
            "text": "Note that in the above example, Python will not be permitted to load dynamic libraries  outside of the protected directory  /data : a dynamic library must reside in either an authenticated or an encrypted region.  To deal with this, we must define one or more authenticated or encrypted file regions that contain the dynamic libraries.    Let us show how to authenticate all files in region  /  $ scone fspf addr fspf.pb / --kernel / --authenticated  We need to add all files that our application might access. Often, these files in the root region might be defined in some container image. Let's see how we can add these files to our region  / .", 
            "title": "Protecting the Root Region"
        }, 
        {
            "location": "/SCONE_Fileshield/#adding-files-from-an-existing-container-image", 
            "text": "We show how to add a subset of the files of container image  sconecuratedimages/crosscompilers:python27  to our root region.\nTo do so, we ensure that we have the newest images:   docker pull sconecuratedimages/crosscompilers:python27  docker pull sconecuratedimages/crosscompilers:scone  How can we add all files in a container to the  fspf ? One way to do so requires to run Docker inside of a Docker container. To be able to do so, we need to permit our outermost docker container to have access to   /var/run/docker.sock :   docker run -it -v /var/run/docker.sock:/var/run/docker.sock -v  $PWD /volume:/data  -v  $PWD /data-original:/data-original  sconecuratedimages/crosscompilers:scone  Let us ensure that Docker is installed in this container:  apt-get update\napt-get install -y docker.io  Now, we want to add all files of some target container. In our example,\nthis is an instance of image  sconecuratedimages/crosscompilers:python27 .\nWe ensure that we pulled the latest image before we start the container:  CONTAINER_ID = ` docker run -d sconecuratedimages/crosscompilers:python27  printf  OK `    We can now copy all files from this container into a new directory  rootvol :  $  cd \n$ mkdir -p rootvol\n$ docker cp  $CONTAINER_ID :/ ./rootvol  Now that we have a copy of the files, we should not forget to garbage collect this container:  docker rm  $CONTAINER_ID   Let's remove some directories that we do not want our program to access, like for example,  /dev :  $ rm -rf rootvol/dev rootvol/proc rootvol/bin rootvol/media rootvol/mnt rootvol/usr/share/X11 rootvol/usr/share/terminfo rootvol/optrootvol/usr/include/c++/ rootvol/usr/lib/tcl8.6 rootvol/usr/lib/gcc rootvol/opt rootvol/sys rootvol/usr/include/c++  Now, we create a root  fspf :  $ scone fspf create fspf.pb\n$ scone fspf addr fspf.pb / --kernel / --authenticated\n$ scone fspf addf fspf.pb / ./rootvol /\n$ scone fspf encrypt fspf.pb   keytag  We can now create a new container image with this file system protection file\nusing this Dockerfile  $ cat   Dockerfile   EOF  FROM sconecuratedimages/crosscompilers:python27  COPY fspf.pb /  EOF \n$ docker build -t sconecuratedimages/crosscompilers:python27-authenticated .  We can run a container as follows: $ docker run -it sconecuratedimages/crosscompilers:python27-authenticated sh  Let us activate the file shield:  $  export   SCONE_FSPF_KEY = ... extract from data-original/keytag ...\n$  export   SCONE_FSPF_TAG = ... extract from data-original/keytag ...\n$  export   SCONE_FSPF = /fspf.pb  Let's run python with authenticated file system:  SCONE_HEAP = 1000000000   SCONE_ALLOW_DLOPEN = 2    SCONE_ALPINE = 1   SCONE_VERSION = 1  /usr/local/bin/python", 
            "title": "Adding files from an existing container image"
        }, 
        {
            "location": "/SCONE_Fileshield/#checking-the-file-system-shield", 
            "text": "Let's us check the file shield by creating a new python program ( helloworld-manual.py ) in\nside of a python container:   docker run -i sconecuratedimages/crosscompilers:python27-authenticated sh\n$ cat   helloworld-manual.py   EOF  print  Hello World  EOF   When we switch on the file shield, the execution of this program inside the enclave will fail: since this file was not part of the original file system, the file system shield will prevent accessing this file.  $  export   SCONE_FSPF_KEY = ... extract from data-original/keytag ...\n$  export   SCONE_FSPF_TAG = ... extract from data-original/keytag ...\n$  export   SCONE_FSPF = /fspf.pb\n$  SCONE_HEAP = 1000000000   SCONE_ALLOW_DLOPEN = 2    SCONE_ALPINE = 1   SCONE_VERSION = 1  /usr/local/bin/python helloworld-manual.py ( fails )   We can, however, add a new file via programs that have access to the key of the  fspf . We can, for example, write a python program to add a new python program to the file system.  By default, we disable that the root fspf is updated. We can enable updates by setting environment variable  SCONE_FSPF_MUTABLE=1 . We plan to permit updates of the root fspf by default in the near future (i.e., we will remove variable  SCONE_FSPF_MUTABLE=1 ).  $  SCONE_HEAP = 1000000000   SCONE_FSPF_MUTABLE = 1   SCONE_ALLOW_DLOPEN = 2    SCONE_ALPINE = 1   SCONE_VERSION = 1  /usr/local/bin/python    PYTHON  f = open( helloworld.py ,  w )  f.write( print  Hello World \\n )  f.close()  PYTHON ```   The tag of the file system protection file is now changed. We can determine the new TAG with the help of command  scone fspf show :  $  export   SCONE_FSPF_TAG = $( scone fspf show --tag /fspf.pb )   Now, we can run the new  helloworld.py :  $  SCONE_HEAP = 1000000000   SCONE_ALLOW_DLOPEN = 2    SCONE_ALPINE = 1   SCONE_VERSION = 1  /usr/local/bin/python helloworld.py\n...\nHello World", 
            "title": "Checking the File System Shield"
        }, 
        {
            "location": "/SCONE_Fileshield/#extended-example", 
            "text": "To learn how to use multiple file system protection files, \nplease have a look at the following screencast.   Below is the script that is executed in the screencast:   docker run -it -v  $PWD :/mnt sconecuratedimages/crosscompilers:scone\n\n$ mkdir -p /example\n$ mkdir -p /mnt/authenticated/\n$ mkdir -p /mnt/encrypted/\n$  cd  /example\n$ mkdir -p .original\n\n$ scone fspf create fspf.pb\n$ scone fspf create authenticated.pb\n$ scone fspf create encrypted.pb # add protection regions \n$ scone fspf addr fspf.pb / -e --ephemeral\n$ scone fspf addr authenticated.pb /mnt/authenticated -a --kernel /mnt/authenticated\n$ scone fspf addr encrypted.pb /mnt/encrypted -e --kernel /mnt/encrypted # add files  # enclave program should expect the files (directories) found by the client in ./original in /mnt/authenticated \n$ scone fspf addf authenticated.pb /mnt/authenticated ./original # enclave program should expect the files (directories) found by the client in ./original in encrypted form in /mnt/encrypted  # the client will write the encrypted files to ./mnt/encrypted \n$ scone fspf addf encrypted.pb /mnt/encrypted ./original ./mnt/encrypted encrypted_key = ` scone fspf encrypt encrypted.pb  |  awk  {print $11} ` \n\n$  echo   encrypted.pb key:  ${ encrypted_key } \n$ scone fspf addfspf fspf.pb authenticated.pb\n$ scone fspf addfspf fspf.pb encrypted.pb  ${ encrypted_key } \n$ cat   example.c   EOF  #include  stdio.h  int main() {      FILE *fp = fopen( /mnt/authenticated/hello ,  w );      fprintf(fp,  hello world\\n );      fclose(fp);      fp = fopen( /mnt/encrypted/hello ,  w );      fprintf(fp,  hello world\\n );      fclose(fp);  }  EOF \n\n$ scone gcc example.c -o sgxex\n$ cat   /etc/sgx-musl.conf   EOF  Q 4  e -1 0 0  s -1 0 0  e -1 1 0  s -1 1 0  e -1 2 0  s -1 2 0  e -1 3 0  s -1 3 0  EOF \n$  SCONE_FSPF = fspf.pb ./sgxex\n$ cat /mnt/authenticated/hello\n$ cat /mnt/encrypted/hello \n$ cat   cat.c   EOF  #include  stdio.h  int main() {      char buf[80];      FILE *fp = fopen( /mnt/authenticated/hello ,  r );      fgets(buf, sizeof(buf), fp);      fclose(fp);      printf( read:  %s \\n , buf);      fp = fopen( /mnt/encrypted/hello ,  r );      fgets(buf, sizeof(buf), fp);      fclose(fp);      printf( read:  %s \\n , buf);  }  EOF \n\n$ scone gcc cat.c -o native_cat\n$ ./native_cat\n$ scone gcc cat.c -o sgxcat\n$  SCONE_FSPF = fspf.pb ./sgxcat", 
            "title": "Extended Example"
        }, 
        {
            "location": "/SCONE_Fileshield/#notes", 
            "text": "The SCONE File Protection documentation is not yet completed and more information will be provided soon.    scontain.com , January 2018.  Questions or Suggestions?", 
            "title": "Notes"
        }, 
        {
            "location": "/ssh/", 
            "text": "ssh Setup\n\n\nssh is standard way to log securely into remote hosts. The SCONE CLI requires that you can log into all hosts of your docker swarm(s) without the need for providing a password. We describe in this section how you could set this up.\n\n\nscone\n\n\nThe \nscone\n utility executes commands via \nssh\n on the SGX-capable machine to install software as well as to deploy and monitor containers.\n\n\n\n\nSince we potentially execute many \nssh\n commands, you need to configure \nssh\n such that\n\n\n\n\n\n\nyou can log into the SGX machines \nwithout\n  having to type a password,\n\n\n\n\n\n\nyou can use the basename of your SGX machines to login, and\n\n\n\n\n\n\nssh is permitted to reuse connections to reduce the execution time of the \nscone\n commands.\n\n\n\n\n\n\nTo do so, you need to configure \nssh\n on your development machine and/or your container in which you run \nscone\n as well as  on the sgx hosts that you are using.\n\n\nDevelopment machine\n\n\nFor each SGX host inside of your swarm, you should add a host alias to your \nssh\n configuration.\n\n\nHost alias\n\n\nTo reduce your typing overhead, \nscone\n assumes that each host has a unique basename and that you configured \nssh\n such that you can log into the host via this basename. For example, instead of typing \nnode2.my.very.long.domain.com\n, you must configure  \nssh\n such that \nssh node2\n is a shortcut  for \nssh node2.my.very.long.domain.com\n.\n\n\nAs a caveat, this basename must be sufficient for other hosts in the \nsame swarm\n to reach \nnode2\n. In the above figure, \nmanager\n must be able to resolve \nnode2\n to the IP address of \nnode2.my.very.long.domain.com\n. By default most swarms are setup this way.\n\n\nOn your development machine (or, more precisely in your development container), \nyou need to add an alias \nnode2\n for \nnode2.my.very.long.domain.com\n, you could add the following lines to your \nssh config\n (stored in \n$HOME/.ssh/config\n):\n\n\nHost node2\n     HostName node2.my.very.long.domain.com\n     Port \n22\n\n     User scone\n     IdentityFile ~/.ssh/id_rsa\n\n\n\n\nssh connection reuse\n\n\nTo be able to reuse a ssh connection, you must configure ssh appropriately. You should set \nControlMaster\n to \nauto\n and you have to specify a control path via option \nControlPath\n. You can define a generic path like \n~/.ssh/ssh_mux_%h_%p_%r\n - this can be the same for all hosts. For example, for some host \nalice\n you might add the following lines to \n$HOME/.ssh/config\n:\n\n\nHost alice\n        ControlMaster auto\n        ControlPath ~/.ssh/ssh_mux_%h_%p_%r\n        user ubuntu\n        port \n10101\n\n        hostname sshproxy.cloudprovider.com\n\n\n\n\nContainer Configuration\n\n\nTo simplify the \nssh\n setup inside of containers, you might want to map your \nssh\n configuration residing in your home directory into the containers in which you run the \nscone\n CLI. Since you probably have a different user ID inside and outside the container, you might want to copy the original \nssh\n configuration:\n\n\n docker run -it -v \n$HOME\n/.ssh:/root/.xssh  sconecuratedimages/sconecli\n\n\n\n\nInside the container, copy the external \nssh\n configuration:\n\n\n$ cp -rf \n$HOME\n/.xssh/* \n$HOME\n/.ssh\n\n\n\n\nSSH Agent\n\n\nIn the container running the \nscone\n CLI, you need to start a \nssh-agent\n in the container in which you run the \nscone\n CLI:\n\n\n$ \nSA\n=\n$(\nssh-agent\n)\n\n$ \neval\n \n$SA\n\n\n\n\n\nand add your public key by executing:\n\n\n$ ssh-add\n\n\n\n\nEnsure that you are now able to log into all hosts of your swarm.\n\n\nSGX Host Setup\n\n\nscone\n expects to have password-less access to all SGX hosts that you want to use for Scone. To do so, you need to add one of your public ssh keys to \n~/.ssh/authorized_keys\n on all SGX host. For example, you could add your public key \n~/.ssh/id_rsa.pub\n to file  \n~/.ssh/authorized_keys\n on each of these hosts.\n\n\nSome commands are required to be executed with \nsudo\n.  We assume that the user has the right to perform a password-less \nsudo\n on the SGX hosts. In case the user does not yet have this right, the user should be added to \n/etc/sudoers\n. Typically, one would add the user with the help of command \nvisudo\n. The entry for user \nalice\n might look like this:\n\n\nalice \nALL\n=(\nALL\n)\n NOPASSWD: ALL\n\n\n\n\nSSH Credentials\n\n\nIn case you do not want to use your standard credentials inside of a container, you need to ensure that you have a pair of authentication keys inside the container. If there exists no public key \n$HOME/.ssh/id_rsa.pub\n (often the case if you use a container), you can generate a new pair by executing:\n\n\n$ ssh-keygen -b \n4096\n -t rsa\n\n\n\n\nAppend the generated public key \nHOME/.ssh/id_rsa.pub* to file *\nHOME/.ssh/id_rsa.pub* to file *\nHOME/.ssh/authorized_keys\n on the SGX hosts for which you to be able to log in without a password.\n\n\nAlso, ensure that your \n$HOME/.ssh/config\n contains an entry for each host of your swarm (see above).\n\n\n \nscontain.com\n, March 2018. \nQuestions or Suggestions?", 
            "title": "ssh setup"
        }, 
        {
            "location": "/ssh/#ssh-setup", 
            "text": "ssh is standard way to log securely into remote hosts. The SCONE CLI requires that you can log into all hosts of your docker swarm(s) without the need for providing a password. We describe in this section how you could set this up.", 
            "title": "ssh Setup"
        }, 
        {
            "location": "/ssh/#scone", 
            "text": "The  scone  utility executes commands via  ssh  on the SGX-capable machine to install software as well as to deploy and monitor containers.   Since we potentially execute many  ssh  commands, you need to configure  ssh  such that    you can log into the SGX machines  without   having to type a password,    you can use the basename of your SGX machines to login, and    ssh is permitted to reuse connections to reduce the execution time of the  scone  commands.    To do so, you need to configure  ssh  on your development machine and/or your container in which you run  scone  as well as  on the sgx hosts that you are using.", 
            "title": "scone"
        }, 
        {
            "location": "/ssh/#development-machine", 
            "text": "For each SGX host inside of your swarm, you should add a host alias to your  ssh  configuration.", 
            "title": "Development machine"
        }, 
        {
            "location": "/ssh/#host-alias", 
            "text": "To reduce your typing overhead,  scone  assumes that each host has a unique basename and that you configured  ssh  such that you can log into the host via this basename. For example, instead of typing  node2.my.very.long.domain.com , you must configure   ssh  such that  ssh node2  is a shortcut  for  ssh node2.my.very.long.domain.com .  As a caveat, this basename must be sufficient for other hosts in the  same swarm  to reach  node2 . In the above figure,  manager  must be able to resolve  node2  to the IP address of  node2.my.very.long.domain.com . By default most swarms are setup this way.  On your development machine (or, more precisely in your development container), \nyou need to add an alias  node2  for  node2.my.very.long.domain.com , you could add the following lines to your  ssh config  (stored in  $HOME/.ssh/config ):  Host node2\n     HostName node2.my.very.long.domain.com\n     Port  22 \n     User scone\n     IdentityFile ~/.ssh/id_rsa", 
            "title": "Host alias"
        }, 
        {
            "location": "/ssh/#ssh-connection-reuse", 
            "text": "To be able to reuse a ssh connection, you must configure ssh appropriately. You should set  ControlMaster  to  auto  and you have to specify a control path via option  ControlPath . You can define a generic path like  ~/.ssh/ssh_mux_%h_%p_%r  - this can be the same for all hosts. For example, for some host  alice  you might add the following lines to  $HOME/.ssh/config :  Host alice\n        ControlMaster auto\n        ControlPath ~/.ssh/ssh_mux_%h_%p_%r\n        user ubuntu\n        port  10101 \n        hostname sshproxy.cloudprovider.com", 
            "title": "ssh connection reuse"
        }, 
        {
            "location": "/ssh/#container-configuration", 
            "text": "To simplify the  ssh  setup inside of containers, you might want to map your  ssh  configuration residing in your home directory into the containers in which you run the  scone  CLI. Since you probably have a different user ID inside and outside the container, you might want to copy the original  ssh  configuration:   docker run -it -v  $HOME /.ssh:/root/.xssh  sconecuratedimages/sconecli  Inside the container, copy the external  ssh  configuration:  $ cp -rf  $HOME /.xssh/*  $HOME /.ssh", 
            "title": "Container Configuration"
        }, 
        {
            "location": "/ssh/#ssh-agent", 
            "text": "In the container running the  scone  CLI, you need to start a  ssh-agent  in the container in which you run the  scone  CLI:  $  SA = $( ssh-agent ) \n$  eval   $SA   and add your public key by executing:  $ ssh-add  Ensure that you are now able to log into all hosts of your swarm.", 
            "title": "SSH Agent"
        }, 
        {
            "location": "/ssh/#sgx-host-setup", 
            "text": "scone  expects to have password-less access to all SGX hosts that you want to use for Scone. To do so, you need to add one of your public ssh keys to  ~/.ssh/authorized_keys  on all SGX host. For example, you could add your public key  ~/.ssh/id_rsa.pub  to file   ~/.ssh/authorized_keys  on each of these hosts.  Some commands are required to be executed with  sudo .  We assume that the user has the right to perform a password-less  sudo  on the SGX hosts. In case the user does not yet have this right, the user should be added to  /etc/sudoers . Typically, one would add the user with the help of command  visudo . The entry for user  alice  might look like this:  alice  ALL =( ALL )  NOPASSWD: ALL", 
            "title": "SGX Host Setup"
        }, 
        {
            "location": "/ssh/#ssh-credentials", 
            "text": "In case you do not want to use your standard credentials inside of a container, you need to ensure that you have a pair of authentication keys inside the container. If there exists no public key  $HOME/.ssh/id_rsa.pub  (often the case if you use a container), you can generate a new pair by executing:  $ ssh-keygen -b  4096  -t rsa  Append the generated public key  HOME/.ssh/id_rsa.pub* to file * HOME/.ssh/id_rsa.pub* to file * HOME/.ssh/authorized_keys  on the SGX hosts for which you to be able to log in without a password.  Also, ensure that your  $HOME/.ssh/config  contains an entry for each host of your swarm (see above).    scontain.com , March 2018.  Questions or Suggestions?", 
            "title": "SSH Credentials"
        }, 
        {
            "location": "/SCONE_ENV/", 
            "text": "SCONE Environment Variables\n\n\nTo simplify development and debugging, SCONE supports a range of environment variables to control its behavior. These environment variables are mainly used for development and debugging. In operational settings, the configuration would be provided in a secure fashion via the SCONE configuration and attestation service.\n\n\nAlso, the performance of SCONE-based applications can be tuned by selecting the appropriate configuration for an application. We have tool support to automatically tune these parameters.\n\n\nSCONE Configuration File\n\n\nThe location of the SCONE configuration file can be controlled with an environment variable:\n\n\n\n\nSCONE_CONFIG\n: if defined, this determines the path of SCONE configuration file. If this environment variable is not defined or the file cannot be opened,  the default configuration file located in \n/etc/sgx-musl.conf\n is read instead. If the default configuration file cannot be read either, the program exits with an error message.\n\n\n\n\nChanging the location of the configuration file is, for example, useful in the context of testing or when you run your application outside of a container when you want to run different applications with different configurations inside of enclaves.\n\n\nThe configuration file can define most of the  behaviors that one can define via environment variables.\nHowever, the **SCONE_* ** environment variables have higher priority than the settings from the config file.\n\n\nFormat of SCONE Configuration File\n\n\nThe format for the configuration file: on each line, there is a\nstatement beginning with a single-character command code, and up to\nthree numbers. The possible commands currently are:\n\n\n\n\n\n\n\n\nCommand\n\n\nOption(s)\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nQ\n\n\nn\n\n\ndefines the number of queues used to execute system calls\n\n\n\n\n\n\n\n\nn [number of queues]\n\n\nsets the number of syscall-return queue pairs to allocate. This is equivalent to setting the \nSCONE_QUEUES\n environment variable\n\n\n\n\n\n\nH\n\n\ns\n\n\ndefines the heap size in bytes\n\n\n\n\n\n\n\n\ns [heap size in bytes\n]\n\n\nsets the size of heap, equivalent to setting \nSCONE_HEAP\n environment variable\n\n\n\n\n\n\nP\n\n\nN\n\n\ndetermines the backoff behavior of the queues\n\n\n\n\n\n\n\n\nN [spin number]\n\n\nequivalent to setting \nSCONE_SSPINS\n environment variable\n\n\n\n\n\n\nL\n\n\nS\n\n\ndetermines the backoff behavior of the queues\n\n\n\n\n\n\n\n\nS [sleep time]\n\n\nequivalent to setting \nSCONE_SSLEEP\n environment variable;\n\n\n\n\n\n\ns\n\n\nC Q R\n\n\nsthread\n serving system calls outside of an enclave\n\n\n\n\n\n\n\n\nC [core-no]\n\n\nif non-negative number: pin this sthread to core \nC\n; if a negative number, do not pin this thread\n\n\n\n\n\n\n\n\nQ [queue-no]\n\n\nin \n[0..n]\n ;  this sthreads serves this queue\n\n\n\n\n\n\n\n\nR [realtime]\n\n\nalways set this to 0\n\n\n\n\n\n\ne\n\n\nC Q R\n\n\nethread\n running inside of enclave and executes application threads (which we call lthreads)\n\n\n\n\n\n\n\n\nC [core-no]\n\n\nnon-negative number: pin to this core; negative number: no pinning ot this thread\n\n\n\n\n\n\n\n\nQ [queue-no]\n\n\nin \n[0..n]\n: this sthreads serves this queue\n\n\n\n\n\n\n\n\nR [realtime]\n\n\nalways set this to 0\n\n\n\n\n\n\n\n\nThe number of \nsthreads\n is automatically increased if more \nsthreads\n are needed to process system calls. An \nsthread\n will block if it does not have any work left to do. Hence, we recommend to start exactly one \nsthread\n per \nethread\n.\n\n\nethreads\n will leave the enclave it there is no more work for them to do. Hence, it makes sense to start one ehthread per core. In some situations, it might even make sense to start one \nethread\n per hyper-thread.\n\n\nUnless you want to limit the the number of CPU resources an enclave should use,  the following is a good generic configuration file:\n\n\n$ sudo tee  /etc/sgx-musl.conf \n EOF\n\n\nQ 4\n\n\ne -1 0 0\n\n\ns -1 0 0\n\n\ne -1 1 0\n\n\ns -1 1 0\n\n\ne -1 2 0\n\n\ns -1 2 0\n\n\ne -1 3 0\n\n\ns -1 3 0\n\n\nEOF\n\n\n\n\n\nRun Mode\n\n\n\n\nSCONE_MODE\n: defines if application should run inside of an enclave or outside in simulation mode.\n\n\n\n\n\n\n\n\n\n\nValue\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nSCONE_MODE=HW\n\n\nrun program inside of enclave. If program fails to create an enclave, it will fail.\n\n\n\n\n\n\nSCONE_MODE=SIM\n\n\nrun program outside of enclave. All SCONE related software runs but just outside of the enclave. This is not 100% compatible with hardware mode since, for example, some instructions are permitted in native mode that are not permitted in hardware mode.\n\n\n\n\n\n\nSCONE_MODE=AUTO\n\n\nrun program inside of enclave if SGX is available. Otherwise, run in simulation mode. AUTO is the default mode\n\n\n\n\n\n\n\n\n\n\nSCONE_HEAP\n: size of the heap allocated for the program during the startup of the enclave.\n\n\n\n\n\n\n\n\n\n\nValue\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nSCONE_HEAP=s\n\n\nthe requested heap size in bytes\n\n\n\n\n\n\nSCONE_HEAP=sM\n\n\nthe requested heap size in MB\n\n\n\n\n\n\nSCONE_HEAP=sG\n\n\nthe requested heap size in GB\n\n\n\n\n\n\n\n\nDebug\n\n\n\n\nSCONE_VERSION\n if defined, SCONE will print the values of some of the SCONE environment variables during startup.\n\n\n\n\nExample output:\n\n\nexport\n \nSCONE_QUEUES\n=\n4\n\n\nexport\n \nSCONE_SLOTS\n=\n256\n\n\nexport\n \nSCONE_SIGPIPE\n=\n0\n\n\nexport\n \nSCONE_MMAP32BIT\n=\n0\n\n\nexport\n \nSCONE_SSPINS\n=\n100\n\n\nexport\n \nSCONE_SSLEEP\n=\n4000\n\n\nexport\n \nSCONE_KERNEL\n=\n0\n\n\nexport\n \nSCONE_HEAP\n=\n1073741824\n\n\nexport\n \nSCONE_CONFIG\n=\n/etc/sgx-musl.conf\n\nexport\n \nSCONE_MODE\n=\nhw\n\nexport\n \nSCONE_SGXBOUNDS\n=\nno\n\nexport\n \nSCONE_ALLOW_DLOPEN\n=\nno\nRevision: 9b355b99170ad434010353bb9f4dca24e532b1b7\nBranch: master\nConfigure options: --enable-file-prot --enable-shared --enable-debug --prefix\n=\n/scone/src/built/cross-compiler/x86_64-linux-musl\n\n\n\n\nDynamic library support:\n\n\n\n\n\n\nSCONE_ALLOW_DLOPEN=\"1\"\n: if defined, permit to load shared libraries after startup. These libraries must be authenticated or encrypted. Note that all libraries that are loaded during startup are measured and contribute to the hash of the enclave, i.e., they are part of \nMRENCLAVE\n. The libraries loaded during startup could reside in a file region that is not protect or that is authenticated. These libraries must not be in an encrypted region since the encryption keys are not yet known during startup.\n\n\n\n\n\n\nSCONE_ALLOW_DLOPEN=\"2\"\n: must be used \nfor debugging only\n: this  value enables loading of dynamic libraries after startup that are neither authenticated nor encrypted. For example, Python programs might dynamically load modules after startup. Our approach to enforce the integrity of these dynamic libraries with the help of a file protection shield, i.e., you should eiter set \nSCONE_ALLOW_DLOPEN=\"1\"\n or you should not define \nSCONE_ALLOW_DLOPEN\n.\n\n\n\n\n\n\nPerformance tuning variables:\n\n\n\n\n\n\nSCONE_QUEUES\n: number of systemcall queues to be used.\n\n\n\n\n\n\nSCONE_SLOTS\n: systemcall queue length: must be larger than the maximum number of \nlthreads\n.\n\n\n\n\n\n\nSCONE_SIGPIPE\n: if set to \"1\", \nSIGPIPE\n signals are delivered to the application\n\n\n\n\n\n\nSCONE_SSPINS=N\n: In case an Ethread does not have any lthread to execute, an Ethread first pauses for up to N times to wait for more work to show up. After that, it sleeps for up to N times. Each time increasing the sleep time. \n\n\n\n\n\n\nSCONE_SSLEEP\n: determines how fast we increase the backoff.\n\n\n\n\n\n\nSafety\n\n\n\n\nSCONE_SGXBOUNDS\n: must be defined to enable bounds checking. Furthermore, you need to compile your program with our SGX boundschecker.\n\n\n\n\nDynamic link loader\n\n\nThe dynamic link loader is part of image \nsconecuratedimages/crosscompilers:runtime\n (\nsee tutorial\n).\n\n\n\n\n\n\nLD_LIBRARY_PATH\n: you can control from where the dynamic link loader looks for shared libraries.\n\n\n\n\n\n\nLD_PRELOAD\n: you can instruct the dynamic link loader to load libraries before loading the libraries specified by the program itself.\n\n\n\n\n\n\nSCONE_ALPINE=1\n: run dynamically-linked program inside of an enclave.\n\n\n\n\n\n\n \nscontain.com\n, November 2017. \nQuestions or Suggestions?", 
            "title": "environment variables"
        }, 
        {
            "location": "/SCONE_ENV/#scone-environment-variables", 
            "text": "To simplify development and debugging, SCONE supports a range of environment variables to control its behavior. These environment variables are mainly used for development and debugging. In operational settings, the configuration would be provided in a secure fashion via the SCONE configuration and attestation service.  Also, the performance of SCONE-based applications can be tuned by selecting the appropriate configuration for an application. We have tool support to automatically tune these parameters.", 
            "title": "SCONE Environment Variables"
        }, 
        {
            "location": "/SCONE_ENV/#scone-configuration-file", 
            "text": "The location of the SCONE configuration file can be controlled with an environment variable:   SCONE_CONFIG : if defined, this determines the path of SCONE configuration file. If this environment variable is not defined or the file cannot be opened,  the default configuration file located in  /etc/sgx-musl.conf  is read instead. If the default configuration file cannot be read either, the program exits with an error message.   Changing the location of the configuration file is, for example, useful in the context of testing or when you run your application outside of a container when you want to run different applications with different configurations inside of enclaves.  The configuration file can define most of the  behaviors that one can define via environment variables.\nHowever, the **SCONE_* ** environment variables have higher priority than the settings from the config file.", 
            "title": "SCONE Configuration File"
        }, 
        {
            "location": "/SCONE_ENV/#format-of-scone-configuration-file", 
            "text": "The format for the configuration file: on each line, there is a\nstatement beginning with a single-character command code, and up to\nthree numbers. The possible commands currently are:     Command  Option(s)  Description      Q  n  defines the number of queues used to execute system calls     n [number of queues]  sets the number of syscall-return queue pairs to allocate. This is equivalent to setting the  SCONE_QUEUES  environment variable    H  s  defines the heap size in bytes     s [heap size in bytes ]  sets the size of heap, equivalent to setting  SCONE_HEAP  environment variable    P  N  determines the backoff behavior of the queues     N [spin number]  equivalent to setting  SCONE_SSPINS  environment variable    L  S  determines the backoff behavior of the queues     S [sleep time]  equivalent to setting  SCONE_SSLEEP  environment variable;    s  C Q R  sthread  serving system calls outside of an enclave     C [core-no]  if non-negative number: pin this sthread to core  C ; if a negative number, do not pin this thread     Q [queue-no]  in  [0..n]  ;  this sthreads serves this queue     R [realtime]  always set this to 0    e  C Q R  ethread  running inside of enclave and executes application threads (which we call lthreads)     C [core-no]  non-negative number: pin to this core; negative number: no pinning ot this thread     Q [queue-no]  in  [0..n] : this sthreads serves this queue     R [realtime]  always set this to 0     The number of  sthreads  is automatically increased if more  sthreads  are needed to process system calls. An  sthread  will block if it does not have any work left to do. Hence, we recommend to start exactly one  sthread  per  ethread .  ethreads  will leave the enclave it there is no more work for them to do. Hence, it makes sense to start one ehthread per core. In some situations, it might even make sense to start one  ethread  per hyper-thread.  Unless you want to limit the the number of CPU resources an enclave should use,  the following is a good generic configuration file:  $ sudo tee  /etc/sgx-musl.conf   EOF  Q 4  e -1 0 0  s -1 0 0  e -1 1 0  s -1 1 0  e -1 2 0  s -1 2 0  e -1 3 0  s -1 3 0  EOF", 
            "title": "Format of SCONE Configuration File"
        }, 
        {
            "location": "/SCONE_ENV/#run-mode", 
            "text": "SCONE_MODE : defines if application should run inside of an enclave or outside in simulation mode.      Value  Description      SCONE_MODE=HW  run program inside of enclave. If program fails to create an enclave, it will fail.    SCONE_MODE=SIM  run program outside of enclave. All SCONE related software runs but just outside of the enclave. This is not 100% compatible with hardware mode since, for example, some instructions are permitted in native mode that are not permitted in hardware mode.    SCONE_MODE=AUTO  run program inside of enclave if SGX is available. Otherwise, run in simulation mode. AUTO is the default mode      SCONE_HEAP : size of the heap allocated for the program during the startup of the enclave.      Value  Description      SCONE_HEAP=s  the requested heap size in bytes    SCONE_HEAP=sM  the requested heap size in MB    SCONE_HEAP=sG  the requested heap size in GB", 
            "title": "Run Mode"
        }, 
        {
            "location": "/SCONE_ENV/#debug", 
            "text": "SCONE_VERSION  if defined, SCONE will print the values of some of the SCONE environment variables during startup.   Example output:  export   SCONE_QUEUES = 4  export   SCONE_SLOTS = 256  export   SCONE_SIGPIPE = 0  export   SCONE_MMAP32BIT = 0  export   SCONE_SSPINS = 100  export   SCONE_SSLEEP = 4000  export   SCONE_KERNEL = 0  export   SCONE_HEAP = 1073741824  export   SCONE_CONFIG = /etc/sgx-musl.conf export   SCONE_MODE = hw export   SCONE_SGXBOUNDS = no export   SCONE_ALLOW_DLOPEN = no\nRevision: 9b355b99170ad434010353bb9f4dca24e532b1b7\nBranch: master\nConfigure options: --enable-file-prot --enable-shared --enable-debug --prefix = /scone/src/built/cross-compiler/x86_64-linux-musl", 
            "title": "Debug"
        }, 
        {
            "location": "/SCONE_ENV/#dynamic-library-support", 
            "text": "SCONE_ALLOW_DLOPEN=\"1\" : if defined, permit to load shared libraries after startup. These libraries must be authenticated or encrypted. Note that all libraries that are loaded during startup are measured and contribute to the hash of the enclave, i.e., they are part of  MRENCLAVE . The libraries loaded during startup could reside in a file region that is not protect or that is authenticated. These libraries must not be in an encrypted region since the encryption keys are not yet known during startup.    SCONE_ALLOW_DLOPEN=\"2\" : must be used  for debugging only : this  value enables loading of dynamic libraries after startup that are neither authenticated nor encrypted. For example, Python programs might dynamically load modules after startup. Our approach to enforce the integrity of these dynamic libraries with the help of a file protection shield, i.e., you should eiter set  SCONE_ALLOW_DLOPEN=\"1\"  or you should not define  SCONE_ALLOW_DLOPEN .", 
            "title": "Dynamic library support:"
        }, 
        {
            "location": "/SCONE_ENV/#performance-tuning-variables", 
            "text": "SCONE_QUEUES : number of systemcall queues to be used.    SCONE_SLOTS : systemcall queue length: must be larger than the maximum number of  lthreads .    SCONE_SIGPIPE : if set to \"1\",  SIGPIPE  signals are delivered to the application    SCONE_SSPINS=N : In case an Ethread does not have any lthread to execute, an Ethread first pauses for up to N times to wait for more work to show up. After that, it sleeps for up to N times. Each time increasing the sleep time.     SCONE_SSLEEP : determines how fast we increase the backoff.", 
            "title": "Performance tuning variables:"
        }, 
        {
            "location": "/SCONE_ENV/#safety", 
            "text": "SCONE_SGXBOUNDS : must be defined to enable bounds checking. Furthermore, you need to compile your program with our SGX boundschecker.", 
            "title": "Safety"
        }, 
        {
            "location": "/SCONE_ENV/#dynamic-link-loader", 
            "text": "The dynamic link loader is part of image  sconecuratedimages/crosscompilers:runtime  ( see tutorial ).    LD_LIBRARY_PATH : you can control from where the dynamic link loader looks for shared libraries.    LD_PRELOAD : you can instruct the dynamic link loader to load libraries before loading the libraries specified by the program itself.    SCONE_ALPINE=1 : run dynamically-linked program inside of an enclave.      scontain.com , November 2017.  Questions or Suggestions?", 
            "title": "Dynamic link loader"
        }, 
        {
            "location": "/MrEnclave/", 
            "text": "Determining MrEnclave\n\n\nAn enclave is identified by a hash value which is called  \nMrEnclave\n. This has is determined by content of the pages of an enclave and the access rights. In particular, the means that some of the \nSCONE environment\n variables like \nSCONE_HEAP\n and \nSCONE_ALLOW_DLOPEN\n  will affect \nMrEnclave\n.\n\n\nTo determine \nMrEnclave\n, we provide a simple way to determine \nMrEnclave\n on the developer site via environment variable  \nSCONE_HASH=1\n.\n\n\nExample: MrEnclave of Python\n\n\nLet us determine \nMrEnclave\n of our python interpreter. We start the container\nand then set environment variable \nSCONE_HASH=1\n to ask SCONE to print \nMrEnclave\n and then terminate and \nSCONE_ALPINE=1\n to ensure that the application is indeed started with SCONE. \n\n\n\n\nNote\n\n\nWhen setting \nSCONE_HASH=1\n the program is not executed - only MrEnclave is printed on stdout.**\n\n\n\n\n docker run -it  sconecuratedimages/crosscompilers:python27 sh\n$ \nSCONE_HASH\n=\n1\n \nSCONE_ALPINE\n=\n1\n /usr/local/bin/python\n5430b3c0ab0e8a24ea4481e6022704cdbbcff68f6457eb0cdeaecfd734fec541\n\n\n\n\nNow, let us change the heap size via environment variable \nSCONE_HEAP\n\nby asking for a \n2GB\n heap:\n\n\n$ \nSCONE_HEAP\n=\n2G \nSCONE_HASH\n=\n1\n \nSCONE_ALPINE\n=\n1\n /usr/local/bin/python\naa25d6e1863819fca72f4f3315131ba4a438d1e1643c030190ca665215912465\n\n\n\n\nBy default, SCONE does not permit to load dynamic libraries after startup. By setting \nSCONE_ALLOW_DLOPEN=1\n, we permit to load dynamic libraries during runtime. This changes \nMrEnclave\n:\n\n\n$ \nSCONE_ALLOW_DLOPEN\n=\n1\n \nSCONE_HEAP\n=\n2G \nSCONE_HASH\n=\n1\n \nSCONE_ALPINE\n=\n1\n /usr/local/bin/python\n9c56db536e046a5fb84a5c482ce86e6592071dff75dc0e3eb27d701cf2c40508\n\n\n\n\nUsing debug output\n\n\nAs an alternative to \nSCONE_HASH=1\n is to print \nMrEnclave\n via debug messages by setting \nSCONE_VERSION=1\n:\n\n\n$ \nSCONE_ALLOW_DLOPEN\n=\n1\n \nSCONE_HEAP\n=\n2G \nSCONE_VERSION\n=\n1\n \nSCONE_ALPINE\n=\n1\n /usr/local/bin/python\n\nexport\n \nSCONE_QUEUES\n=\n4\n\n\nexport\n \nSCONE_SLOTS\n=\n256\n\n\nexport\n \nSCONE_SIGPIPE\n=\n0\n\n\nexport\n \nSCONE_MMAP32BIT\n=\n0\n\n\nexport\n \nSCONE_SSPINS\n=\n100\n\n\nexport\n \nSCONE_SSLEEP\n=\n4000\n\n\nexport\n \nSCONE_KERNEL\n=\n0\n\n\nexport\n \nSCONE_HEAP\n=\n2147483648\n\n\nexport\n \nSCONE_STACK\n=\n81920\n\n\nexport\n \nSCONE_CONFIG\n=\n/etc/sgx-musl.conf\n\nexport\n \nSCONE_MODE\n=\nsim\n\nexport\n \nSCONE_SGXBOUNDS\n=\nno\n\nexport\n \nSCONE_VARYS\n=\nno\n\nexport\n \nSCONE_ALLOW_DLOPEN\n=\nyes \n(\nprotected\n)\n\n\nexport\n \nSCONE_MPROTECT\n=\nno\nRevision: b6a40e091e2adb253f019401723d2a734e887a74 \n(\nFri Jan \n26\n \n07\n:44:44 \n2018\n +0100\n)\n\nBranch: master \n(\ndirty\n)\n\nConfigure options: --enable-shared --enable-debug --prefix\n=\n/scone/src/built/cross-compiler/x86_64-linux-musl\n\nEnclave hash: 9c56db536e046a5fb84a5c482ce86e6592071dff75dc0e3eb27d701cf2c40508\nPython \n2\n.7.14 \n(\ndefault, Jan \n10\n \n2018\n, \n05\n:35:30\n)\n \n\n[\nGCC \n6\n.4.0\n]\n on linux2\nType \nhelp\n, \ncopyright\n, \ncredits\n or \nlicense\n \nfor\n more information.\n\n\n\n\n \nscontain.com\n, January 2018. \nQuestions or Suggestions?", 
            "title": "MrEnclave"
        }, 
        {
            "location": "/MrEnclave/#determining-mrenclave", 
            "text": "An enclave is identified by a hash value which is called   MrEnclave . This has is determined by content of the pages of an enclave and the access rights. In particular, the means that some of the  SCONE environment  variables like  SCONE_HEAP  and  SCONE_ALLOW_DLOPEN   will affect  MrEnclave .  To determine  MrEnclave , we provide a simple way to determine  MrEnclave  on the developer site via environment variable   SCONE_HASH=1 .", 
            "title": "Determining MrEnclave"
        }, 
        {
            "location": "/MrEnclave/#example-mrenclave-of-python", 
            "text": "Let us determine  MrEnclave  of our python interpreter. We start the container\nand then set environment variable  SCONE_HASH=1  to ask SCONE to print  MrEnclave  and then terminate and  SCONE_ALPINE=1  to ensure that the application is indeed started with SCONE.    Note  When setting  SCONE_HASH=1  the program is not executed - only MrEnclave is printed on stdout.**    docker run -it  sconecuratedimages/crosscompilers:python27 sh\n$  SCONE_HASH = 1   SCONE_ALPINE = 1  /usr/local/bin/python\n5430b3c0ab0e8a24ea4481e6022704cdbbcff68f6457eb0cdeaecfd734fec541  Now, let us change the heap size via environment variable  SCONE_HEAP \nby asking for a  2GB  heap:  $  SCONE_HEAP = 2G  SCONE_HASH = 1   SCONE_ALPINE = 1  /usr/local/bin/python\naa25d6e1863819fca72f4f3315131ba4a438d1e1643c030190ca665215912465  By default, SCONE does not permit to load dynamic libraries after startup. By setting  SCONE_ALLOW_DLOPEN=1 , we permit to load dynamic libraries during runtime. This changes  MrEnclave :  $  SCONE_ALLOW_DLOPEN = 1   SCONE_HEAP = 2G  SCONE_HASH = 1   SCONE_ALPINE = 1  /usr/local/bin/python\n9c56db536e046a5fb84a5c482ce86e6592071dff75dc0e3eb27d701cf2c40508", 
            "title": "Example: MrEnclave of Python"
        }, 
        {
            "location": "/MrEnclave/#using-debug-output", 
            "text": "As an alternative to  SCONE_HASH=1  is to print  MrEnclave  via debug messages by setting  SCONE_VERSION=1 :  $  SCONE_ALLOW_DLOPEN = 1   SCONE_HEAP = 2G  SCONE_VERSION = 1   SCONE_ALPINE = 1  /usr/local/bin/python export   SCONE_QUEUES = 4  export   SCONE_SLOTS = 256  export   SCONE_SIGPIPE = 0  export   SCONE_MMAP32BIT = 0  export   SCONE_SSPINS = 100  export   SCONE_SSLEEP = 4000  export   SCONE_KERNEL = 0  export   SCONE_HEAP = 2147483648  export   SCONE_STACK = 81920  export   SCONE_CONFIG = /etc/sgx-musl.conf export   SCONE_MODE = sim export   SCONE_SGXBOUNDS = no export   SCONE_VARYS = no export   SCONE_ALLOW_DLOPEN = yes  ( protected )  export   SCONE_MPROTECT = no\nRevision: b6a40e091e2adb253f019401723d2a734e887a74  ( Fri Jan  26   07 :44:44  2018  +0100 ) \nBranch: master  ( dirty ) \nConfigure options: --enable-shared --enable-debug --prefix = /scone/src/built/cross-compiler/x86_64-linux-musl\n\nEnclave hash: 9c56db536e046a5fb84a5c482ce86e6592071dff75dc0e3eb27d701cf2c40508\nPython  2 .7.14  ( default, Jan  10   2018 ,  05 :35:30 )   [ GCC  6 .4.0 ]  on linux2\nType  help ,  copyright ,  credits  or  license   for  more information.    scontain.com , January 2018.  Questions or Suggestions?", 
            "title": "Using debug output"
        }, 
        {
            "location": "/news/", 
            "text": "SCONE News\n\n\n\n\n\n\nNEWS (2018-03-17): SCONE adds an updated curated nginx image (sconecuratedimages/apps:nginx-1.13-alpine\n\n\n\n\n\n\nNEWS (2018-02-02): SCONE adds support for \nZookeeper\n (sconecuratedimages/crosscompilers:zookeeper).\n\n\n\n\n\n\nNEWS (2018-02-02): SCONE adds support for \nPython 3.6.4\n (sconecuratedimages/crosscompilers:python3.6.4-alpine3.7).\n\n\n\n\n\n\nNEWS (2018-01-27): SCONE adds support for \nJava\n.\n\n\n\n\n\n\nNEWS (2018-01-27): SCONE adds curated \nNode\n image (sconecuratedimages/crosscompilers:node-4.8.6).\n\n\n\n\n\n\nNEWS (2018-01-27): SCONE adds curated MongoDB image (sconecuratedimages/crosscompilers:mongodb).\n\n\n\n\n\n\nNEWS (2018-01-27): SCONE adds curated Memcached image (sconecuratedimages/crosscompilers:memcached).\n\n\n\n\n\n\nNEWS (2018-01-27): SCONE adds curated Vault image (sconecuratedimages/crosscompilers:vault).\n\n\n\n\n\n\nSOON TO COME: While SGX is affected by Spectre, SCONE adds protection against Spectre (variants 1 and 2)", 
            "title": "News"
        }, 
        {
            "location": "/news/#scone-news", 
            "text": "NEWS (2018-03-17): SCONE adds an updated curated nginx image (sconecuratedimages/apps:nginx-1.13-alpine    NEWS (2018-02-02): SCONE adds support for  Zookeeper  (sconecuratedimages/crosscompilers:zookeeper).    NEWS (2018-02-02): SCONE adds support for  Python 3.6.4  (sconecuratedimages/crosscompilers:python3.6.4-alpine3.7).    NEWS (2018-01-27): SCONE adds support for  Java .    NEWS (2018-01-27): SCONE adds curated  Node  image (sconecuratedimages/crosscompilers:node-4.8.6).    NEWS (2018-01-27): SCONE adds curated MongoDB image (sconecuratedimages/crosscompilers:mongodb).    NEWS (2018-01-27): SCONE adds curated Memcached image (sconecuratedimages/crosscompilers:memcached).    NEWS (2018-01-27): SCONE adds curated Vault image (sconecuratedimages/crosscompilers:vault).    SOON TO COME: While SGX is affected by Spectre, SCONE adds protection against Spectre (variants 1 and 2)", 
            "title": "SCONE News"
        }, 
        {
            "location": "/SCONE_Publications/", 
            "text": "SCONE Related Publications\n\n\nSCONE: Secure Linux Containers with Intel SGX, USENIX, OSDI 2016\n\n\nThis paper describes how we support unmodified applications inside of enclaves. The focus is on our asynchronous system\n call interface.\n\n\n\n\n\n\nAuthors\n:  Sergei Arnautov, Bohdan Trach, Franz Gregor, Thomas Knauth, Andr\u00e9 Martin, Christian Priebe, Joshua Lind, Divya Muthukumaran, Daniel O'Keeffe, Mark L Stillwell, David Goltzsche, Dave Eyers, R\u00fcdiger Kapitza, Peter Pietzuch, Christof Fetzer\n\n\n\n\n\n\nMedia\n: \npdf\n, \nslides\n\n\naudio\n\n\n\n\n\n\nAbstract\n:  In multi-tenant environments, Linux containers managed by Docker or Kubernetes have a lower resource footprint, faster startup times, and higher I/O performance compared to virtual machines (VMs) on hypervisors. Yet their weaker isolation guarantees, enforced through software kernel mechanisms, make it easier for attackers to compromise the confidentiality and integrity of application data within containers.\nWe describe SCONE, a secure container mechanism for Docker that uses the SGX trusted execution support of Intel CPUs to protect container processes from outside attacks. The design of SCONE leads to (i) a small trusted computing base (TCB) and (ii) a low performance overhead: SCONE offers a secure C standard library interface that transparently encrypts/decrypts I/O data; to reduce the performance impact of thread synchronization and system calls within SGX enclaves, SCONE supports user-level threading and asynchronous system calls. Our evaluation shows that it protects unmodified applications with SGX, achieving 0.6x\u20131.2x of native throughput.\n\n\n\n\n\n\nBuilding Critical Applications Using Microservices, IEEE Security \n Privacy, Volume: 14 Issue: 6, December 2016\n\n\n\n\n\n\nAuthor\n: Christof Fetzer\n\n\n\n\n\n\nMedia\n: \nhtml\n\n\n\n\n\n\nAbstract\n: Safeguarding the correctness of critical software is a grand challenge. A microservice-based system is described that builds trustworthy systems on top of legacy hardware and software components, ensuring microservices' integrity, confidentiality, and correct execution with the help of secure enclaves.\n\n\n\n\n\n\nSGXBounds: Memory Safety for Shielded Execution, EuroSys 2017\n\n\nTo protect the code running inside of an enclave, we implemented a novel bounds checker for enclaves. While we had expected\n to just be able to use MPX, we had to realized that MPX does not perform that well inside of enclaves. For details regarding\n the overheads,  please see this paper. \nThis won the best paper award of EuroSys 2017.\n\n\n\n\n\n\nAuthors\n: D. Kuvaiskii, O. Oleksenko, S. Arnautov, B. Trach, P. Bhatotia, P. Felber, C. Fetzer \n\n\n\n\n\n\nMedia\n: \npdf\n \n\n\n\n\n\n\nAbstract\n: Shielded execution based on Intel SGX provides strong security guarantees for legacy applications running on untrusted platforms. However, memory safety attacks such as Heartbleed can render the confidentiality and integrity properties of shielded execution completely ineffective. To prevent these attacks, the state-of-the-art memory-safety approaches can be used in the context of shielded execution. In this work, we first showcase that two prominent software- and hardware-based defenses, AddressSanitizer and Intel MPX respectively, are impractical for shielded execution due to high performance and memory overheads. This motivated our design of SGXBounds -- an efficient memory-safety approach for shielded execution exploiting the architectural features of Intel SGX. Our design is based on a simple combination of tagged pointers and compact memory layout. We implemented SGXBounds based on the LLVM compiler framework targeting unmodified multithreaded applications. Our evaluation using Phoenix, PARSEC, and RIPE benchmark suites shows that SGXBounds has performance and memory overheads of 18% and 0.1% respectively, while providing security guarantees similar to AddressSanitizer and Intel MPX. We have obtained similar results with four real-world case studies: SQLite, Memcached, Apache, and Nginx.\n\n\n\n\n\n\nFFQ: A Fast Single-Producer/Multiple-Consumer Concurrent FIFO Queue, IPDPS 2017\n\n\nThis paper describes our new lock-free queue for our asynchronous system calls.\n\n\n\n\n\n\nAuthors\n: Sergei Arnautov, Pascal Felber, Christof Fetzer and Bohdan Trach\n\n\n\n\n\n\nMedia\n: \nsorry, not yet available\n \n\n\n\n\n\n\nAbstract\n:  With the spreading of multi-core architectures, operating systems and applications are becoming increasingly more concurrent and their scalability is often limited by the primitives used to synchronize the different hardware threads. In this paper, we address the problem of how to optimize the throughput of a system with multiple producer and consumer threads. Such applications typically synchronize their threads via multi- producer/multi-consumer FIFO queues, but existing solutions have poor scalability, as we could observe when designing a secure application framework that requires high-throughput communication between many concurrent threads. In our target system, however, the items enqueued by different producers do not necessarily need to be FIFO ordered. Hence, we propose a fast FIFO queue, FFQ, that aims at maximizing throughput by specializing the algorithm for single-producer/multiple-consumer settings: each producer has its own queue from which multiple consumers can concurrently dequeue. Furthermore, while we pro- vide a wait-free interface for producers, we limit ourselves to lock-free consumers to eliminate the need for helping. We also propose a multi-producer variant to show which synchronization operations we were able to remove by focusing on a single producer variant. Our evaluation analyses the performance using micro- benchmarks and compares our results with other state-of-the-art solutions: FFQ exhibits excellent performance and scalability.\n\n\n\n\n\n\nPESOS: Policy Enhanced Secure Object Store, EuroSys 2018\n\n\n\n\n\n\nAuthors\n:  Robert Krahn, Bohdan Trach (TU Dresden), Anjo Vahldiek-Oberwagner (MPI-SWS), Thomas Knauth (Intel/TU Dresden), Pramod Bhatotia (University of Edinburgh), and Christof Fetzer (TU Dresden)\n\n\n\n\n\n\nMedia\n: \nsorry, not yet available\n \n\n\n\n\n\n\nAbstract\n:  Third-party storage services pose the risk of integrity and confidentiality violations as the current storage policy enforcement mechanisms are spread across many layers in the system stack. To mitigate these security vulnerabilities, we present the design and implementation of Pesos, a Policy Enhanced Secure Object Store (Pesos) for untrusted third-party storage providers. Pesos allows clients to specify per-object security policies, concisely and separately from the storage stack, and enforces these policies by securely mediating the I/O in the persistence layer through a single uni ed enforcement layer. More broadly, Pesos exposes a rich set of storage policies ensuring the integrity, confidentiality, and access accounting for data storage through a declarative policy language.\n\n\nPesos enforces these policies on untrusted commodity plat- forms by leveraging a combination of two trusted computing technologies: Intel SGX for trusted execution environment (TEE) and Kinetic Open Storage for trusted storage. We have implemented Pesos as a fully-functional storage system supporting many useful end-to-end storage features, and a range of effective performance optimizations. We evaluated Pesos using a range of micro-benchmarks, and real-world use cases. Our evaluation shows that Pesos incurs reasonable performance overheads for the enforcement of policies while keeping the trusted computing base (TCB) small.\n\n\n\n\n\n\nShieldBox: Secure Middleboxes using Shielded Execution, SIGCOMM SOSR 2018\n\n\n\n\n\n\nAuthors\n: Bohdan Trach, Alfred Krohmer, Franz Gregor, Sergei Arnautov, Pramod Bhatotia, Christof Fetzer\n\n\n\n\n\n\nMedia\n: \nsorry, not yet available\n \n\n\n\n\n\n\nAbstract\n: Middleboxes that process confidential  data cannot be securely deployed in untrusted cloud environments.   To securely outsource middleboxes to the cloud, state-of-the-art systems advocate network processing over the encrypted traffic.  Unfortunately, these systems support only restrictive functionalities, and incur prohibitively high overheads.% due to the complex computations involved over the encrypted traffic.\n\n\nThis motivated the design of ShieldBox---a secure middlebox framework for deploying high-performance network functions (NFs) over untrusted commodity servers.  ShieldBox securely processes encrypted traffic inside a secure container by leveraging shielded execution. More specifically, ShieldBox builds on hardware-assisted memory protection based on Intel SGX to provide strong confidentiality and integrity guarantees. For middlebox developers, ShieldBox exposes a generic interface based on Click to design and implement a wide-range of NFs  using its out-of-the-box elements and C++ extensions. For network operators, ShieldBox provides configuration and attestation service for seamless and verifiable deployment of middleboxes. We have implemented  ShieldBox supporting important end-to-end features required for secure network processing, and performance optimizations. Our extensive evaluation shows that ShieldBox achieves a near-native throughput and latency to securely process confidential data at line rate.\n\n\n\n\n\n\n \nscontain.com\n, March 2018. \nQuestions or Suggestions?", 
            "title": "Publications"
        }, 
        {
            "location": "/SCONE_Publications/#scone-related-publications", 
            "text": "SCONE: Secure Linux Containers with Intel SGX, USENIX, OSDI 2016  This paper describes how we support unmodified applications inside of enclaves. The focus is on our asynchronous system\n call interface.    Authors :  Sergei Arnautov, Bohdan Trach, Franz Gregor, Thomas Knauth, Andr\u00e9 Martin, Christian Priebe, Joshua Lind, Divya Muthukumaran, Daniel O'Keeffe, Mark L Stillwell, David Goltzsche, Dave Eyers, R\u00fcdiger Kapitza, Peter Pietzuch, Christof Fetzer    Media :  pdf ,  slides  audio    Abstract :  In multi-tenant environments, Linux containers managed by Docker or Kubernetes have a lower resource footprint, faster startup times, and higher I/O performance compared to virtual machines (VMs) on hypervisors. Yet their weaker isolation guarantees, enforced through software kernel mechanisms, make it easier for attackers to compromise the confidentiality and integrity of application data within containers.\nWe describe SCONE, a secure container mechanism for Docker that uses the SGX trusted execution support of Intel CPUs to protect container processes from outside attacks. The design of SCONE leads to (i) a small trusted computing base (TCB) and (ii) a low performance overhead: SCONE offers a secure C standard library interface that transparently encrypts/decrypts I/O data; to reduce the performance impact of thread synchronization and system calls within SGX enclaves, SCONE supports user-level threading and asynchronous system calls. Our evaluation shows that it protects unmodified applications with SGX, achieving 0.6x\u20131.2x of native throughput.    Building Critical Applications Using Microservices, IEEE Security   Privacy, Volume: 14 Issue: 6, December 2016    Author : Christof Fetzer    Media :  html    Abstract : Safeguarding the correctness of critical software is a grand challenge. A microservice-based system is described that builds trustworthy systems on top of legacy hardware and software components, ensuring microservices' integrity, confidentiality, and correct execution with the help of secure enclaves.    SGXBounds: Memory Safety for Shielded Execution, EuroSys 2017  To protect the code running inside of an enclave, we implemented a novel bounds checker for enclaves. While we had expected\n to just be able to use MPX, we had to realized that MPX does not perform that well inside of enclaves. For details regarding\n the overheads,  please see this paper.  This won the best paper award of EuroSys 2017.    Authors : D. Kuvaiskii, O. Oleksenko, S. Arnautov, B. Trach, P. Bhatotia, P. Felber, C. Fetzer     Media :  pdf      Abstract : Shielded execution based on Intel SGX provides strong security guarantees for legacy applications running on untrusted platforms. However, memory safety attacks such as Heartbleed can render the confidentiality and integrity properties of shielded execution completely ineffective. To prevent these attacks, the state-of-the-art memory-safety approaches can be used in the context of shielded execution. In this work, we first showcase that two prominent software- and hardware-based defenses, AddressSanitizer and Intel MPX respectively, are impractical for shielded execution due to high performance and memory overheads. This motivated our design of SGXBounds -- an efficient memory-safety approach for shielded execution exploiting the architectural features of Intel SGX. Our design is based on a simple combination of tagged pointers and compact memory layout. We implemented SGXBounds based on the LLVM compiler framework targeting unmodified multithreaded applications. Our evaluation using Phoenix, PARSEC, and RIPE benchmark suites shows that SGXBounds has performance and memory overheads of 18% and 0.1% respectively, while providing security guarantees similar to AddressSanitizer and Intel MPX. We have obtained similar results with four real-world case studies: SQLite, Memcached, Apache, and Nginx.    FFQ: A Fast Single-Producer/Multiple-Consumer Concurrent FIFO Queue, IPDPS 2017  This paper describes our new lock-free queue for our asynchronous system calls.    Authors : Sergei Arnautov, Pascal Felber, Christof Fetzer and Bohdan Trach    Media :  sorry, not yet available      Abstract :  With the spreading of multi-core architectures, operating systems and applications are becoming increasingly more concurrent and their scalability is often limited by the primitives used to synchronize the different hardware threads. In this paper, we address the problem of how to optimize the throughput of a system with multiple producer and consumer threads. Such applications typically synchronize their threads via multi- producer/multi-consumer FIFO queues, but existing solutions have poor scalability, as we could observe when designing a secure application framework that requires high-throughput communication between many concurrent threads. In our target system, however, the items enqueued by different producers do not necessarily need to be FIFO ordered. Hence, we propose a fast FIFO queue, FFQ, that aims at maximizing throughput by specializing the algorithm for single-producer/multiple-consumer settings: each producer has its own queue from which multiple consumers can concurrently dequeue. Furthermore, while we pro- vide a wait-free interface for producers, we limit ourselves to lock-free consumers to eliminate the need for helping. We also propose a multi-producer variant to show which synchronization operations we were able to remove by focusing on a single producer variant. Our evaluation analyses the performance using micro- benchmarks and compares our results with other state-of-the-art solutions: FFQ exhibits excellent performance and scalability.    PESOS: Policy Enhanced Secure Object Store, EuroSys 2018    Authors :  Robert Krahn, Bohdan Trach (TU Dresden), Anjo Vahldiek-Oberwagner (MPI-SWS), Thomas Knauth (Intel/TU Dresden), Pramod Bhatotia (University of Edinburgh), and Christof Fetzer (TU Dresden)    Media :  sorry, not yet available      Abstract :  Third-party storage services pose the risk of integrity and confidentiality violations as the current storage policy enforcement mechanisms are spread across many layers in the system stack. To mitigate these security vulnerabilities, we present the design and implementation of Pesos, a Policy Enhanced Secure Object Store (Pesos) for untrusted third-party storage providers. Pesos allows clients to specify per-object security policies, concisely and separately from the storage stack, and enforces these policies by securely mediating the I/O in the persistence layer through a single uni ed enforcement layer. More broadly, Pesos exposes a rich set of storage policies ensuring the integrity, confidentiality, and access accounting for data storage through a declarative policy language.  Pesos enforces these policies on untrusted commodity plat- forms by leveraging a combination of two trusted computing technologies: Intel SGX for trusted execution environment (TEE) and Kinetic Open Storage for trusted storage. We have implemented Pesos as a fully-functional storage system supporting many useful end-to-end storage features, and a range of effective performance optimizations. We evaluated Pesos using a range of micro-benchmarks, and real-world use cases. Our evaluation shows that Pesos incurs reasonable performance overheads for the enforcement of policies while keeping the trusted computing base (TCB) small.    ShieldBox: Secure Middleboxes using Shielded Execution, SIGCOMM SOSR 2018    Authors : Bohdan Trach, Alfred Krohmer, Franz Gregor, Sergei Arnautov, Pramod Bhatotia, Christof Fetzer    Media :  sorry, not yet available      Abstract : Middleboxes that process confidential  data cannot be securely deployed in untrusted cloud environments.   To securely outsource middleboxes to the cloud, state-of-the-art systems advocate network processing over the encrypted traffic.  Unfortunately, these systems support only restrictive functionalities, and incur prohibitively high overheads.% due to the complex computations involved over the encrypted traffic.  This motivated the design of ShieldBox---a secure middlebox framework for deploying high-performance network functions (NFs) over untrusted commodity servers.  ShieldBox securely processes encrypted traffic inside a secure container by leveraging shielded execution. More specifically, ShieldBox builds on hardware-assisted memory protection based on Intel SGX to provide strong confidentiality and integrity guarantees. For middlebox developers, ShieldBox exposes a generic interface based on Click to design and implement a wide-range of NFs  using its out-of-the-box elements and C++ extensions. For network operators, ShieldBox provides configuration and attestation service for seamless and verifiable deployment of middleboxes. We have implemented  ShieldBox supporting important end-to-end features required for secure network processing, and performance optimizations. Our extensive evaluation shows that ShieldBox achieves a near-native throughput and latency to securely process confidential data at line rate.      scontain.com , March 2018.  Questions or Suggestions?", 
            "title": "SCONE Related Publications"
        }, 
        {
            "location": "/aboutScone/", 
            "text": "About SCONE\n\n\nThe SCONE platform is commercially supported by \nscontain.com\n. \n\n\nThe SCONE platform has been developed at the \nSystems Engineering group\n at \nTU Dresden\n in the context of the following EU projects:\n\n\n\n\n\n\nSereca\n which investigates how to use Intel SGX enclave in the context of reactive programs written in \nVert.x\n.\n\n\n\n\n\n\nSecure Cloud\n which focuses on the processing of (big) data in untrusted clouds.\n\n\n\n\n\n\nWe investigate use cases and extensions of SCONE in the context of the following EU projects:\n\n\n\n\n\n\nSELIS\n: we investigate how to secure data processing within a \nShared European Logistics Intelligent Information Space\n with the help of SCONE.\n\n\n\n\n\n\nATMOSPHERE\n: a new EU project in which we address secure data management services. This will help to extend the SCONE platform.\n\n\n\n\n\n\nLEGATO\n: a new EU project in which we address \nhigh integrity computations\n inside of enclaves to be able to detect and tolerate miscomputations inside of enclaves.\n\n\n\n\n\n\nComputing Resources\n\n\nscontain.com\n can provide access to SGX-capable machines.\n\n\nConsulting Services\n\n\nscontain.com\n provides consulting services as well as helping you to port your applications to SGX.\n\n\nContact\n\n\nIf you want to evaluate the SCONE platform, want to rent some SGX-capable computing resources, need SGX and SCONE-related consulting, or have some technical questions, please contact us at \ninfo@scontain.com\n.\n\n\nLegal Notice\n\n\nThe content of this documentation is maintained by \nscontain.com\n.\n\n\n \nscontain.com\n, March 2018. \nQuestions or Suggestions?", 
            "title": "About Scone"
        }, 
        {
            "location": "/aboutScone/#about-scone", 
            "text": "The SCONE platform is commercially supported by  scontain.com .   The SCONE platform has been developed at the  Systems Engineering group  at  TU Dresden  in the context of the following EU projects:    Sereca  which investigates how to use Intel SGX enclave in the context of reactive programs written in  Vert.x .    Secure Cloud  which focuses on the processing of (big) data in untrusted clouds.    We investigate use cases and extensions of SCONE in the context of the following EU projects:    SELIS : we investigate how to secure data processing within a  Shared European Logistics Intelligent Information Space  with the help of SCONE.    ATMOSPHERE : a new EU project in which we address secure data management services. This will help to extend the SCONE platform.    LEGATO : a new EU project in which we address  high integrity computations  inside of enclaves to be able to detect and tolerate miscomputations inside of enclaves.", 
            "title": "About SCONE"
        }, 
        {
            "location": "/aboutScone/#computing-resources", 
            "text": "scontain.com  can provide access to SGX-capable machines.", 
            "title": "Computing Resources"
        }, 
        {
            "location": "/aboutScone/#consulting-services", 
            "text": "scontain.com  provides consulting services as well as helping you to port your applications to SGX.", 
            "title": "Consulting Services"
        }, 
        {
            "location": "/aboutScone/#contact", 
            "text": "If you want to evaluate the SCONE platform, want to rent some SGX-capable computing resources, need SGX and SCONE-related consulting, or have some technical questions, please contact us at  info@scontain.com .", 
            "title": "Contact"
        }, 
        {
            "location": "/aboutScone/#legal-notice", 
            "text": "The content of this documentation is maintained by  scontain.com .    scontain.com , March 2018.  Questions or Suggestions?", 
            "title": "Legal Notice"
        }, 
        {
            "location": "/glossary/", 
            "text": "Glossary\n\n\n\n\n\n\ncloud-native application\n. An application designed to run inside of a cloud. One requirement is that the application is deployed with the help of containers.\n\n\n\n\n\n\ncontainer\n. An light-weight alternative to a virtual machine (VM). The isolation of containers is implemented by the operating system. Docker and Kubernetes use Linux for isolation. In the case of VMs, the isolation is implemented with the help of CPU extensions.\n\n\n\n\n\n\nenclave\n. This is an alias for \nSGX enclave\n.\n\n\n\n\n\n\nEPC\n. A cache of memory pages belonging to enclaves. This cache resides in a reserved part of the main memory that is directly managed by the CPU (and not by the operating system or the hypervisor). The data in this cache is encrypted. Unlike enclave pages residing in the main memory, the CPU can encrypt and decrypt individual cache lines residing inside the EPC. This results in low overheads.\n\n\n\n\n\n\nSecure container\n. A container which uses additional hardware isolation mechanisms, i.e., SGX to provide better application security. In particular, a secure container runs one or more \nsecure programs\n. Additionally, the integrity and confidentiality of files inside a secure container are protected by SCONE.\n\n\n\n\n\n\nSecure program\n. A program that executes inside an enclave.\n\n\n\n\n\n\nSGX\n. A CPU extension by Intel that permits to create SGX enclaves.\n\n\n\n\n\n\nSGX enclave\n. A protected area inside the address space of a program such that only the code inside this enclave can access the data and code stored in this address range. All pages belonging to an enclave are encrypted by the CPU and only the CPU knows the encryption key. These pages can reside in the main memory or the EPC.\n\n\n\n\n\n\nethread, lthread, sthread\n. SCONE usese different kind of threads:\n\n\n\n\nethread\n: a thread that executes application threads inside of an enclave\n\n\nlthread\n: an application thread. Typically, created by the application directly or indirectly via a \npthread_create\n call. In SCONE, this \npthread_create\n call will create a \nlthread\n. The lthread is executed by some \nethread\n. In this way, we can quickly switch to another application thread whenever an application thread would get block. In this way, we reduce the number of enclave entries and exits - which are costly.\n\n\nsthread\n: a thread that runs outside of the enclave and that executes system calls on behalf of the threads running inside the enclave\n\n\n\n\n\n\n\n\n \nscontain.com\n, November 2017. \nQuestions or Suggestions?", 
            "title": "Glossary"
        }, 
        {
            "location": "/glossary/#glossary", 
            "text": "cloud-native application . An application designed to run inside of a cloud. One requirement is that the application is deployed with the help of containers.    container . An light-weight alternative to a virtual machine (VM). The isolation of containers is implemented by the operating system. Docker and Kubernetes use Linux for isolation. In the case of VMs, the isolation is implemented with the help of CPU extensions.    enclave . This is an alias for  SGX enclave .    EPC . A cache of memory pages belonging to enclaves. This cache resides in a reserved part of the main memory that is directly managed by the CPU (and not by the operating system or the hypervisor). The data in this cache is encrypted. Unlike enclave pages residing in the main memory, the CPU can encrypt and decrypt individual cache lines residing inside the EPC. This results in low overheads.    Secure container . A container which uses additional hardware isolation mechanisms, i.e., SGX to provide better application security. In particular, a secure container runs one or more  secure programs . Additionally, the integrity and confidentiality of files inside a secure container are protected by SCONE.    Secure program . A program that executes inside an enclave.    SGX . A CPU extension by Intel that permits to create SGX enclaves.    SGX enclave . A protected area inside the address space of a program such that only the code inside this enclave can access the data and code stored in this address range. All pages belonging to an enclave are encrypted by the CPU and only the CPU knows the encryption key. These pages can reside in the main memory or the EPC.    ethread, lthread, sthread . SCONE usese different kind of threads:   ethread : a thread that executes application threads inside of an enclave  lthread : an application thread. Typically, created by the application directly or indirectly via a  pthread_create  call. In SCONE, this  pthread_create  call will create a  lthread . The lthread is executed by some  ethread . In this way, we can quickly switch to another application thread whenever an application thread would get block. In this way, we reduce the number of enclave entries and exits - which are costly.  sthread : a thread that runs outside of the enclave and that executes system calls on behalf of the threads running inside the enclave       scontain.com , November 2017.  Questions or Suggestions?", 
            "title": "Glossary"
        }
    ]
}