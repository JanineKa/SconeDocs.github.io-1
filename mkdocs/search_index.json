{
    "docs": [
        {
            "location": "/",
            "text": "Welcome to SCONE\n\n\nSCONE is a platform to \nbuild and run secure applications\n with the help of \nIntel SGX\n (Software Guard eXtensions). \nIn a nutshell, our objective is to run applications such that data is \nalways encrypted\n, i.e., all data at rest, all data on the wire as well as all data in main memory is encrypted.\n\n\nOur aim is it to make securing your application \nas easy as possible\n. Switching to SCONE is simple since applications do not need to be modified. Moreover, we provide a tight integration with Docker Swarm.  In case you are already using Docker \ncompose files\n, we provide a simple way to secure your services and applications.\n\n\nWhile SCONE focuses on securing containers and cloud-native applications, SCONE can help you to secure almost any program running on top of Linux.\n\n\nSo, what problems can SCONE help me to solve?\n\n\nSecure application configuration\n\n\nSCONE provides applications with secrets in a secure fashion. \nWhy is that a problem?\n  Say, you want to run MySQL and you configure MySQL to encrypt its data at rest. To do so, MySQL requires a key to decrypt and encrypt its files. One can store this key in the MySQL configuration file but this configuration file cannot be encrypted since MySQL would need a key to decrypt the file. SCONE helps developers to solve such configuration issues in the following ways:\n\n\n\n\n\n\nsecure configuration files\n. SCONE can transparently decrypt encrypted configuration files. It will give access to the plain text only to a given program, like, MySQL. No source code changes are needed for this to work.\n\n\n\n\n\n\nsecure environment variables\n. SCONE gives applications access to environment variables that are not visible to anybody else - even users with root access or the operating system. \nWhy would I need this?\n Consider the MySQL example from above. You can pass user passwords via environment variables like \nMYSQL_ROOT_PASSWORD\n and \nMYSQL_PASSWORD\n to the MySQL. We need to protect these environment variables to prevent unauthorized accesses to the MySQL database.\n\n\n\n\n\n\nsecure command line arguments\n. Some applications might not use environment variables but command line arguments to pass secrets to the application. SCONE provides a secure way to pass arguments to your application without other privileged parties, like the operating system, being able to see the arguments.\n\n\n\n\n\n\nTransparent attestation\n\n\nIn operations mode, \nSCONE will verify that the correct code is running\n before passing the configuration to the application. To ensure this, SCONE provides a \nlocal attestation and configuration service\n: this service provides only the code with the correct signature (\nMRENCLAVE\n) with its secrets. For debugging and development, you can run code inside of enclaves without attestation.\n\n\nSecure main memory\n\n\nAn adversary with root access can read the memory content of any process. In this way, an adversary can gain access to keys that an application is using, for example, the keys to protect its data at rest. SCONE helps to \nprotect the main memory\n:\n\n\n\n\n\n\nno access by adversaries - even those who have already gained root access,\n\n\n\n\n\n\nno access by the operating system - even if compromised,\n\n\n\n\n\n\nno access by the hypervisor - even if compromised, and\n\n\n\n\n\n\nno access by the cloud provider, and\n\n\n\n\n\n\nno access by evil maids - despite having physical access to the host.\n\n\n\n\n\n\nIntegration with secure key store\n\n\nEncryption keys must be protected. In many installations, one does not want humans to be able to see encryption keys. Hence, one generates keys and stores these in keystores. SCONE supports the integration with a keystore.\n\n\nTransparent TLS encryption\n\n\nSome popular applications like memcached do not support TLS out of the box. SCONE can transparently add TLS encryption to TCP connections. In SCONE, we never uses an external process for TLS termination. In this way, the plain text is never seen by the operating system or any adversary.\n\n\nTransparent file protection\n\n\nSCONE protects via \ntransparent file protection\n the integrity and confidentiality of not only configuration files but any file. This protection does not require any source code changes. A file can either be \nintegrity-protected\n only (i.e., the file is stored in plain text but modifications are detected) or \nconfidentiality-\n and \nintegrity-protected\n (i.e., the file is encrypted and modifications are detected).\n\n\nEase of use\n\n\nWe provide slightly \nextended Docker compose files\n to start an application consisting of a set of services inside of enclaves.\n\n\n\u00a9 \nscontain.com\n, November 2017. \nQuestions or Suggestions?",
            "title": "Introduction"
        },
        {
            "location": "/#welcome-to-scone",
            "text": "SCONE is a platform to  build and run secure applications  with the help of  Intel SGX  (Software Guard eXtensions). \nIn a nutshell, our objective is to run applications such that data is  always encrypted , i.e., all data at rest, all data on the wire as well as all data in main memory is encrypted.  Our aim is it to make securing your application  as easy as possible . Switching to SCONE is simple since applications do not need to be modified. Moreover, we provide a tight integration with Docker Swarm.  In case you are already using Docker  compose files , we provide a simple way to secure your services and applications.  While SCONE focuses on securing containers and cloud-native applications, SCONE can help you to secure almost any program running on top of Linux.",
            "title": "Welcome to SCONE"
        },
        {
            "location": "/#so-what-problems-can-scone-help-me-to-solve",
            "text": "Secure application configuration  SCONE provides applications with secrets in a secure fashion.  Why is that a problem?   Say, you want to run MySQL and you configure MySQL to encrypt its data at rest. To do so, MySQL requires a key to decrypt and encrypt its files. One can store this key in the MySQL configuration file but this configuration file cannot be encrypted since MySQL would need a key to decrypt the file. SCONE helps developers to solve such configuration issues in the following ways:    secure configuration files . SCONE can transparently decrypt encrypted configuration files. It will give access to the plain text only to a given program, like, MySQL. No source code changes are needed for this to work.    secure environment variables . SCONE gives applications access to environment variables that are not visible to anybody else - even users with root access or the operating system.  Why would I need this?  Consider the MySQL example from above. You can pass user passwords via environment variables like  MYSQL_ROOT_PASSWORD  and  MYSQL_PASSWORD  to the MySQL. We need to protect these environment variables to prevent unauthorized accesses to the MySQL database.    secure command line arguments . Some applications might not use environment variables but command line arguments to pass secrets to the application. SCONE provides a secure way to pass arguments to your application without other privileged parties, like the operating system, being able to see the arguments.    Transparent attestation  In operations mode,  SCONE will verify that the correct code is running  before passing the configuration to the application. To ensure this, SCONE provides a  local attestation and configuration service : this service provides only the code with the correct signature ( MRENCLAVE ) with its secrets. For debugging and development, you can run code inside of enclaves without attestation.  Secure main memory  An adversary with root access can read the memory content of any process. In this way, an adversary can gain access to keys that an application is using, for example, the keys to protect its data at rest. SCONE helps to  protect the main memory :    no access by adversaries - even those who have already gained root access,    no access by the operating system - even if compromised,    no access by the hypervisor - even if compromised, and    no access by the cloud provider, and    no access by evil maids - despite having physical access to the host.    Integration with secure key store  Encryption keys must be protected. In many installations, one does not want humans to be able to see encryption keys. Hence, one generates keys and stores these in keystores. SCONE supports the integration with a keystore.  Transparent TLS encryption  Some popular applications like memcached do not support TLS out of the box. SCONE can transparently add TLS encryption to TCP connections. In SCONE, we never uses an external process for TLS termination. In this way, the plain text is never seen by the operating system or any adversary.  Transparent file protection  SCONE protects via  transparent file protection  the integrity and confidentiality of not only configuration files but any file. This protection does not require any source code changes. A file can either be  integrity-protected  only (i.e., the file is stored in plain text but modifications are detected) or  confidentiality-  and  integrity-protected  (i.e., the file is encrypted and modifications are detected).  Ease of use  We provide slightly  extended Docker compose files  to start an application consisting of a set of services inside of enclaves.  \u00a9  scontain.com , November 2017.  Questions or Suggestions?",
            "title": "So, what problems can SCONE help me to solve?"
        },
        {
            "location": "/outline/",
            "text": "Getting Started\n\n\nThe simplest way to get started with SCONE is to use it in \nsimulation\n mode.\nIn \nsimulation mode\n, you do not need to install any new software on your host - as long as you have already a Docker engine running.\nYou could start by looking at the introduction of the \nSCONE CLI\n. \n\n\nAfter that, you could try to compile a simple \nhello world\n program. For now, we have a light access control to some of the SCONE docker images - like the ones needed for compiling the hello world program. Just send an email with your free \nDocker ID\n to \ninfo@scontain.com\n. After gaining access, you can compile the hello world program as shown in the \nSCONE Tutorial\n. You can run the program in simulation mode.  After that,  check out how you could automate the compilation with the help of a \nDockerfile\n. Next would be to run an example application in a \nSwarm\n.\n\n\nHardware Mode\n\n\nIf you have access to a SGX-capable host, you can, of course, run your programs in Intel SGX enclaves. For ease of use, we create all Docker images used in this tutorial such that applications run inside of enclaves if available. Otherwise, they run in simulation mode, i.e., outside of an enclave but all SCONE software running. When running your software in operations, you would of course force the programs to run only inside of enclaves. We show later how this can be enforced with the help of SCONE remote attestation.\n\n\nTo be able to use hardware mode, programs need access to the SGX device. If your hosts have already a Intel SGX driver installed, you are all set. Otherwise, SCONE can help you to install a lightly patched SGX driver that provides some more monitoring information - like the number of free EPC pages of a host (\nSCONE Host Setup\n).\n\n\nIf you want to run your programs in containers across multiple hosts with the help of a Docker swarm, you need to give access to the sgx device. Right now, Docker Swarm does not support mapping a device on all hosts. We have a simple patched version that gives access to the sgx device to all containers across all hosts. \nSCONE Host Setup\n installs this patched Docker engine.\n\n\nAfter having installed the SGX driver, you can run different applications inside of enclaves. In addition to \nC\n, \nC++\n, \nGO\n, \nFortran\n and \nRust\n (as part of SCONE crosscompiler image \nssconecuratedimages/crosscompilers:scone\n ), we also support \nPython\n and soon Node.js, Java, PHP and Lua.\n\n\nOutline\n\n\nThe SCONE platform technical documentation is structured as follows:\n\n\n\n\n\n\nSCONE Application-Oriented Security\n: a short introduction into SCONE's unique security approach.\n\n\n\n\n\n\nSCONE Background\n: a short introduction into why good systems security is difficult to achieve and how SCONE helps to complement systems security.\n\n\n\n\n\n\nSCONE CLI\n: introduces the SCONE command line interface and how to install the SCONE CLI.\n\n\n\n\n\n\nSCONE Host Setup\n: to run secure containers, we need to configure each host to run a Linux SGX driver and also a (for convenience, a patched) Docker engine.\n\n\n\n\n\n\nSCONE SGX Toolchain\n: SCONE supports cross-compilers (C, C++, more to come soon) to compile and build  applications for SGX enclaves.\n\n\n\n\n\n\nSCONE Curated Container Images\n: we will support a set of secure container images to simplify the use of SCONE.\n\n\n\n\n\n\nSCONE Tutorial\n: we show how to compile simple example applications with the help of the SCONE SGX Toolchain.\n\n\n\n\n\n\nSCONE Create Image\n: we show how to create a container image for Docker hub.\n\n\n\n\n\n\nSCONE Dockerfile\n: we support the use of Dockerfiles to generate Docker images that contain services running inside of SGX enclaves (this requires a \npatched Docker Engine\n).\n\n\n\n\n\n\nSCONE Swarm Example\n: shows an example of how to start SCONE services on top of a Docker Swarm.\n\n\n\n\n\n\nSCONE File Protection\n: we show how to transparently encrypt or authenticate files in the file system.\n\n\n\n\n\n\nGO inside Enclave\n: shows how to run GO programs inside of enclaves.\n\n\n\n\n\n\nPython inside Enclave\n: shows how to run simple Python applications inside of enclaves.\n\n\n\n\n\n\nSCONE Environment variables\n: short description of the SCONE environment variables.\n\n\n\n\n\n\nDetailed description of \nscone command line interface:\n\n\n\n\n\n\nscone host\n: installs patched SGX driver and patched Docker Engine on a host\n\n\n\n\n\n\nscone swarm\n: check if all hosts of a docker swarm are correctly installed\n\n\n\n\n\n\n\n\n\n\nPublications\n:  some of the technical aspects of SCONE have been published in scientific papers. We provide a short summary of the papers and links to the pdfs.\n\n\n\n\n\n\nGlossary\n:  definition of terms used within the SCONE technical documentation.\n\n\n\n\n\n\n\u00a9 \nscontain.com\n, November 2017. \nQuestions or Suggestions?",
            "title": "Getting Started"
        },
        {
            "location": "/outline/#getting-started",
            "text": "The simplest way to get started with SCONE is to use it in  simulation  mode.\nIn  simulation mode , you do not need to install any new software on your host - as long as you have already a Docker engine running.\nYou could start by looking at the introduction of the  SCONE CLI .   After that, you could try to compile a simple  hello world  program. For now, we have a light access control to some of the SCONE docker images - like the ones needed for compiling the hello world program. Just send an email with your free  Docker ID  to  info@scontain.com . After gaining access, you can compile the hello world program as shown in the  SCONE Tutorial . You can run the program in simulation mode.  After that,  check out how you could automate the compilation with the help of a  Dockerfile . Next would be to run an example application in a  Swarm .",
            "title": "Getting Started"
        },
        {
            "location": "/outline/#hardware-mode",
            "text": "If you have access to a SGX-capable host, you can, of course, run your programs in Intel SGX enclaves. For ease of use, we create all Docker images used in this tutorial such that applications run inside of enclaves if available. Otherwise, they run in simulation mode, i.e., outside of an enclave but all SCONE software running. When running your software in operations, you would of course force the programs to run only inside of enclaves. We show later how this can be enforced with the help of SCONE remote attestation.  To be able to use hardware mode, programs need access to the SGX device. If your hosts have already a Intel SGX driver installed, you are all set. Otherwise, SCONE can help you to install a lightly patched SGX driver that provides some more monitoring information - like the number of free EPC pages of a host ( SCONE Host Setup ).  If you want to run your programs in containers across multiple hosts with the help of a Docker swarm, you need to give access to the sgx device. Right now, Docker Swarm does not support mapping a device on all hosts. We have a simple patched version that gives access to the sgx device to all containers across all hosts.  SCONE Host Setup  installs this patched Docker engine.  After having installed the SGX driver, you can run different applications inside of enclaves. In addition to  C ,  C++ ,  GO ,  Fortran  and  Rust  (as part of SCONE crosscompiler image  ssconecuratedimages/crosscompilers:scone  ), we also support  Python  and soon Node.js, Java, PHP and Lua.",
            "title": "Hardware Mode"
        },
        {
            "location": "/outline/#outline",
            "text": "The SCONE platform technical documentation is structured as follows:    SCONE Application-Oriented Security : a short introduction into SCONE's unique security approach.    SCONE Background : a short introduction into why good systems security is difficult to achieve and how SCONE helps to complement systems security.    SCONE CLI : introduces the SCONE command line interface and how to install the SCONE CLI.    SCONE Host Setup : to run secure containers, we need to configure each host to run a Linux SGX driver and also a (for convenience, a patched) Docker engine.    SCONE SGX Toolchain : SCONE supports cross-compilers (C, C++, more to come soon) to compile and build  applications for SGX enclaves.    SCONE Curated Container Images : we will support a set of secure container images to simplify the use of SCONE.    SCONE Tutorial : we show how to compile simple example applications with the help of the SCONE SGX Toolchain.    SCONE Create Image : we show how to create a container image for Docker hub.    SCONE Dockerfile : we support the use of Dockerfiles to generate Docker images that contain services running inside of SGX enclaves (this requires a  patched Docker Engine ).    SCONE Swarm Example : shows an example of how to start SCONE services on top of a Docker Swarm.    SCONE File Protection : we show how to transparently encrypt or authenticate files in the file system.    GO inside Enclave : shows how to run GO programs inside of enclaves.    Python inside Enclave : shows how to run simple Python applications inside of enclaves.    SCONE Environment variables : short description of the SCONE environment variables.    Detailed description of  scone command line interface:    scone host : installs patched SGX driver and patched Docker Engine on a host    scone swarm : check if all hosts of a docker swarm are correctly installed      Publications :  some of the technical aspects of SCONE have been published in scientific papers. We provide a short summary of the papers and links to the pdfs.    Glossary :  definition of terms used within the SCONE technical documentation.    \u00a9  scontain.com , November 2017.  Questions or Suggestions?",
            "title": "Outline"
        },
        {
            "location": "/appsecurity/",
            "text": "Application-Oriented Security\n\n\nSCONE supports developers and \nservice providers\n (i.e., companies operating applications accessible via the Internet)\nto protect the confidentiality and integrity of their applications - even when running in environments that\ncannot be completely trusted. SCONE's focus is on supporting the development of programs running inside of containers like \nmicroservice-based applications\n as well as \ncloud-native applications\n. However, SCONE can protect most programs running on top of Linux.\n\n\n\n\nSCONE supports developers and service providers to ensure end-to-end encryption in the sense that \ndata is always encrypted\n, i.e., while being transmitted,\nwhile being at rest and even while being processed. The latter has only recently become possible with the help of a novel CPU extension by Intel (SGX). To reduce the required computing resources, a service provider can decide what to protect and what not to protect.  For example, a service that operates only on encrypted data might not need to be protected with SGX.\n\n\nOur general recommendation is, however, that developers should protect all parts of an application. The cost of computing resources have been dropping dramatically and hence, the reduction in cost might not be justified when compared with the potential costs - and also loss of reputation - by data breaches. SCONE supports horizontal scalability, i.e., throughput and latency can typically be controlled via the number of instances of a service.\n\n\nEase of Use\n\n\nSCONE supports strong application-oriented security with a workflow like Docker, i.e., SCONE supports \nDockerfiles\n as well as extended Docker \ncompose\n files. This simplifies the construction and operation of applications consisting of a set of containers. This fits, in particular, modern cloud-native applications consisting of microservices and each microservice runs either in a standard or a secure container.\n\n\nThe Docker Engine itself is not protected. The Docker Engine, like the operating system, never sees any plain text data. This facilitates that the Docker Engine or the Docker Swarm can be managed by a cloud provider. SCONE helps a service providers to ensure the confidentiality and integrity of the application data while the cloud provider will ensure the availability of the service. For example, with the help of Docker Swarm, failed containers will automatically be restarted on an appropriate host.\n\n\n\u00a9 \nscontain.com\n, November 2017. \nQuestions or Suggestions?",
            "title": "Application-oriented security"
        },
        {
            "location": "/appsecurity/#application-oriented-security",
            "text": "SCONE supports developers and  service providers  (i.e., companies operating applications accessible via the Internet)\nto protect the confidentiality and integrity of their applications - even when running in environments that\ncannot be completely trusted. SCONE's focus is on supporting the development of programs running inside of containers like  microservice-based applications  as well as  cloud-native applications . However, SCONE can protect most programs running on top of Linux.   SCONE supports developers and service providers to ensure end-to-end encryption in the sense that  data is always encrypted , i.e., while being transmitted,\nwhile being at rest and even while being processed. The latter has only recently become possible with the help of a novel CPU extension by Intel (SGX). To reduce the required computing resources, a service provider can decide what to protect and what not to protect.  For example, a service that operates only on encrypted data might not need to be protected with SGX.  Our general recommendation is, however, that developers should protect all parts of an application. The cost of computing resources have been dropping dramatically and hence, the reduction in cost might not be justified when compared with the potential costs - and also loss of reputation - by data breaches. SCONE supports horizontal scalability, i.e., throughput and latency can typically be controlled via the number of instances of a service.",
            "title": "Application-Oriented Security"
        },
        {
            "location": "/appsecurity/#ease-of-use",
            "text": "SCONE supports strong application-oriented security with a workflow like Docker, i.e., SCONE supports  Dockerfiles  as well as extended Docker  compose  files. This simplifies the construction and operation of applications consisting of a set of containers. This fits, in particular, modern cloud-native applications consisting of microservices and each microservice runs either in a standard or a secure container.  The Docker Engine itself is not protected. The Docker Engine, like the operating system, never sees any plain text data. This facilitates that the Docker Engine or the Docker Swarm can be managed by a cloud provider. SCONE helps a service providers to ensure the confidentiality and integrity of the application data while the cloud provider will ensure the availability of the service. For example, with the help of Docker Swarm, failed containers will automatically be restarted on an appropriate host.  \u00a9  scontain.com , November 2017.  Questions or Suggestions?",
            "title": "Ease of Use"
        },
        {
            "location": "/background/",
            "text": "SCONE Background\n\n\n\n\nCloud Security\n. The objective of SCONE is to help service providers to build secure applications for public, private or hybrid clouds. This means that the focus of SCONE is on application-oriented security and not on the security of the underlying cloud system. Of course, SCONE-based applications benefit from strong security properties of the underlying cloud because this minimizes, for example, the attack surface of SCONE-based applications and by providing higher availability. SCONE helps to ensure the security of an application, i.e., the application's integrity and confidentiality, even if the security of the underlying cloud or system software would be compromised. The security of applications is ensured with the help of Intel SGX enclaves. \n\n\n\n\n\n\n\n\nWorkflow\n. SCONE combines strong security with the ease of use of Docker. SCONE supports a workflow very similar to that of Docker. It supports the construction of applications consisting of multiple containers while ensuring end-to-end encryption between all application components in the sense that all network traffic, all files and even all computation is encrypted. A service provider can ensure the confidentiality and integrity of all application data. In particular, SCONE supports the construction of applications such that no higher privileged software like the operating system or the hypervisor, nor any system administrator with root access nor cold boot attacks can gain access to application data.\n\n\n\n\n\n\n\n\nSide Channel Attacks\n. Side Channel attacks on Intel SGX are the focus of a several recent research papers. First, mounting a successful side-channels is much more difficult than just dumping the memory of an existing application. In SCONE, we provide scheduling within enclaves which makes it more difficult for an attacker to determine which core is executing what function. Moreover, we are working on a compiler extension that will harden applications against side channel attacks. Until will release this extension, a pragmatic solution would be to run applications that might be suceptible to side channel attacks either on \nisolated hosts\n or on \nbaremetal clouds\n.\n\n\n\n\nProblem: Defender's Dilemma\n\n\nTraditionally, one ensures the security of an application by ensuring that the system software, i.e., the hypervisor, operating system and cloud software is trustworthy. This not only protects the integrity and confidentiality of the system data but also protects the security of the applications. A service provider running applications in the cloud must trust all system software and also all administrators who have root or physical access to these systems.\n\n\nA popular way to intrude into a system is to steal the credentials of a system administrator. With these root credentials, one gains access to all data being processed in this system as well as all keys that are kept in main memory or in some plain text files. If stealing credentials would be too difficult, an attacker will look for other ways to attack a system, like, exploiting known code vulnerabilities.\n\n\nFor an attacker, it might be sufficient to exploit a single vulnerability in either the application or the system software to violate the application security. The problem is that the defenders must protect against the exploitation of all code vulnerabilities that might exist in the source code. A service provider might not have access to all source code of the system software that the cloud provider uses to operate the cloud. Even if the source code were available, this will typically be too large to be inspected.\n\n\nTo show that this is a difficult problem, let's look at the number of lines of source code of common system software components. While lines of source code is not an ideal  indicator for the number of vulnerabilities, it gives some indication of the problem we are facing. Some security researchers state that given the current state of the art, only code with up to 10,000s of lines of code can be reasonably inspected. Just the system software itself contains millions of lines of code. This is orders of magnitudes more than we can reasonably expect to be able to inspect.\n\n\nSCONE runs on top of Linux - which contains millions of lines of code and is still growing in size with each release:\n\n\n\n\nLinux Lines of Code (StefanPohl, CC0, \noriginal\n}\n\n\nOpenStack is a popular open source software to manage clouds. OpenStack - despite being relatively young - has been growing dramatically over the years that it has already reached 5 million lines of code (including comments and blank lines):\n\n\n\n\nOpenStack Lines of Code (OpenHub \noriginal\n)\n\n\nTo manage containers, we need an engine like Docker. Docker is younger than OpenStack but has nevertheless reached already more than 180,000 lines of code:\n\n\n\n\nDocker Lines of Code (OpenHub \noriginal\n)\n\n\nCode complexity\n.There is no one-to-one correlation between lines of codes and bugs. Static analysis of open source code repositories indicates approximately 0.61 defects per 1,000 LOC. A recent analysis of Linux shows that, despite an increasing number of defects being fixed, there are always approximately 5,000 defects waiting to be fixed. Not all of these defects can, however, be exploited for security attacks. Another analysis found that approximately 500 security-relevant bugs were fixed in Linux over the past five years - bugs that had been in the kernel for five years before being discovered and fixed. Commercial code had a slightly higher defect density than open source projects. Hence, we need to expect vulnerabilities in commercial software too.\n\n\nSCONE Approach\n\n\nThe approach of SCONE is to partition the code and to place essential components of an application into separate enclaves. Practically, it is quite difficult to split an existing code base of a single process into one component that runs inside an enclave and a component that runs outside of an enclave. However, many modern applications - like cloud-native applications - are already partitioned in several components running in separate address spaces. These components are typically called microservices. This partitioning facilitates a more intelligent scaling of services as well as a scaling of the development team.\n\n\nA large application might consist of a variety of microservices. Not all microservices of an application need to run inside enclaves to protect the application\u2019s integrity and confidentiality. For example, some services might only process encrypted data, like encrypted log data, and do not need to run inside enclaves.  Also, the resource manager does not need to run in an enclave either. However, we recommend that each microservice that has the credential to send requests to at least one microservice running inside an enclave, should itself also run inside of an enclave to restrict the access to enclaved microservices.\n\n\nCurrent SGX-capable CPUs have a limited EPC (Extended Page Cache) size. If the working set of a microservice does not fit inside the EPC, overheads can become high. The usage of microservices supports horizontal scalability. This helps to cope with limited EPC (extended page cache) by spreading secure microservices across different hosts.\n\n\n\u00a9 \nscontain.com\n, November 2017. \nQuestions or Suggestions?",
            "title": "SCONE Background"
        },
        {
            "location": "/background/#scone-background",
            "text": "Cloud Security . The objective of SCONE is to help service providers to build secure applications for public, private or hybrid clouds. This means that the focus of SCONE is on application-oriented security and not on the security of the underlying cloud system. Of course, SCONE-based applications benefit from strong security properties of the underlying cloud because this minimizes, for example, the attack surface of SCONE-based applications and by providing higher availability. SCONE helps to ensure the security of an application, i.e., the application's integrity and confidentiality, even if the security of the underlying cloud or system software would be compromised. The security of applications is ensured with the help of Intel SGX enclaves.      Workflow . SCONE combines strong security with the ease of use of Docker. SCONE supports a workflow very similar to that of Docker. It supports the construction of applications consisting of multiple containers while ensuring end-to-end encryption between all application components in the sense that all network traffic, all files and even all computation is encrypted. A service provider can ensure the confidentiality and integrity of all application data. In particular, SCONE supports the construction of applications such that no higher privileged software like the operating system or the hypervisor, nor any system administrator with root access nor cold boot attacks can gain access to application data.     Side Channel Attacks . Side Channel attacks on Intel SGX are the focus of a several recent research papers. First, mounting a successful side-channels is much more difficult than just dumping the memory of an existing application. In SCONE, we provide scheduling within enclaves which makes it more difficult for an attacker to determine which core is executing what function. Moreover, we are working on a compiler extension that will harden applications against side channel attacks. Until will release this extension, a pragmatic solution would be to run applications that might be suceptible to side channel attacks either on  isolated hosts  or on  baremetal clouds .",
            "title": "SCONE Background"
        },
        {
            "location": "/background/#problem-defenders-dilemma",
            "text": "Traditionally, one ensures the security of an application by ensuring that the system software, i.e., the hypervisor, operating system and cloud software is trustworthy. This not only protects the integrity and confidentiality of the system data but also protects the security of the applications. A service provider running applications in the cloud must trust all system software and also all administrators who have root or physical access to these systems.  A popular way to intrude into a system is to steal the credentials of a system administrator. With these root credentials, one gains access to all data being processed in this system as well as all keys that are kept in main memory or in some plain text files. If stealing credentials would be too difficult, an attacker will look for other ways to attack a system, like, exploiting known code vulnerabilities.  For an attacker, it might be sufficient to exploit a single vulnerability in either the application or the system software to violate the application security. The problem is that the defenders must protect against the exploitation of all code vulnerabilities that might exist in the source code. A service provider might not have access to all source code of the system software that the cloud provider uses to operate the cloud. Even if the source code were available, this will typically be too large to be inspected.  To show that this is a difficult problem, let's look at the number of lines of source code of common system software components. While lines of source code is not an ideal  indicator for the number of vulnerabilities, it gives some indication of the problem we are facing. Some security researchers state that given the current state of the art, only code with up to 10,000s of lines of code can be reasonably inspected. Just the system software itself contains millions of lines of code. This is orders of magnitudes more than we can reasonably expect to be able to inspect.  SCONE runs on top of Linux - which contains millions of lines of code and is still growing in size with each release:   Linux Lines of Code (StefanPohl, CC0,  original }  OpenStack is a popular open source software to manage clouds. OpenStack - despite being relatively young - has been growing dramatically over the years that it has already reached 5 million lines of code (including comments and blank lines):   OpenStack Lines of Code (OpenHub  original )  To manage containers, we need an engine like Docker. Docker is younger than OpenStack but has nevertheless reached already more than 180,000 lines of code:   Docker Lines of Code (OpenHub  original )  Code complexity .There is no one-to-one correlation between lines of codes and bugs. Static analysis of open source code repositories indicates approximately 0.61 defects per 1,000 LOC. A recent analysis of Linux shows that, despite an increasing number of defects being fixed, there are always approximately 5,000 defects waiting to be fixed. Not all of these defects can, however, be exploited for security attacks. Another analysis found that approximately 500 security-relevant bugs were fixed in Linux over the past five years - bugs that had been in the kernel for five years before being discovered and fixed. Commercial code had a slightly higher defect density than open source projects. Hence, we need to expect vulnerabilities in commercial software too.",
            "title": "Problem: Defender's Dilemma"
        },
        {
            "location": "/background/#scone-approach",
            "text": "The approach of SCONE is to partition the code and to place essential components of an application into separate enclaves. Practically, it is quite difficult to split an existing code base of a single process into one component that runs inside an enclave and a component that runs outside of an enclave. However, many modern applications - like cloud-native applications - are already partitioned in several components running in separate address spaces. These components are typically called microservices. This partitioning facilitates a more intelligent scaling of services as well as a scaling of the development team.  A large application might consist of a variety of microservices. Not all microservices of an application need to run inside enclaves to protect the application\u2019s integrity and confidentiality. For example, some services might only process encrypted data, like encrypted log data, and do not need to run inside enclaves.  Also, the resource manager does not need to run in an enclave either. However, we recommend that each microservice that has the credential to send requests to at least one microservice running inside an enclave, should itself also run inside of an enclave to restrict the access to enclaved microservices.  Current SGX-capable CPUs have a limited EPC (Extended Page Cache) size. If the working set of a microservice does not fit inside the EPC, overheads can become high. The usage of microservices supports horizontal scalability. This helps to cope with limited EPC (extended page cache) by spreading secure microservices across different hosts.  \u00a9  scontain.com , November 2017.  Questions or Suggestions?",
            "title": "SCONE Approach"
        },
        {
            "location": "/SCONE_CLI/",
            "text": "SCONE CLI\n\n\nWe maintain a single unified command line interface (CLI) \nscone\n that helps to to start and stop secure containers as well as secure applications. \nscone\n also provides functionality to install and monitor SCONE hosts.\n\n\nThe \nscone\n command is structured similar as the \ndocker\n CLI or the \ninfinit\n CLI:\n\n\n\n\nOne needs to specify an \nobject\n (like \nhost\n) and a \ncommand\n (like \ninstall\n) and some options. For some commands, some of the options are actually not optional but mandatory.\n\n\nSee below how to install scone on Ubuntu. Instead of installing \nscone\n in a VM or a host, you could just start it in a container. Assuming\nthat you have docker installed, you try the following examples by running the following container:\n\n\n> docker run -it  sconecuratedimages/sconecli\n\n\n\n\nTo simplify ssh setup, you might want map your ssh configuration into the container. Since you probably have a different user ID inside and outside the container, you might want to copy the original ssh configuration:\n\n\n> docker run -it -v $HOME/.ssh:/root/.xssh  sconecuratedimages/sconecli\n\n\n\n\nInside the container, copy the external ssh configuration:\n\n\n$ cp -r $HOME/.xssh/* $HOME/.ssh/\n\n\n\n\nNote\n\n\nThe \nscone\n utility requires \nssh\n connectivity to your SGX hosts. See \nSCONE host setup\n for some more\ninformation on how to set up ssh.\n\n\nHelp\n\n\nscone\n has a built in help. To get a list of all \nobjects\n, just type:\n\n\n$ scone --help\n\n\n\n\nTo get a list of all \ncommands\n for a given \nobject\n (like host), execute:\n\n\n$ scone host --help\n\n\n\n\nTo get a list of all options for a given \nobject\n and \ncommand\n (e.g., host install) and some examples, just execute:\n\n\n$ scone host install --help\n\n\n\n\nbash auto-completion\n\n\nIf you are using \nbash\n as your shell, \nscone\n supports auto-completion. This means that instead you can use the \nTAB\n key to see the options. For example,\n\n\n$ scone <TAB>\n\n\n\n\nwill show all available objects. If you have already specified an object, auto-completion helps you to list all commands:\n\n\n$ scone host <TAB>\n\n\n\n\nIf you also specified an command, it will provide you with a list of options (that you have not specified yet):\n\n\n$ scone host install <TAB>\n\n\n\n\nOf course, it also supports auto-completion:\n\n\n$ scone host install -n<TAB>\n\n\n\n\nwill result in \n\n\n$ scone host install -name\n\n\n\n\nInstallation of scone\n\n\nOn Ubuntu platform, you can just execute\n\n\n> curl -L https://sconecontainers.github.io/install.sh | bash\n\n\n\n\nYou could alternatively first download the above installation script and store it as file \ninstall.sh\n, inspect it and then run it \n./install.sh\"\n.\n\n\nThe \nscone\n command is located in directory \n/opt/scone/bin/\n. You might want this directory to add this to you \nPATH\n\n\n> PATH=/opt/scone/bin/:$PATH\n\n\n\n\nFor convenience, you might want to add above statement to your \n.bashrc\n script (- in case you are using bash).\n\n\nManual installation of SCONE Deb Package\n\n\nYou can execute the following commands (these are the same as in the above installation script):\n\n\nKEYNAME=\"96B9BADB\"\nREPO=\"deb https://sconecontainers.github.io/SCONE ./\"\n\nsudo apt-get update\nsudo apt-get install -y linux-image-extra-$(uname -r) linux-image-extra-virtual\n\nsudo sudo apt-get install -y apt-transport-https ca-certificates\nsudo apt-key adv \\\n  --keyserver hkp://ha.pool.sks-keyservers.net:80 \\\n  --recv-keys $KEYNAME\n\necho $REPO | sudo tee /etc/apt/sources.list.d/scone.list\n\nsudo apt-get clean\nsudo apt-get update\n\napt-cache policy scone\n\nsudo apt-get install -y scone\n\n\n\n\nThe \nscone\n utility will be installed at \n/opt/scone/bin\n. Hence, it makes sense to add this path to your \nPATH\n:\n\n\n> export PATH=/opt/scone/bin:$PATH\n\n\n\n\nYou can then execute some scone commands to see if the installation was successful:\n\n\n> scone --version\n> scone --help\n> scone volume --help\n\n\n\n\nScreencast\n\n\n\n\n\u00a9 \nscontain.com\n, November 2017. \nQuestions or Suggestions?",
            "title": "SCONE CLI"
        },
        {
            "location": "/SCONE_CLI/#scone-cli",
            "text": "We maintain a single unified command line interface (CLI)  scone  that helps to to start and stop secure containers as well as secure applications.  scone  also provides functionality to install and monitor SCONE hosts.  The  scone  command is structured similar as the  docker  CLI or the  infinit  CLI:   One needs to specify an  object  (like  host ) and a  command  (like  install ) and some options. For some commands, some of the options are actually not optional but mandatory.  See below how to install scone on Ubuntu. Instead of installing  scone  in a VM or a host, you could just start it in a container. Assuming\nthat you have docker installed, you try the following examples by running the following container:  > docker run -it  sconecuratedimages/sconecli  To simplify ssh setup, you might want map your ssh configuration into the container. Since you probably have a different user ID inside and outside the container, you might want to copy the original ssh configuration:  > docker run -it -v $HOME/.ssh:/root/.xssh  sconecuratedimages/sconecli  Inside the container, copy the external ssh configuration:  $ cp -r $HOME/.xssh/* $HOME/.ssh/",
            "title": "SCONE CLI"
        },
        {
            "location": "/SCONE_CLI/#note",
            "text": "The  scone  utility requires  ssh  connectivity to your SGX hosts. See  SCONE host setup  for some more\ninformation on how to set up ssh.",
            "title": "Note"
        },
        {
            "location": "/SCONE_CLI/#help",
            "text": "scone  has a built in help. To get a list of all  objects , just type:  $ scone --help  To get a list of all  commands  for a given  object  (like host), execute:  $ scone host --help  To get a list of all options for a given  object  and  command  (e.g., host install) and some examples, just execute:  $ scone host install --help",
            "title": "Help"
        },
        {
            "location": "/SCONE_CLI/#bash-auto-completion",
            "text": "If you are using  bash  as your shell,  scone  supports auto-completion. This means that instead you can use the  TAB  key to see the options. For example,  $ scone <TAB>  will show all available objects. If you have already specified an object, auto-completion helps you to list all commands:  $ scone host <TAB>  If you also specified an command, it will provide you with a list of options (that you have not specified yet):  $ scone host install <TAB>  Of course, it also supports auto-completion:  $ scone host install -n<TAB>  will result in   $ scone host install -name",
            "title": "bash auto-completion"
        },
        {
            "location": "/SCONE_CLI/#installation-of-scone",
            "text": "On Ubuntu platform, you can just execute  > curl -L https://sconecontainers.github.io/install.sh | bash  You could alternatively first download the above installation script and store it as file  install.sh , inspect it and then run it  ./install.sh\" .  The  scone  command is located in directory  /opt/scone/bin/ . You might want this directory to add this to you  PATH  > PATH=/opt/scone/bin/:$PATH  For convenience, you might want to add above statement to your  .bashrc  script (- in case you are using bash).",
            "title": "Installation of scone"
        },
        {
            "location": "/SCONE_CLI/#manual-installation-of-scone-deb-package",
            "text": "You can execute the following commands (these are the same as in the above installation script):  KEYNAME=\"96B9BADB\"\nREPO=\"deb https://sconecontainers.github.io/SCONE ./\"\n\nsudo apt-get update\nsudo apt-get install -y linux-image-extra-$(uname -r) linux-image-extra-virtual\n\nsudo sudo apt-get install -y apt-transport-https ca-certificates\nsudo apt-key adv \\\n  --keyserver hkp://ha.pool.sks-keyservers.net:80 \\\n  --recv-keys $KEYNAME\n\necho $REPO | sudo tee /etc/apt/sources.list.d/scone.list\n\nsudo apt-get clean\nsudo apt-get update\n\napt-cache policy scone\n\nsudo apt-get install -y scone  The  scone  utility will be installed at  /opt/scone/bin . Hence, it makes sense to add this path to your  PATH :  > export PATH=/opt/scone/bin:$PATH  You can then execute some scone commands to see if the installation was successful:  > scone --version\n> scone --help\n> scone volume --help",
            "title": "Manual installation of SCONE Deb Package"
        },
        {
            "location": "/SCONE_CLI/#screencast",
            "text": "\u00a9  scontain.com , November 2017.  Questions or Suggestions?",
            "title": "Screencast"
        },
        {
            "location": "/SCONE_HOST_SETUP/",
            "text": "SCONE: Host Installation Guide\n\n\nThis page describes how\n\n\n\n\n\n\nto set up a host such that it can run SCONE secure containers, i.e., containers in which processes run inside of SGX enclaves, and\n\n\n\n\n\n\nwe remind you of how to set up your \nssh\n configuration to be able to use your scone CLI.\n\n\n\n\n\n\nDuring this setup, we\n\n\n\n\n\n\ninstall a \npatched\n Intel SGX driver - required for better monitoring support,\n\n\n\n\n\n\ninstall a \npatched\n docker engine - to ensure that all containers have access to SGX, and\n\n\n\n\n\n\nstart or join a docker swarm - if requested by command line options.\n\n\n\n\n\n\nPrerequisite\n:\n\n\n\n\n\n\nWe assume that you have set up the \nscone\n \ncommand line interface\n.\nThe \nscone\n CLI can run on your developer machine, a virtual machine or inside a container.\nThe easiest way to get started is to run \nscone\n in a Docker container.\nNo matter where \nscone\n is running, it requires that \nssh\n be properly installed.\n\n\n\n\n\n\nWe recommend to install Ubuntu 17.04 or Ubuntu 17.10 on the Swarm machines. For older versions of Ubuntu, some minor manual fixing might be needed during installation of the SGX drivers and the Docker engine. The screencast below shows some of the issues one faces on older Ubuntu versions. The secure containers are mostly based on Ubuntu or Alpine Linux (smaller image size).\n\n\n\n\n\n\nNOTE\n This host setup will enable the execution of \nSCONE secure containers\n on SGX-enabled machines. These containers can run one or more \nsecure programs\n, i.e., programs that are executed inside SGX enclaves.   The secure programs of secure containers can be statically-linked or dynamically-linked (see \nSCONE SGX Toolchain\n). The host itself runs statically-linked secure programs only - this is to avoid failures do to library versioning issues.\n\n\nSSH\n\n\nThe \nscone\n utility executes commands via \nssh\n on the SGX-capable machine that should be installed.\n\n\n\n\nSince we potentially execute many \nssh\n commands, you need to configure \nssh\n such that\n\n\n\n\n\n\nyou can log into the SGX machines \nwithout\n  having to type a password,\n\n\n\n\n\n\nyou can use the basename of your SGX machines to log in , and\n\n\n\n\n\n\nssh is permitted to reuse connections to reduce the execution time of the \nscone\n commands.\n\n\n\n\n\n\nPasswordless ssh\n\n\nTo set up passwordless ssh authentication, you need to ensure that you have a pair of authentication keys. If there exists no public key \n$HOME/.ssh/id_rsa.pub\n (often the case if you use a container), you can generate a new pair by executing:\n\n\n> ssh-keygen -b 4096 -t rsa\n\n\n\n\nAppend the generated public key \n$HOME/.ssh/id_rsa.pub\n to file \n$HOME/.ssh/authorized_keys\n on the SGX hosts for which you to be able to log in without a password.\n\n\nAdditionally, you need to start a ssh-agent (on your developer machine):\n\n\n$ SA=`ssh-agent`\n$ eval \"$SA\"\n\n\n\n\nand add your authentication keys by executing:\n\n\n$ ssh-add\n\n\n\n\nHost alias\n\n\nTo reduce your typing overhead, you can use the basename of a host - if you configure ssh appropriately. For example, instead of typing \nnode2.my.very.long.domain.com\n, you could configure  \nssh\n such that \nnode2\n refers to \nnode2.my.very.long.domain.com\n.\n\n\nAs a caveat, this basename must be sufficient for other hosts in the \nsame swarm\n to reach \nnode2\n. In the above figure, \nmanager\n must be able to resolve \nnode2\n to the IP address of \nnode2.my.very.long.domain.com\n.\n\n\nTo add an alias for \nnode2.my.very.long.domain.com\n, you could add the following lines to your \nssh config\n (stored in \n$HOME/.ssh/config\n):\n\n\nHost node2\n     HostName node2.my.very.long.domain.com\n     Port 22\n     User scone\n     IdentityFile ~/.ssh/id_rsa\n\n\n\n\nssh connection reuse\n\n\nTo be able to reuse a ssh connection, you must configure ssh appropriately. You should set \nControlMaster\n to \nauto\n and you have to specify a control path via option \nControlPath\n. You can define a generic path like \n~/.ssh/ssh_mux_%h_%p_%r\n - this can be the same for all hosts. For example, for some host \nalice\n you might add the following lines to \n$HOME/.ssh/config\n:\n\n\nHost alice\n        ControlMaster auto\n        ControlPath ~/.ssh/ssh_mux_%h_%p_%r\n        user ubuntu\n        port 10101\n        hostname sshproxy.cloudprovider.com\n\n\n\n\nInstallation of a single host\n\n\nAfter you set up the \nscone CLI\n and your passwordless ssh works, you can install the scone related software on a new host, say, \nalice\n as follows:\n\n\n$ scone host install --name alice\n\n\n\n\nTo verify that a host is properly installed for SCONE and contains the newest patched Docker engine and SGX driver, just execute:\n\n\n$ scone host check --name alice\n\n\n\n\nThis command will issue an error unless the newest versions of the patched Docker engine and the patched SGX driver is installed.\n\n\nInstallation of a swarm\n\n\nFor a set of hosts to form a (Docker) swarm, you need to decide which hosts should be managers and which should be just members of the swarm. Say, you decided that \nalice\n and \nbob\n should be managers but \ncaroline\n a non-manager, execute the following:\n\n\n$ scone host install --name alice --as-manager\n$ scone host install --name bob --as-manager --join alice\n$ scone host install --name caroline  --join alice\n\n\n\n\nNote that the hosts must be able to communicate with each other (i.e., not partitioned through firewalls). Docker recommends/expects  that they will be in the same local area network.\n\n\nChecking your Installation\n\n\nTo test the installation, one can run a simple hello-world container:\n\n\n> sudo docker run hello-world\n\n\n\n\nBackground Information\n\n\nPatched Docker Engine (Moby)\n\n\nFor an container to be able to use SGX, it has to have access to a device (/dev/isgx). This device permits the container to talk to the SGX driver. This driver is needed, in particular, to create SGX enclaves. \n\n\nSome docker commands (like \ndocker run\n) support an option --device (i.e., \n--device /dev/isgx\n) which allows us to give a container access to the SGX device. We need to point out that some docker commands (like \ndocker build\n) do, however, not yet support the device option. Therefore, we maintain and install a slightly patched docker engine (i.e., a variant of moby): this engine ensures that each container has access to the SGX device (/dev/isgx).  With the help of this patched engine, we can use Dockerfiles to generate container images (see this \nTutorial\n).\n\n\nRight now we provide a patched version of the currently active branch of Moby (a.k.a., the Docker engine): 17.05.0-ce, build 89658be (November 11, 2017).\n\n\nPatched SGX Driver\n\n\nWe also maintain a patched version of the SGX driver. This version adds some additional monitoring like the number of \navailable\n and \nfree\n EPC (Extended Page Cache) pages.\n\n\nRight now, we provide a patched version of the latest Intel SGX driver (November 11, 2017).\n\n\nScreencast\n\n\nThis screencast shows the installation of three machines SGX-capable hosts. In this screencast, we show the installation on machines that run older versions of Ubuntu (sgx2 = 14.04, sgx3 = 14.04 with custom kernel, and sgx4 = 16.04). In this case, we will see some warnings since \nscone host\n depends on \nsystemd\n to start the swarm. In case \nsystemd\n is not available, \nscone host\n will still be able to install the patched SGX driver and the patched Docker engine.\n\n\nAnother issue that one sometimes faces is that an older SGX drivers is already installed but cannot be offloaded and replaced by the patched driver by \nscone host\n. The reason for that is that typically that some process is still using the \n/dev/isgx\n device. This needs to be manually fixed by stopping the process or by rebooting the machine. Alternatively, one can use the existing SGX driver. However, the monitoring of the used EPC pages will not be provided in this case.\n\n\n\n\n\u00a9 \nscontain.com\n, November 2017. \nQuestions or Suggestions?",
            "title": "SCONE Host Setup"
        },
        {
            "location": "/SCONE_HOST_SETUP/#scone-host-installation-guide",
            "text": "This page describes how    to set up a host such that it can run SCONE secure containers, i.e., containers in which processes run inside of SGX enclaves, and    we remind you of how to set up your  ssh  configuration to be able to use your scone CLI.    During this setup, we    install a  patched  Intel SGX driver - required for better monitoring support,    install a  patched  docker engine - to ensure that all containers have access to SGX, and    start or join a docker swarm - if requested by command line options.    Prerequisite :    We assume that you have set up the  scone   command line interface .\nThe  scone  CLI can run on your developer machine, a virtual machine or inside a container.\nThe easiest way to get started is to run  scone  in a Docker container.\nNo matter where  scone  is running, it requires that  ssh  be properly installed.    We recommend to install Ubuntu 17.04 or Ubuntu 17.10 on the Swarm machines. For older versions of Ubuntu, some minor manual fixing might be needed during installation of the SGX drivers and the Docker engine. The screencast below shows some of the issues one faces on older Ubuntu versions. The secure containers are mostly based on Ubuntu or Alpine Linux (smaller image size).    NOTE  This host setup will enable the execution of  SCONE secure containers  on SGX-enabled machines. These containers can run one or more  secure programs , i.e., programs that are executed inside SGX enclaves.   The secure programs of secure containers can be statically-linked or dynamically-linked (see  SCONE SGX Toolchain ). The host itself runs statically-linked secure programs only - this is to avoid failures do to library versioning issues.",
            "title": "SCONE: Host Installation Guide"
        },
        {
            "location": "/SCONE_HOST_SETUP/#ssh",
            "text": "The  scone  utility executes commands via  ssh  on the SGX-capable machine that should be installed.   Since we potentially execute many  ssh  commands, you need to configure  ssh  such that    you can log into the SGX machines  without   having to type a password,    you can use the basename of your SGX machines to log in , and    ssh is permitted to reuse connections to reduce the execution time of the  scone  commands.    Passwordless ssh  To set up passwordless ssh authentication, you need to ensure that you have a pair of authentication keys. If there exists no public key  $HOME/.ssh/id_rsa.pub  (often the case if you use a container), you can generate a new pair by executing:  > ssh-keygen -b 4096 -t rsa  Append the generated public key  $HOME/.ssh/id_rsa.pub  to file  $HOME/.ssh/authorized_keys  on the SGX hosts for which you to be able to log in without a password.  Additionally, you need to start a ssh-agent (on your developer machine):  $ SA=`ssh-agent`\n$ eval \"$SA\"  and add your authentication keys by executing:  $ ssh-add  Host alias  To reduce your typing overhead, you can use the basename of a host - if you configure ssh appropriately. For example, instead of typing  node2.my.very.long.domain.com , you could configure   ssh  such that  node2  refers to  node2.my.very.long.domain.com .  As a caveat, this basename must be sufficient for other hosts in the  same swarm  to reach  node2 . In the above figure,  manager  must be able to resolve  node2  to the IP address of  node2.my.very.long.domain.com .  To add an alias for  node2.my.very.long.domain.com , you could add the following lines to your  ssh config  (stored in  $HOME/.ssh/config ):  Host node2\n     HostName node2.my.very.long.domain.com\n     Port 22\n     User scone\n     IdentityFile ~/.ssh/id_rsa  ssh connection reuse  To be able to reuse a ssh connection, you must configure ssh appropriately. You should set  ControlMaster  to  auto  and you have to specify a control path via option  ControlPath . You can define a generic path like  ~/.ssh/ssh_mux_%h_%p_%r  - this can be the same for all hosts. For example, for some host  alice  you might add the following lines to  $HOME/.ssh/config :  Host alice\n        ControlMaster auto\n        ControlPath ~/.ssh/ssh_mux_%h_%p_%r\n        user ubuntu\n        port 10101\n        hostname sshproxy.cloudprovider.com",
            "title": "SSH"
        },
        {
            "location": "/SCONE_HOST_SETUP/#installation-of-a-single-host",
            "text": "After you set up the  scone CLI  and your passwordless ssh works, you can install the scone related software on a new host, say,  alice  as follows:  $ scone host install --name alice  To verify that a host is properly installed for SCONE and contains the newest patched Docker engine and SGX driver, just execute:  $ scone host check --name alice  This command will issue an error unless the newest versions of the patched Docker engine and the patched SGX driver is installed.",
            "title": "Installation of a single host"
        },
        {
            "location": "/SCONE_HOST_SETUP/#installation-of-a-swarm",
            "text": "For a set of hosts to form a (Docker) swarm, you need to decide which hosts should be managers and which should be just members of the swarm. Say, you decided that  alice  and  bob  should be managers but  caroline  a non-manager, execute the following:  $ scone host install --name alice --as-manager\n$ scone host install --name bob --as-manager --join alice\n$ scone host install --name caroline  --join alice  Note that the hosts must be able to communicate with each other (i.e., not partitioned through firewalls). Docker recommends/expects  that they will be in the same local area network.  Checking your Installation  To test the installation, one can run a simple hello-world container:  > sudo docker run hello-world",
            "title": "Installation of a swarm"
        },
        {
            "location": "/SCONE_HOST_SETUP/#background-information",
            "text": "Patched Docker Engine (Moby)  For an container to be able to use SGX, it has to have access to a device (/dev/isgx). This device permits the container to talk to the SGX driver. This driver is needed, in particular, to create SGX enclaves.   Some docker commands (like  docker run ) support an option --device (i.e.,  --device /dev/isgx ) which allows us to give a container access to the SGX device. We need to point out that some docker commands (like  docker build ) do, however, not yet support the device option. Therefore, we maintain and install a slightly patched docker engine (i.e., a variant of moby): this engine ensures that each container has access to the SGX device (/dev/isgx).  With the help of this patched engine, we can use Dockerfiles to generate container images (see this  Tutorial ).  Right now we provide a patched version of the currently active branch of Moby (a.k.a., the Docker engine): 17.05.0-ce, build 89658be (November 11, 2017).  Patched SGX Driver  We also maintain a patched version of the SGX driver. This version adds some additional monitoring like the number of  available  and  free  EPC (Extended Page Cache) pages.  Right now, we provide a patched version of the latest Intel SGX driver (November 11, 2017).  Screencast  This screencast shows the installation of three machines SGX-capable hosts. In this screencast, we show the installation on machines that run older versions of Ubuntu (sgx2 = 14.04, sgx3 = 14.04 with custom kernel, and sgx4 = 16.04). In this case, we will see some warnings since  scone host  depends on  systemd  to start the swarm. In case  systemd  is not available,  scone host  will still be able to install the patched SGX driver and the patched Docker engine.  Another issue that one sometimes faces is that an older SGX drivers is already installed but cannot be offloaded and replaced by the patched driver by  scone host . The reason for that is that typically that some process is still using the  /dev/isgx  device. This needs to be manually fixed by stopping the process or by rebooting the machine. Alternatively, one can use the existing SGX driver. However, the monitoring of the used EPC pages will not be provided in this case.   \u00a9  scontain.com , November 2017.  Questions or Suggestions?",
            "title": "Background Information"
        },
        {
            "location": "/SCONE_toolchain/",
            "text": "SCONE SGX Toolchain\n\n\nSCONE comes with compiler support for popular languages: C, C++, GO, Rust as well as Fortran.\nThe objective of these (cross-) compilers are to compile applications - generally without source code changes - such that they can run inside of SGX enclaves.\n\n\nTo simplify the use of these (cross-) compilers, SCONE maintains curated container image that includes these cross-compilers.\n\n\nCompiler variants\n\n\nDepending on if you want to generate a \ndynamically-linked\n or a \nstatically-linked\n binary, you\ncan use a standard compiler (dynamic) or you need to use a cross compiler (static). The compiler can run on any system, i.e., does not require SGX to run. To generate a statically-linked binary, you need to run on a SGX-enabled machine (for now).\n\n\nNote\n Independently, if you use a dynamic or static linking, the hash of an enclave (MRENCLAVE) will encompass the whole code, i.e., includes all libraries. Any updates of a library on your host might prevent the execution of a SCONE binary because of a wrong MRENCLAVE. Hence, we recommend to use only statically-linked programs on the host. In containers, which have a more controlled environment, we support both statically as well as dynamically linked binaries. The main advantage of dynamic linking is that for many programs we do not change the build process when moved to SCONE.\n\n\nNote\n also that SCONE supports the loading of dynamic libraries after a program has already started inside of an enclave. This feature is required by modern languages like Java. Enabling general loading of dynamic library introduces the risk that one could load malicious code inside of an enclave. Hence, we switch this feature off by default. For debugging programs, you can enable this feature via an environment variable (\nexport SCONE_ALLOW_DLOPEN=2).\n For production enclaves, you will need to protect the integrity of the shared libraries with the help of the \nSCONE file shield\n.\n\n\nDynamically-Linked Binaries\n\n\nThe easiest to get started is to compile your programs such that\n\n\n\n\n\n\nthe generated code is position independent (\n-fPIC\n),\n\n\n\n\n\n\nthe thread local storage model is global-dynamic (\n-ftls-model=global-dynamic\n),\n\n\n\n\n\n\nyour binary is dynamically linked (i.e., do not use \n-static\n), and\n\n\n\n\n\n\nlink against musl as your libc (i.e., not glibc or any other libc).\n\n\n\n\n\n\n\n\nWhen a program is started, SCONE uses its own dynamic link loader to replace libc by a SCONE libc. The SCONE dynamic linker will load the program inside a new SGX enclave and SCONE libc will enable programs to run inside the SGX enclaves, e.g., execute system calls and protect them from attacks via shields like the \nfile system shield\n.\n\n\nTo simplify the compiling of your programs for scone, we make available a docker image \nsconecuratedimages/muslgcc\n which includes \ngcc\n and \ng++\n support. The options will by default be set as shown above. You need, however, to make sure that your Makefiles will not overwrite these options.\n\n\nStatically-Linked Binaries\n\n\nFor statically linked binaries, we make available a (private) docker image \nsconecuratedimages/crosscompilers:scone\n which can produce statically linked binaries. In the statically linked binaries, we replace the interface to the operating system (i.e., libc) by a variant that enables programs to run inside Intel SGX enclaves.\n\n\n\n\nNote that a statically linked binaries might look like a dynamically-linked binary. For example, if you look at a statically-linked program \nweb-srv-go\n, you will still see dynamic dependencies:\n\n\n$ ldd web-srv-go \n    linux-vdso.so.1 =>  (0x00007ffe423fd000)\n    libpthread.so.0 => /lib/x86_64-linux-gnu/libpthread.so.0 (0x00007effa344f000)\n    libc.so.6 => /lib/x86_64-linux-gnu/libc.so.6 (0x00007effa3085000)\n    /lib64/ld-linux-x86-64.so.2 (0x00007effa366c000)\n\n\n\n\nThe reason for that is that the statically linked binary that runs inside of an enclave is wrapped in a dynamically linked \nloader\n program. The loader program creates the enclave, moves the program code inside the enclave and starts threads that will enter the enclave. The code that is moved inside the enclave is, however, statically linked.\n\n\nUsing the cross compiler container\n\n\nHow to use the compiler:\n\n\n\n\n\n\nuse this as a base image and build your programs inside of a container we a  \nDockerfile\n), or\n\n\n\n\n\n\nmap volumes such that the compiler can compile files living outside the container (see \nSCONE Tutorial\n).\n\n\n\n\n\n\nFor an example how to use the crosscompilers, see how to compile a programs written in \nGO\n.\n\n\nExample\n\n\nNote\n  on some systems you will need to run \ndocker\n with root permissions, i.e., in this case you should prefix a \n\n\n> docker ...\n\n\n\n\ncommand with \nsudo\n, i.e., you execute \n\n\n> sudo docker ...\n\n\n\n\nOne can run the above compiler inside of a container while the compiled files reside outside the container. Say, your code is\nin file \nmyapp.c\n in your current directory (\n$PWD\n). You can compile this code as follows:\n\n\n> docker run --rm  -v \"$PWD\":/usr/src/myapp -w /usr/src/myapp sconecuratedimages/muslgcc gcc myapp.c\n\n\n\n\nThis call will generate a binary \na.out\n in your working directory. This binary is dynamically linked against musl:\n\n\n> ldd a.out \n    /lib/ld-musl-x86_64.so.1 (0x7fb0379f9000)\n    libc.musl-x86_64.so.1 => /lib/ld-musl-x86_64.so.1 (0x7fb0379f9000)\n\n\n\n\nThis binary can run natively only if you have musl installed at the correct position in your development machine (and your development machine runs Linux). Alternatively, you can run the binary in a container:\n\n\n> docker run --rm  -v \"$PWD\":/usr/src/myapp -w /usr/src/myapp sconecuratedimages/muslgcc ./a.out\n\n\n\n\nTo run this inside of SGX enclaves with SCONE, you need access to the SCONE runtime systems. For more details, see our \nhello world\n in Section \nSCONE Tutorial\n. This is not very convenient and hence, we provide a) a simpler version with the help of \nDockerfiles\n. \n\n\nIn most cases, you might just set to use one of our crosscompilers in your \nconfigure\n script or \nMakefile\n. A simple way is to use the Docker image \nsconecuratedimages/crosscompilers:scone\n as a base image and then clone your code inside the container and set one or more of our compilers (\nscone-gcc, scone-g++, scone-gccgo, scone-gfortran, and scone-rustc\n) to be used in your build. For Rust, we support also our variant of \ncargo\n which is \nscone-cargo\n.\n\n\nDebugger support\n\n\nWe also support \ngdb\n to debug applications running inside of enclaves. To get started, we recommend that you first ensure that your program runs natively linked against musl. Most programs will do - after all, the Alpine Linux distribution is completely based on musl. The debugger is available in image \nsconecuratedimages/crosscompilers:scone\n as \nscone-gdb\n.\n\n\nFor example on how to use the debugger, see how to debug a program written in  \nGO\n.\n\n\n\u00a9 \nscontain.com\n, November 2017. \nQuestions or Suggestions?",
            "title": "SCONE SGX toolchain"
        },
        {
            "location": "/SCONE_toolchain/#scone-sgx-toolchain",
            "text": "SCONE comes with compiler support for popular languages: C, C++, GO, Rust as well as Fortran.\nThe objective of these (cross-) compilers are to compile applications - generally without source code changes - such that they can run inside of SGX enclaves.  To simplify the use of these (cross-) compilers, SCONE maintains curated container image that includes these cross-compilers.",
            "title": "SCONE SGX Toolchain"
        },
        {
            "location": "/SCONE_toolchain/#compiler-variants",
            "text": "Depending on if you want to generate a  dynamically-linked  or a  statically-linked  binary, you\ncan use a standard compiler (dynamic) or you need to use a cross compiler (static). The compiler can run on any system, i.e., does not require SGX to run. To generate a statically-linked binary, you need to run on a SGX-enabled machine (for now).  Note  Independently, if you use a dynamic or static linking, the hash of an enclave (MRENCLAVE) will encompass the whole code, i.e., includes all libraries. Any updates of a library on your host might prevent the execution of a SCONE binary because of a wrong MRENCLAVE. Hence, we recommend to use only statically-linked programs on the host. In containers, which have a more controlled environment, we support both statically as well as dynamically linked binaries. The main advantage of dynamic linking is that for many programs we do not change the build process when moved to SCONE.  Note  also that SCONE supports the loading of dynamic libraries after a program has already started inside of an enclave. This feature is required by modern languages like Java. Enabling general loading of dynamic library introduces the risk that one could load malicious code inside of an enclave. Hence, we switch this feature off by default. For debugging programs, you can enable this feature via an environment variable ( export SCONE_ALLOW_DLOPEN=2).  For production enclaves, you will need to protect the integrity of the shared libraries with the help of the  SCONE file shield .  Dynamically-Linked Binaries  The easiest to get started is to compile your programs such that    the generated code is position independent ( -fPIC ),    the thread local storage model is global-dynamic ( -ftls-model=global-dynamic ),    your binary is dynamically linked (i.e., do not use  -static ), and    link against musl as your libc (i.e., not glibc or any other libc).     When a program is started, SCONE uses its own dynamic link loader to replace libc by a SCONE libc. The SCONE dynamic linker will load the program inside a new SGX enclave and SCONE libc will enable programs to run inside the SGX enclaves, e.g., execute system calls and protect them from attacks via shields like the  file system shield .  To simplify the compiling of your programs for scone, we make available a docker image  sconecuratedimages/muslgcc  which includes  gcc  and  g++  support. The options will by default be set as shown above. You need, however, to make sure that your Makefiles will not overwrite these options.  Statically-Linked Binaries  For statically linked binaries, we make available a (private) docker image  sconecuratedimages/crosscompilers:scone  which can produce statically linked binaries. In the statically linked binaries, we replace the interface to the operating system (i.e., libc) by a variant that enables programs to run inside Intel SGX enclaves.   Note that a statically linked binaries might look like a dynamically-linked binary. For example, if you look at a statically-linked program  web-srv-go , you will still see dynamic dependencies:  $ ldd web-srv-go \n    linux-vdso.so.1 =>  (0x00007ffe423fd000)\n    libpthread.so.0 => /lib/x86_64-linux-gnu/libpthread.so.0 (0x00007effa344f000)\n    libc.so.6 => /lib/x86_64-linux-gnu/libc.so.6 (0x00007effa3085000)\n    /lib64/ld-linux-x86-64.so.2 (0x00007effa366c000)  The reason for that is that the statically linked binary that runs inside of an enclave is wrapped in a dynamically linked  loader  program. The loader program creates the enclave, moves the program code inside the enclave and starts threads that will enter the enclave. The code that is moved inside the enclave is, however, statically linked.",
            "title": "Compiler variants"
        },
        {
            "location": "/SCONE_toolchain/#using-the-cross-compiler-container",
            "text": "How to use the compiler:    use this as a base image and build your programs inside of a container we a   Dockerfile ), or    map volumes such that the compiler can compile files living outside the container (see  SCONE Tutorial ).    For an example how to use the crosscompilers, see how to compile a programs written in  GO .",
            "title": "Using the cross compiler container"
        },
        {
            "location": "/SCONE_toolchain/#example",
            "text": "Note   on some systems you will need to run  docker  with root permissions, i.e., in this case you should prefix a   > docker ...  command with  sudo , i.e., you execute   > sudo docker ...  One can run the above compiler inside of a container while the compiled files reside outside the container. Say, your code is\nin file  myapp.c  in your current directory ( $PWD ). You can compile this code as follows:  > docker run --rm  -v \"$PWD\":/usr/src/myapp -w /usr/src/myapp sconecuratedimages/muslgcc gcc myapp.c  This call will generate a binary  a.out  in your working directory. This binary is dynamically linked against musl:  > ldd a.out \n    /lib/ld-musl-x86_64.so.1 (0x7fb0379f9000)\n    libc.musl-x86_64.so.1 => /lib/ld-musl-x86_64.so.1 (0x7fb0379f9000)  This binary can run natively only if you have musl installed at the correct position in your development machine (and your development machine runs Linux). Alternatively, you can run the binary in a container:  > docker run --rm  -v \"$PWD\":/usr/src/myapp -w /usr/src/myapp sconecuratedimages/muslgcc ./a.out  To run this inside of SGX enclaves with SCONE, you need access to the SCONE runtime systems. For more details, see our  hello world  in Section  SCONE Tutorial . This is not very convenient and hence, we provide a) a simpler version with the help of  Dockerfiles .   In most cases, you might just set to use one of our crosscompilers in your  configure  script or  Makefile . A simple way is to use the Docker image  sconecuratedimages/crosscompilers:scone  as a base image and then clone your code inside the container and set one or more of our compilers ( scone-gcc, scone-g++, scone-gccgo, scone-gfortran, and scone-rustc ) to be used in your build. For Rust, we support also our variant of  cargo  which is  scone-cargo .",
            "title": "Example"
        },
        {
            "location": "/SCONE_toolchain/#debugger-support",
            "text": "We also support  gdb  to debug applications running inside of enclaves. To get started, we recommend that you first ensure that your program runs natively linked against musl. Most programs will do - after all, the Alpine Linux distribution is completely based on musl. The debugger is available in image  sconecuratedimages/crosscompilers:scone  as  scone-gdb .  For example on how to use the debugger, see how to debug a program written in   GO .  \u00a9  scontain.com , November 2017.  Questions or Suggestions?",
            "title": "Debugger support"
        },
        {
            "location": "/SCONE_Curated_Images/",
            "text": "SCONE Curated Images\n\n\nWe provide a set of curated SCONE container images on a (partially private) repositories on Docker hub:\n\n\n Private images:\n (i.e., you need to send me email to get access)\n\n\n\n\nsconecuratedimages/crosscompilers:scone\n: a container image with all the SCONE crosscompilers.\n\n\nsconecuratedimages/crosscompilers:runtime\n: a container image that can run dynamically linked applications inside of an enclave.\n\n\nsconecuratedimages/crosscompilers:python27\n: a container image including a \npython interpreter\n running inside of an enclave.\n\n\n\n\nsconecuratedimages/crosscompilers:node-4.8.6\n: a container image for \nnode\n running inside an enclave.\n\n\n\n\n\n\nsconecuratedimages/helloworld: a simple hello world example that runs inside of an enclave.\n\n\n\n\n\n\n Public images:\n\n\n\n\nsconecuratedimages/muslgcc\n: public container image to compile programs ready to be used by SCONE\n\n\nsconecuratedimages/tutorial\n: contains container images related to our SCONE tutorial\n\n\nsconecuratedimages/sconedocu\n: public container image containing a copy of the SCONE documentation\n\n\n\n\nScone Documentation\n\n\nTo run a local copy of the SCONE documentation, just perform the following steps:\n\n\n> docker pull sconecuratedimages/sconedocu\n> docker run -d -p 8080:80  sconecuratedimages/sconedocu\n\n\n\n\nView the documentation in your browser at http://127.0.0.1:8080 .\nOn a MAC, just type:\n\n\n> open http://127.0.0.1:8080\n\n\n\n\nto view this docu.\n\n\nLogin in\n\n\nAccess to some SCONE images is still restricted. First, get access\nto the private images by sending email to scontain.ceo@gmail.com. \nSecond, log into to docker hub via:\n\n\n> docker login\n\n\n\n\nbefore you will be able to pull any of the private curated images.\n\n\nScone Compilers\n\n\nTo run a local copy of the SCONE (cross-)compilers, just pull the appropriate image on your computer.\n\n\nDynamically-Linked Binaries\n\n\nEven if you have no SGX CPU extension / no SGX driver installed on your computer,\nyou can use a standard gcc compiler - as long as the requirements mentioned in \nSGX ToolChain\n are satisfied.\n\n\n> docker pull sconecuratedimages/muslgcc\n\n\n\n\nNote that the binaries generated with the above image are just native binaries, i.e., they run \noutside of enclaves\n. To be able to run the binary inside of an enclave, you need to have installed the SCONE runtime library.\n\n\nTo run a dynamically-linked binary, one needs a special runtime environment. We provide this in form of a (private) container image:\n\n\n> docker pull sconecuratedimages/crosscompilers:runtime\n\n\n\n\nStatically-Linked Binaries\n\n\nTo generate statically-linked secure binaries you need a cross compiler. You can pull\nthis image from Docker hub (you need to be granted access rights for that):\n\n\n> docker pull sconecuratedimages/crosscompilers:scone\n\n\n\n\nScone Hello World\n\n\nYou can pull the following (private) image:\n\n\n> docker pull sconecuratedimages/helloworld\n\n\n\n\nIf you installed the patched Docker engine (see \nSCONE Host Setup\n), run the helloworld program inside of an enclave via\n\n\n> docker run sconecuratedimages/helloworld\nHello World\n\n\n\n\nThis command will fail in case you have the standard Docker engine installed:\n\n\n> docker run sconecuratedimages/helloworld\nerror opening sgx device: No such file or directory\n\n\n\n\nYou can run on the standard Docker engine - if you have the SGX driver installed:\n\n\n> docker run --device=/dev/isgx sconecuratedimages/helloworld\nHello World\n\n\n\n\nIf you do not have the SGX driver installed, you get an error message:\n\n\n> docker run --device=/dev/isgx sconecuratedimages/helloworld\ndocker: Error response from daemon: linux runtime spec devices: error gathering device information while adding custom device \"/dev/isgx\": no such file or directory.\n\n\n\n\nIn this case, install the SGX driver and the patched Docker engine as described in\n\nSCONE Host Setup\n. This installation will fail in case you disabled SGX in the BIOS or your CPU is not SGX-enabled.\n\n\nScreencast\n\n\n\n\n\u00a9 \nscontain.com\n, November 2017. \nQuestions or Suggestions?",
            "title": "SCONE Curated Images"
        },
        {
            "location": "/SCONE_Curated_Images/#scone-curated-images",
            "text": "We provide a set of curated SCONE container images on a (partially private) repositories on Docker hub:   Private images:  (i.e., you need to send me email to get access)   sconecuratedimages/crosscompilers:scone : a container image with all the SCONE crosscompilers.  sconecuratedimages/crosscompilers:runtime : a container image that can run dynamically linked applications inside of an enclave.  sconecuratedimages/crosscompilers:python27 : a container image including a  python interpreter  running inside of an enclave.   sconecuratedimages/crosscompilers:node-4.8.6 : a container image for  node  running inside an enclave.    sconecuratedimages/helloworld: a simple hello world example that runs inside of an enclave.     Public images:   sconecuratedimages/muslgcc : public container image to compile programs ready to be used by SCONE  sconecuratedimages/tutorial : contains container images related to our SCONE tutorial  sconecuratedimages/sconedocu : public container image containing a copy of the SCONE documentation",
            "title": "SCONE Curated Images"
        },
        {
            "location": "/SCONE_Curated_Images/#scone-documentation",
            "text": "To run a local copy of the SCONE documentation, just perform the following steps:  > docker pull sconecuratedimages/sconedocu\n> docker run -d -p 8080:80  sconecuratedimages/sconedocu  View the documentation in your browser at http://127.0.0.1:8080 .\nOn a MAC, just type:  > open http://127.0.0.1:8080  to view this docu.",
            "title": "Scone Documentation"
        },
        {
            "location": "/SCONE_Curated_Images/#login-in",
            "text": "Access to some SCONE images is still restricted. First, get access\nto the private images by sending email to scontain.ceo@gmail.com. \nSecond, log into to docker hub via:  > docker login  before you will be able to pull any of the private curated images.",
            "title": "Login in"
        },
        {
            "location": "/SCONE_Curated_Images/#scone-compilers",
            "text": "To run a local copy of the SCONE (cross-)compilers, just pull the appropriate image on your computer.",
            "title": "Scone Compilers"
        },
        {
            "location": "/SCONE_Curated_Images/#dynamically-linked-binaries",
            "text": "Even if you have no SGX CPU extension / no SGX driver installed on your computer,\nyou can use a standard gcc compiler - as long as the requirements mentioned in  SGX ToolChain  are satisfied.  > docker pull sconecuratedimages/muslgcc  Note that the binaries generated with the above image are just native binaries, i.e., they run  outside of enclaves . To be able to run the binary inside of an enclave, you need to have installed the SCONE runtime library.  To run a dynamically-linked binary, one needs a special runtime environment. We provide this in form of a (private) container image:  > docker pull sconecuratedimages/crosscompilers:runtime",
            "title": "Dynamically-Linked Binaries"
        },
        {
            "location": "/SCONE_Curated_Images/#statically-linked-binaries",
            "text": "To generate statically-linked secure binaries you need a cross compiler. You can pull\nthis image from Docker hub (you need to be granted access rights for that):  > docker pull sconecuratedimages/crosscompilers:scone",
            "title": "Statically-Linked Binaries"
        },
        {
            "location": "/SCONE_Curated_Images/#scone-hello-world",
            "text": "You can pull the following (private) image:  > docker pull sconecuratedimages/helloworld  If you installed the patched Docker engine (see  SCONE Host Setup ), run the helloworld program inside of an enclave via  > docker run sconecuratedimages/helloworld\nHello World  This command will fail in case you have the standard Docker engine installed:  > docker run sconecuratedimages/helloworld\nerror opening sgx device: No such file or directory  You can run on the standard Docker engine - if you have the SGX driver installed:  > docker run --device=/dev/isgx sconecuratedimages/helloworld\nHello World  If you do not have the SGX driver installed, you get an error message:  > docker run --device=/dev/isgx sconecuratedimages/helloworld\ndocker: Error response from daemon: linux runtime spec devices: error gathering device information while adding custom device \"/dev/isgx\": no such file or directory.  In this case, install the SGX driver and the patched Docker engine as described in SCONE Host Setup . This installation will fail in case you disabled SGX in the BIOS or your CPU is not SGX-enabled.",
            "title": "Scone Hello World"
        },
        {
            "location": "/SCONE_Curated_Images/#screencast",
            "text": "\u00a9  scontain.com , November 2017.  Questions or Suggestions?",
            "title": "Screencast"
        },
        {
            "location": "/SCONE_TUTORIAL/",
            "text": "SCONE Tutorial\n\n\nPrerequisites\n\n\nEnsure that the SGX driver is installed\n\n\nCheck on the host as well as inside your containers that the SGX device \n/dev/isgx\n is visible:\n\n\n> ls /dev/isgx \n/dev/isgx\n\n\n\n\nIf the driver is not installed, read Section \nSCONE Host Setup\n to learn how to install the SGX driver.\n\n\nChecking availability of SGX device inside of containers\n\n\nSometimes, Docker does not automatically map the SGX device inside of containers. We provide a patched Docker engine and a patched SGX driver that together permit to automatically open the sgx device inside of containers.\n\n\nIf your node is part of a Docker swarm, we provide a simple check as part of the \nscone CLI\n. Say, the leader node of your swarm  is called \nbeatrix\n, then you could just execute:\n\n\n$ scone swarm check --verbose --manager beatrix\n\n\n\n\nIn case your node is not part of a Docker swarm, you can run the checks manually. For that, we provide a container image\nthat helps to check if the SGX device can be accessed from inside a container. Just execute the following commands to check if your containers have access to the SGX device:\n\n\n# preferred alternative: required for swarms to work: SGX device is available in all containers by default\n> sudo docker run --rm sconecuratedimages/checksgx || echo \"SGX device is not automatically mapped inside of container\"\n# alternative: use --device option without --privileged flag\n> sudo docker run --device=/dev/isgx --rm sconecuratedimages/checksgx || echo \"--device=/dev/isgx: not sufficient to access SGX device inside of container\"\n# last alternative: use --device option without --privileged flag\n> sudo docker run -v /dev/isgx:/dev/isgx --privileged  --rm sconecuratedimages/checksgx || echo \"SGX device NOT available inside of container\"\n\n\n\n\nUse the first alternative that works in your installation to give containers access to the SGX device.\n\n\nInstall sgxmusl cross compiler image\n\n\nEnsure that you installed the various sconecuratedimages/crosscompilers container image:\n\n\n> docker image ls sconecuratedimages/*\nREPOSITORY                          TAG                    IMAGE ID            CREATED             SIZE\nsconecuratedimages/crosscompilers   latest                 dff7975b7f32        7 hours ago         1.57GB\nsconecuratedimages/crosscompilers   scone                  dff7975b7f32        7 hours ago         1.57GB\n\n\n\n\nIf the cross compiler image is not yet installed, read Section \nSCONE Curated Container Images\n to learn how to install the SCONE cross compiler image.\n\n\nIf the docker command fails, please ensure that docker is indeed installed (see \nSCONE Host Setup\n. Also, on some systems you might need to use \nsodo\n to run docker commands.\n\n\nInstall the tutorial\n\n\nClone the tutorial:\n\n\n> git clone https://github.com/christoffetzer/SCONE_TUTORIAL.git\n\n\n\n\nNative Hello World\n\n\nEnsure that \nhello world\n runs natively on your machine:\n\n\n> cd SCONE_TUTORIAL/HelloWorld/\n> gcc hello_world.c  -o native_hello_world\n> ./native_hello_world\nHello World\n\n\n\n\nNote that the generated executable, i.e., \nsim_hello_world\n, will only run on Linux.\n\n\nStatically-Linked Hello World\n\n\nThe default cross compiler variant that runs \nhello world\n inside of an enclave is \nscone gcc\n and you can find this in container \nsconecuratedimages/crosscompilers:scone\n.\nThis variant requires access to the SGX device. \nIn Linux, the SGX device is made available as \n/dev/isgx\n and we can give the cross compiler inside of an container access via option \n--device=/dev/isgx\n:\n\n\n> docker run --rm --device=/dev/isgx -v \"$PWD\":/usr/src/myapp -w /usr/src/myapp sconecuratedimages/crosscompilers:scone scone-gcc hello_world.c  -o sgx_hello_world\n\n\n\n\nThis generates a statically linked binary. However, as we mentioned above, the binary looks like a dynamically\nlinked binary since it is wrapped in a dynamically linked loader program:\n\n\n> ldd ./sgx_hello_world \n    linux-vdso.so.1 =>  (0x00007ffcf73ad000)\n    libpthread.so.0 => /lib/x86_64-linux-gnu/libpthread.so.0 (0x00007f7c2a0e9000)\n    libc.so.6 => /lib/x86_64-linux-gnu/libc.so.6 (0x00007f7c29d1f000)\n    /lib64/ld-linux-x86-64.so.2 (0x00007f7c2a306000)\n\n\n\n\nEnsure that file \n/etc/sgx-musl.conf\n exists. If not, store some default file like:\n\n\n> printf \"Q 1\\ne 0 0 0\\ns 1 0 0\\n\" | sudo tee /etc/sgx-musl.conf\n\n\n\n\nTo run \nsgx_hello_world\n, in an enclave, just execute:\n\n\n> ./sgx_hello_world\nHello World\n\n\n\n\nTo see some more debug messages, set environment variable \nSCONE_VERSION=1\n:\n\n\n> SCONE_VERSION=1 ./sgx_hello_world\nexport SCONE_QUEUES=4\nexport SCONE_SLOTS=256\nexport SCONE_SIGPIPE=0\nexport SCONE_MMAP32BIT=0\nexport SCONE_SSPINS=100\nexport SCONE_SSLEEP=4000\nexport SCONE_KERNEL=0\nexport SCONE_HEAP=67108864\nexport SCONE_CONFIG=/etc/sgx-musl.conf\nexport SCONE_MODE=hw\nexport SCONE_SGXBOUNDS=no\nexport SCONE_ALLOW_DLOPEN=no\nRevision: 9b355b99170ad434010353bb9f4dca24e532b1b7\nBranch: master\nConfigure options: --enable-file-prot --enable-shared --enable-debug --prefix=/scone/src/built/cross-compiler/x86_64-linux-musl\n\nHello World\n\n\n\n\nThe debug outputs \nSCONE_MODE=hw\n shows that \nsgx_hello_world\n runs in hardware mode, i.e., inside an SGX enclave.\n\n\nNote.\n The compilation as well as the hello world program will fail in case you do not have an SGX driver installed.\n\n\nDynamically-Linked Hello World\n\n\n> docker run --rm  -v \"$PWD\":/usr/src/myapp -w /usr/src/myapp sconecuratedimages/muslgcc gcc  hello_world.c -o dyn_hello_world\n\n\n\n\nTo run this natively, just execute the following:\n\n\n> docker run --rm  -v \"$PWD\":/usr/src/myapp -w /usr/src/myapp sconecuratedimages/muslgcc ./dyn_hello_world\n\n\n\n\nTo run a dynamically-linked binary in an enclave, you need to run this in a special runtime environment. In this environment you can ask binaries to run inside of enclaves by setting environment \nSCONE_ALPINE=1\n. To indicate that we are indeed running inside an enclave, we ask to issue some debug messages from inside the enclave by setting environment variable \nSCONE_VERSION=1\n:\n\n\nHardware Mode vs Simulation Mode\n\n\nFor debugging, we support three different modes for execution: \nhardware, simulation, and automatic\n:\n\n\n\n\n\n\nhardware\n: by setting environment variable to \nSCONE_MODE=HW\n, SCONE will enforce running this application inside an SGX enclave.\n\n\n\n\n\n\nsimulation\n: by setting environment variable to \nSCONE_MODE=SIM\n, SCONE will enforce running this application in native mode (i.e., outside of an enclave). This will run all SCONE functionality but outside enclaves. This is intended for development and debugging on machines that are not SGX-capable.\n\n\n\n\n\n\nautomatic\n: by setting environment variable to \nSCONE_MODE=AUTO\n, SCONE will run the application inside of an SGX enclave if available and otherwise in simulation mode. (This is the default mode)\n\n\n\n\n\n\nNOTE\n: In production mode, you must only permit running in hardware mode. Scone ensures this with the help of \nremote attestation\n: the SCONE configuration and attestation service (CAS) will only provide configuration information and secrets to an application only after it has proven (with the help of SGX CPU extensions) that it is indeed running inside an SGX enclave.\n\n\nExecution on a SGX-capable machine\n\n\n> docker run --rm  -v \"$PWD\":/usr/src/myapp -e SCONE_MODE=HW -e SCONE_ALPINE=1 -e SCONE_VERSION=1 sconecuratedimages/crosscompilers:runtime /usr/src/myapp/dyn_hello_world\nexport SCONE_QUEUES=4\nexport SCONE_SLOTS=256\nexport SCONE_SIGPIPE=0\nexport SCONE_MMAP32BIT=0\nexport SCONE_SSPINS=100\nexport SCONE_SSLEEP=4000\nexport SCONE_KERNEL=0\nexport SCONE_HEAP=67108864\nexport SCONE_CONFIG=/etc/sgx-musl.conf\nexport SCONE_MODE=hw\nConfigure parameters: \n1.1.15\nHello World\n\n\n\n\nExecution on a non-SGX machine\n\n\nIf you run this inside a container without access to SGX (/dev/isgx), for example, when running on a Mac, you will see the following error message:\n\n\n> docker run --rm  -v \"$PWD\":/usr/src/myapp -e SCONE_MODE=HW -e SCONE_ALPINE=1 -e SCONE_VERSION=1 sconecuratedimages/crosscompilers:runtime /usr/src/myapp/dyn_hello_world\n[Error] Could not create enclave: Error opening SGX device\n\n\n\n\nYou could run this in simulation mode as follows:\n\n\n> docker run --rm  -v \"$PWD\":/usr/src/myapp -e SCONE_MODE=SIM -e SCONE_ALPINE=1 -e SCONE_VERSION=1 sconecuratedimages/crosscompilers:runtime /usr/src/myapp/dyn_hello_world\nexport SCONE_QUEUES=4\nexport SCONE_SLOTS=256\nexport SCONE_SIGPIPE=0\nexport SCONE_MMAP32BIT=0\nexport SCONE_SSPINS=100\nexport SCONE_SSLEEP=4000\nexport SCONE_KERNEL=0\nexport SCONE_HEAP=67108864\nexport SCONE_CONFIG=/etc/sgx-musl.conf\nexport SCONE_MODE=sim\nConfigure parameters: \n1.1.15\nHello World\n\n\n\n\nAlternatively, you could run this program in automatic mode:\n\n\n> docker run --rm  -v \"$PWD\":/usr/src/myapp -e SCONE_MODE=AUTO -e SCONE_ALPINE=1 -e SCONE_VERSION=1 sconecuratedimages/crosscompilers:runtime \nexport SCONE_QUEUES=4\nexport SCONE_SLOTS=256\nexport SCONE_SIGPIPE=0\nexport SCONE_MMAP32BIT=0\nexport SCONE_SSPINS=100\nexport SCONE_SSLEEP=4000\nexport SCONE_KERNEL=0\nexport SCONE_HEAP=67108864\nexport SCONE_CONFIG=/etc/sgx-musl.conf\nexport SCONE_MODE=sim\nConfigure parameters:\n1.1.15\nHelloWorld\n\n\n\n\nRun STRACE\n\n\nLets see how we can trace the program. Say, you have compile the program as shown above. After that you enter a cross compiler container and strace hello world as follows:\n\n\n> docker run --cap-add SYS_PTRACE -it --rm --device=/dev/isgx -v \"$PWD\":/usr/src/myapp -w /usr/src/myapp sconecuratedimages/crosscompilers strace  -f /usr/src/myapp/sgx_hello_world > strace.log\nHello World\nhead strace.log\nexecve(\"/usr/src/myapp/sgx_hello_world\", [\"/usr/src/myapp/sgx_hello_world\"], [/* 10 vars */]) = 0\nbrk(NULL)                               = 0x10e8000\naccess(\"/etc/ld.so.nohwcap\", F_OK)      = -1 ENOENT (No such file or directory)\nmmap(NULL, 8192, PROT_READ|PROT_WRITE, MAP_PRIVATE|MAP_ANONYMOUS, -1, 0) = 0x7f17f07f1000\naccess(\"/etc/ld.so.preload\", R_OK)      = -1 ENOENT (No such file or directory)\nopen(\"/etc/ld.so.cache\", O_RDONLY|O_CLOEXEC) = 3\nfstat(3, {st_mode=S_IFREG|0644, st_size=18506, ...}) = 0\nmmap(NULL, 18506, PROT_READ, MAP_PRIVATE, 3, 0) = 0x7f17f07ec000\nclose(3)                                = 0\naccess(\"/etc/ld.so.nohwcap\", F_OK)      = -1 ENOENT (No such file or directory)\n\n\n\n\nScreencast\n\n\n\n\n\u00a9 \nscontain.com\n, November 2017. \nQuestions or Suggestions?",
            "title": "SCONE Tutorial"
        },
        {
            "location": "/SCONE_TUTORIAL/#scone-tutorial",
            "text": "",
            "title": "SCONE Tutorial"
        },
        {
            "location": "/SCONE_TUTORIAL/#prerequisites",
            "text": "Ensure that the SGX driver is installed  Check on the host as well as inside your containers that the SGX device  /dev/isgx  is visible:  > ls /dev/isgx \n/dev/isgx  If the driver is not installed, read Section  SCONE Host Setup  to learn how to install the SGX driver.  Checking availability of SGX device inside of containers  Sometimes, Docker does not automatically map the SGX device inside of containers. We provide a patched Docker engine and a patched SGX driver that together permit to automatically open the sgx device inside of containers.  If your node is part of a Docker swarm, we provide a simple check as part of the  scone CLI . Say, the leader node of your swarm  is called  beatrix , then you could just execute:  $ scone swarm check --verbose --manager beatrix  In case your node is not part of a Docker swarm, you can run the checks manually. For that, we provide a container image\nthat helps to check if the SGX device can be accessed from inside a container. Just execute the following commands to check if your containers have access to the SGX device:  # preferred alternative: required for swarms to work: SGX device is available in all containers by default\n> sudo docker run --rm sconecuratedimages/checksgx || echo \"SGX device is not automatically mapped inside of container\"\n# alternative: use --device option without --privileged flag\n> sudo docker run --device=/dev/isgx --rm sconecuratedimages/checksgx || echo \"--device=/dev/isgx: not sufficient to access SGX device inside of container\"\n# last alternative: use --device option without --privileged flag\n> sudo docker run -v /dev/isgx:/dev/isgx --privileged  --rm sconecuratedimages/checksgx || echo \"SGX device NOT available inside of container\"  Use the first alternative that works in your installation to give containers access to the SGX device.  Install sgxmusl cross compiler image  Ensure that you installed the various sconecuratedimages/crosscompilers container image:  > docker image ls sconecuratedimages/*\nREPOSITORY                          TAG                    IMAGE ID            CREATED             SIZE\nsconecuratedimages/crosscompilers   latest                 dff7975b7f32        7 hours ago         1.57GB\nsconecuratedimages/crosscompilers   scone                  dff7975b7f32        7 hours ago         1.57GB  If the cross compiler image is not yet installed, read Section  SCONE Curated Container Images  to learn how to install the SCONE cross compiler image.  If the docker command fails, please ensure that docker is indeed installed (see  SCONE Host Setup . Also, on some systems you might need to use  sodo  to run docker commands.",
            "title": "Prerequisites"
        },
        {
            "location": "/SCONE_TUTORIAL/#install-the-tutorial",
            "text": "Clone the tutorial:  > git clone https://github.com/christoffetzer/SCONE_TUTORIAL.git",
            "title": "Install the tutorial"
        },
        {
            "location": "/SCONE_TUTORIAL/#native-hello-world",
            "text": "Ensure that  hello world  runs natively on your machine:  > cd SCONE_TUTORIAL/HelloWorld/\n> gcc hello_world.c  -o native_hello_world\n> ./native_hello_world\nHello World  Note that the generated executable, i.e.,  sim_hello_world , will only run on Linux.",
            "title": "Native Hello World"
        },
        {
            "location": "/SCONE_TUTORIAL/#statically-linked-hello-world",
            "text": "The default cross compiler variant that runs  hello world  inside of an enclave is  scone gcc  and you can find this in container  sconecuratedimages/crosscompilers:scone .\nThis variant requires access to the SGX device. \nIn Linux, the SGX device is made available as  /dev/isgx  and we can give the cross compiler inside of an container access via option  --device=/dev/isgx :  > docker run --rm --device=/dev/isgx -v \"$PWD\":/usr/src/myapp -w /usr/src/myapp sconecuratedimages/crosscompilers:scone scone-gcc hello_world.c  -o sgx_hello_world  This generates a statically linked binary. However, as we mentioned above, the binary looks like a dynamically\nlinked binary since it is wrapped in a dynamically linked loader program:  > ldd ./sgx_hello_world \n    linux-vdso.so.1 =>  (0x00007ffcf73ad000)\n    libpthread.so.0 => /lib/x86_64-linux-gnu/libpthread.so.0 (0x00007f7c2a0e9000)\n    libc.so.6 => /lib/x86_64-linux-gnu/libc.so.6 (0x00007f7c29d1f000)\n    /lib64/ld-linux-x86-64.so.2 (0x00007f7c2a306000)  Ensure that file  /etc/sgx-musl.conf  exists. If not, store some default file like:  > printf \"Q 1\\ne 0 0 0\\ns 1 0 0\\n\" | sudo tee /etc/sgx-musl.conf  To run  sgx_hello_world , in an enclave, just execute:  > ./sgx_hello_world\nHello World  To see some more debug messages, set environment variable  SCONE_VERSION=1 :  > SCONE_VERSION=1 ./sgx_hello_world\nexport SCONE_QUEUES=4\nexport SCONE_SLOTS=256\nexport SCONE_SIGPIPE=0\nexport SCONE_MMAP32BIT=0\nexport SCONE_SSPINS=100\nexport SCONE_SSLEEP=4000\nexport SCONE_KERNEL=0\nexport SCONE_HEAP=67108864\nexport SCONE_CONFIG=/etc/sgx-musl.conf\nexport SCONE_MODE=hw\nexport SCONE_SGXBOUNDS=no\nexport SCONE_ALLOW_DLOPEN=no\nRevision: 9b355b99170ad434010353bb9f4dca24e532b1b7\nBranch: master\nConfigure options: --enable-file-prot --enable-shared --enable-debug --prefix=/scone/src/built/cross-compiler/x86_64-linux-musl\n\nHello World  The debug outputs  SCONE_MODE=hw  shows that  sgx_hello_world  runs in hardware mode, i.e., inside an SGX enclave.  Note.  The compilation as well as the hello world program will fail in case you do not have an SGX driver installed.",
            "title": "Statically-Linked Hello World"
        },
        {
            "location": "/SCONE_TUTORIAL/#dynamically-linked-hello-world",
            "text": "> docker run --rm  -v \"$PWD\":/usr/src/myapp -w /usr/src/myapp sconecuratedimages/muslgcc gcc  hello_world.c -o dyn_hello_world  To run this natively, just execute the following:  > docker run --rm  -v \"$PWD\":/usr/src/myapp -w /usr/src/myapp sconecuratedimages/muslgcc ./dyn_hello_world  To run a dynamically-linked binary in an enclave, you need to run this in a special runtime environment. In this environment you can ask binaries to run inside of enclaves by setting environment  SCONE_ALPINE=1 . To indicate that we are indeed running inside an enclave, we ask to issue some debug messages from inside the enclave by setting environment variable  SCONE_VERSION=1 :  Hardware Mode vs Simulation Mode  For debugging, we support three different modes for execution:  hardware, simulation, and automatic :    hardware : by setting environment variable to  SCONE_MODE=HW , SCONE will enforce running this application inside an SGX enclave.    simulation : by setting environment variable to  SCONE_MODE=SIM , SCONE will enforce running this application in native mode (i.e., outside of an enclave). This will run all SCONE functionality but outside enclaves. This is intended for development and debugging on machines that are not SGX-capable.    automatic : by setting environment variable to  SCONE_MODE=AUTO , SCONE will run the application inside of an SGX enclave if available and otherwise in simulation mode. (This is the default mode)    NOTE : In production mode, you must only permit running in hardware mode. Scone ensures this with the help of  remote attestation : the SCONE configuration and attestation service (CAS) will only provide configuration information and secrets to an application only after it has proven (with the help of SGX CPU extensions) that it is indeed running inside an SGX enclave.  Execution on a SGX-capable machine  > docker run --rm  -v \"$PWD\":/usr/src/myapp -e SCONE_MODE=HW -e SCONE_ALPINE=1 -e SCONE_VERSION=1 sconecuratedimages/crosscompilers:runtime /usr/src/myapp/dyn_hello_world\nexport SCONE_QUEUES=4\nexport SCONE_SLOTS=256\nexport SCONE_SIGPIPE=0\nexport SCONE_MMAP32BIT=0\nexport SCONE_SSPINS=100\nexport SCONE_SSLEEP=4000\nexport SCONE_KERNEL=0\nexport SCONE_HEAP=67108864\nexport SCONE_CONFIG=/etc/sgx-musl.conf\nexport SCONE_MODE=hw\nConfigure parameters: \n1.1.15\nHello World  Execution on a non-SGX machine  If you run this inside a container without access to SGX (/dev/isgx), for example, when running on a Mac, you will see the following error message:  > docker run --rm  -v \"$PWD\":/usr/src/myapp -e SCONE_MODE=HW -e SCONE_ALPINE=1 -e SCONE_VERSION=1 sconecuratedimages/crosscompilers:runtime /usr/src/myapp/dyn_hello_world\n[Error] Could not create enclave: Error opening SGX device  You could run this in simulation mode as follows:  > docker run --rm  -v \"$PWD\":/usr/src/myapp -e SCONE_MODE=SIM -e SCONE_ALPINE=1 -e SCONE_VERSION=1 sconecuratedimages/crosscompilers:runtime /usr/src/myapp/dyn_hello_world\nexport SCONE_QUEUES=4\nexport SCONE_SLOTS=256\nexport SCONE_SIGPIPE=0\nexport SCONE_MMAP32BIT=0\nexport SCONE_SSPINS=100\nexport SCONE_SSLEEP=4000\nexport SCONE_KERNEL=0\nexport SCONE_HEAP=67108864\nexport SCONE_CONFIG=/etc/sgx-musl.conf\nexport SCONE_MODE=sim\nConfigure parameters: \n1.1.15\nHello World  Alternatively, you could run this program in automatic mode:  > docker run --rm  -v \"$PWD\":/usr/src/myapp -e SCONE_MODE=AUTO -e SCONE_ALPINE=1 -e SCONE_VERSION=1 sconecuratedimages/crosscompilers:runtime \nexport SCONE_QUEUES=4\nexport SCONE_SLOTS=256\nexport SCONE_SIGPIPE=0\nexport SCONE_MMAP32BIT=0\nexport SCONE_SSPINS=100\nexport SCONE_SSLEEP=4000\nexport SCONE_KERNEL=0\nexport SCONE_HEAP=67108864\nexport SCONE_CONFIG=/etc/sgx-musl.conf\nexport SCONE_MODE=sim\nConfigure parameters:\n1.1.15\nHelloWorld",
            "title": "Dynamically-Linked Hello World"
        },
        {
            "location": "/SCONE_TUTORIAL/#run-strace",
            "text": "Lets see how we can trace the program. Say, you have compile the program as shown above. After that you enter a cross compiler container and strace hello world as follows:  > docker run --cap-add SYS_PTRACE -it --rm --device=/dev/isgx -v \"$PWD\":/usr/src/myapp -w /usr/src/myapp sconecuratedimages/crosscompilers strace  -f /usr/src/myapp/sgx_hello_world > strace.log\nHello World\nhead strace.log\nexecve(\"/usr/src/myapp/sgx_hello_world\", [\"/usr/src/myapp/sgx_hello_world\"], [/* 10 vars */]) = 0\nbrk(NULL)                               = 0x10e8000\naccess(\"/etc/ld.so.nohwcap\", F_OK)      = -1 ENOENT (No such file or directory)\nmmap(NULL, 8192, PROT_READ|PROT_WRITE, MAP_PRIVATE|MAP_ANONYMOUS, -1, 0) = 0x7f17f07f1000\naccess(\"/etc/ld.so.preload\", R_OK)      = -1 ENOENT (No such file or directory)\nopen(\"/etc/ld.so.cache\", O_RDONLY|O_CLOEXEC) = 3\nfstat(3, {st_mode=S_IFREG|0644, st_size=18506, ...}) = 0\nmmap(NULL, 18506, PROT_READ, MAP_PRIVATE, 3, 0) = 0x7f17f07ec000\nclose(3)                                = 0\naccess(\"/etc/ld.so.nohwcap\", F_OK)      = -1 ENOENT (No such file or directory)",
            "title": "Run STRACE"
        },
        {
            "location": "/SCONE_TUTORIAL/#screencast",
            "text": "\u00a9  scontain.com , November 2017.  Questions or Suggestions?",
            "title": "Screencast"
        },
        {
            "location": "/SCONE_GENERATE_IMAGE/",
            "text": "Generating Container Image with SCONE\n\n\nWe show how to generate a Docker image that contains our \nhello world\n running inside of an enclave and pushing this to docker hub. We only show this for the statically-linked binary. You can see that this code is quite awkward. It is much easier to generate images with a Dockerfile - which we show in the next section.\n\n\nPrerequisites\n\n\nCheck that all prerequisites from \nSCONE Tutorial\n are satisfied. \nClone the SCONE_TUTORIAL before you start creating a \nhello world\n image.\n\n\nGenerate HelloWorld image\n\n\nWe generate a \nhello world\n container image. \n\n\n> cd SCONE_TUTORIAL/CreateImage\n\n\n\n\nYou can either execute all step manually by copy&pasting all instructions or you can just execute\n\n\n> docker login\n> sudo ./Dockerfile.sh\n\n\n\n\nand watch the outputs.\n\n\nPlease change the image name to a repository on docker hub to which you can write:\n\n\n> export TAG=\"latest\"\n> export IMAGE_NAME=\"sconecuratedimages/helloworld\"\n\n\n\n\nWe generate container and compile hello world inside of this container with the help of our standard SCONE cross compiler:\n\n\n\n> CONTAINER_ID=`docker run -d -it --device=/dev/isgx  -v $(pwd):/mnt sconecuratedimages/crosscompilers:scone bash -c \"\nset -e\nprintf 'Q 1\\ne 0 0 0\\ns 1 0 0\\n' > /etc/sgx-musl.conf\nsgxmusl-hw-async-gcc /mnt/hello_world.c  -o /usr/local/bin/sgx_hello_world\n\"`\n\n\n\n\nNote that above will fail if you do not have access to the SGX device \n/dev/isgx\n.\n\n\nTurn the container into an image:\n\n\n> IMAGE_ID=$(docker commit -p -c 'CMD sgx_hello_world' $CONTAINER_ID $IMAGE_NAME:$TAG)\n\n\n\n\nYou can run this image by executing:\n\n\n> sudo docker run --device=/dev/isgx $IMAGE_NAME:$TAG\n\n\n\n\nYou can push this image to Docker. However, ensure that you first login to docker:\n\n\n> sudo docker login\n\n\n\n\nbefore you push the image to docker hub:\n\n\n> sudo docker push $IMAGE_NAME:$TAG\n\n\n\n\nNote: this will fail in case you do not have the permission to push to this repository. \n\n\nScreencast\n\n\n\n\n\u00a9 \nscontain.com\n, November 2017. \nQuestions or Suggestions?",
            "title": "SCONE Create Image"
        },
        {
            "location": "/SCONE_GENERATE_IMAGE/#generating-container-image-with-scone",
            "text": "We show how to generate a Docker image that contains our  hello world  running inside of an enclave and pushing this to docker hub. We only show this for the statically-linked binary. You can see that this code is quite awkward. It is much easier to generate images with a Dockerfile - which we show in the next section.",
            "title": "Generating Container Image with SCONE"
        },
        {
            "location": "/SCONE_GENERATE_IMAGE/#prerequisites",
            "text": "Check that all prerequisites from  SCONE Tutorial  are satisfied. \nClone the SCONE_TUTORIAL before you start creating a  hello world  image.",
            "title": "Prerequisites"
        },
        {
            "location": "/SCONE_GENERATE_IMAGE/#generate-helloworld-image",
            "text": "We generate a  hello world  container image.   > cd SCONE_TUTORIAL/CreateImage  You can either execute all step manually by copy&pasting all instructions or you can just execute  > docker login\n> sudo ./Dockerfile.sh  and watch the outputs.  Please change the image name to a repository on docker hub to which you can write:  > export TAG=\"latest\"\n> export IMAGE_NAME=\"sconecuratedimages/helloworld\"  We generate container and compile hello world inside of this container with the help of our standard SCONE cross compiler:  \n> CONTAINER_ID=`docker run -d -it --device=/dev/isgx  -v $(pwd):/mnt sconecuratedimages/crosscompilers:scone bash -c \"\nset -e\nprintf 'Q 1\\ne 0 0 0\\ns 1 0 0\\n' > /etc/sgx-musl.conf\nsgxmusl-hw-async-gcc /mnt/hello_world.c  -o /usr/local/bin/sgx_hello_world\n\"`  Note that above will fail if you do not have access to the SGX device  /dev/isgx .  Turn the container into an image:  > IMAGE_ID=$(docker commit -p -c 'CMD sgx_hello_world' $CONTAINER_ID $IMAGE_NAME:$TAG)  You can run this image by executing:  > sudo docker run --device=/dev/isgx $IMAGE_NAME:$TAG  You can push this image to Docker. However, ensure that you first login to docker:  > sudo docker login  before you push the image to docker hub:  > sudo docker push $IMAGE_NAME:$TAG  Note: this will fail in case you do not have the permission to push to this repository.",
            "title": "Generate HelloWorld image"
        },
        {
            "location": "/SCONE_GENERATE_IMAGE/#screencast",
            "text": "\u00a9  scontain.com , November 2017.  Questions or Suggestions?",
            "title": "Screencast"
        },
        {
            "location": "/SCONE_Dockerfile/",
            "text": "Dockerfile\n\n\nWe show how to generate a first secure container image with the help of a Dockerfile.\n\n\nPrerequisites\n\n\nEnsure that the sgx driver is installed\n\n\n> ls /dev/isgx \n/dev/isgx\n\n\n\n\nIf the driver is not installed, read Section \nSCONE Host Setup\n to learn how to install the SGX driver.\n\n\nEnsure that the patched docker engine is installed\n\n\nWe need \ndocker build\n in this example. This command does not permit to map devices in the newly created containers. Hence, we provide a patched Docker engine \nSCONE Host Setup\n.\n\n\nInstall the tutorial\n\n\nClone the tutorial: \n\n\n> git clone https://github.com/christoffetzer/SCONE_TUTORIAL.git\n\n\n\n\nAccess to SCONE Curated Images\n\n\nRight now, access to the curated images is still restricted. Please, send email to scontain.ceo@gmail.com to request access.\n\n\nGenerate HelloAgain image (dynamically-linked)\n\n\nWe first generate a \nhello again\n container image with a dynamically-linked secure program:\n\n\n> cd SCONE_TUTORIAL/DLDockerFile\n\n\n\n\nThe Dockerfile to generate the new image looks like this:\n\n\nFROM sconecuratedimages/crosscompilers:runtime\n\nMAINTAINER Christof Fetzer \"christof.fetzer@gmail.com\"\n\nRUN mkdir /hello\n\nCOPY dyn_hello_again /hello/\n\n\nCMD SCONE_MODE=HW SCONE_ALPINE=1 SCONE_VERSION=1 /hello/dyn_hello_again\n\n\n\n\nThis assumes that we already generated the dynamically linked binary with\nan appropriately configured gcc. We generate this with the provided gcc image:\n\n\n> docker run --rm  -v \"$PWD\":/usr/src/myapp -w /usr/src/myapp sconecuratedimages/muslgcc gcc  hello_again.c -o dyn_hello_again\n\n\n\n\nWe provide a little script that generates the image and pushes it to Docker hub (which should fail since you should not have the credentials):\n\n\n> ./generate.sh\n\n\n\n\nYou can run this program inside of enclave (with the output of debug messages):\n\n\n> docker run -it sconecuratedimages/helloworld:dynamic\nexport SCONE_QUEUES=4\nexport SCONE_SLOTS=256\nexport SCONE_SIGPIPE=0\nexport SCONE_MMAP32BIT=0\nexport SCONE_SSPINS=100\nexport SCONE_SSLEEP=4000\nexport SCONE_KERNEL=0\nexport SCONE_HEAP=67108864\nexport SCONE_CONFIG=/etc/sgx-musl.conf\nexport SCONE_MODE=hw\nConfigure parameters: \n1.1.15\nHello Again\n\n\n\n\nThis image is nicely small (only 11MB) since it only contains the runtime environment and no development environment.\n\n\nRunning on a docker engine without access to SGX, we get an error message:\n\n\n> docker run -it sconecuratedimages/helloworld:dynamic\n[Error] Could not create enclave: Error opening SGX device \n\n\n\n\nScreencast\n\n\n\n\nGenerate HelloAgain image (statically-linked)\n\n\nWe generate a \nhello again\n container image. \n\n\n> cd SCONE_TUTORIAL/DockerFile\n\n\n\n\nThe Dockerfile is quite straight forward:\n\n\nFROM sconecuratedimages/crosscompilers:scone\n\nMAINTAINER Christof Fetzer \"christof.fetzer@gmail.com\"\n\nRUN mkdir /hello\n\nCOPY hello_again.c /hello/\n\nRUN cd /hello && scone-gcc hello_again.c -o again\n\nCMD [\"/hello/again\"]\n\n\n\n\nYou can either execute all step manually (see below) or you can just execute\n\n\n> docker login\n./generate.sh\n\n\n\n\nand watch the outputs. The push of the image should fail since you should not have the access rights to push the image to Docker hub.\n\n\nWe define the image name and tag that we want to generate:\n\n\nexport TAG=\"again\"\nexport FULLTAG=\"sconecuratedimages/helloworld:$TAG\"\n\n\n\n\nWe build the image:\n\n\n> docker build --pull -t $FULLTAG .\n\n\n\n\n> docker run -it $FULLTAG\n\n\n\n\nWe push it to docker hub (will fail unless you have the right to push \n$FULLTAG\n):\n\n\n> docker push $FULLTAG\n\n\n\n\nPlease change the image name to a repository on docker hub to which you can write:\n\n\n> export TAG=\"latest\"\n> export IMAGE_NAME=\"sconecuratedimages/helloAgain\"\n\n\n\n\nScreencast\n\n\n\n\n\u00a9 \nscontain.com\n, November 2017. \nQuestions or Suggestions?",
            "title": "SCONE Dockerfile"
        },
        {
            "location": "/SCONE_Dockerfile/#dockerfile",
            "text": "We show how to generate a first secure container image with the help of a Dockerfile.",
            "title": "Dockerfile"
        },
        {
            "location": "/SCONE_Dockerfile/#prerequisites",
            "text": "Ensure that the sgx driver is installed  > ls /dev/isgx \n/dev/isgx  If the driver is not installed, read Section  SCONE Host Setup  to learn how to install the SGX driver.  Ensure that the patched docker engine is installed  We need  docker build  in this example. This command does not permit to map devices in the newly created containers. Hence, we provide a patched Docker engine  SCONE Host Setup .  Install the tutorial  Clone the tutorial:   > git clone https://github.com/christoffetzer/SCONE_TUTORIAL.git  Access to SCONE Curated Images  Right now, access to the curated images is still restricted. Please, send email to scontain.ceo@gmail.com to request access.",
            "title": "Prerequisites"
        },
        {
            "location": "/SCONE_Dockerfile/#generate-helloagain-image-dynamically-linked",
            "text": "We first generate a  hello again  container image with a dynamically-linked secure program:  > cd SCONE_TUTORIAL/DLDockerFile  The Dockerfile to generate the new image looks like this:  FROM sconecuratedimages/crosscompilers:runtime\n\nMAINTAINER Christof Fetzer \"christof.fetzer@gmail.com\"\n\nRUN mkdir /hello\n\nCOPY dyn_hello_again /hello/\n\n\nCMD SCONE_MODE=HW SCONE_ALPINE=1 SCONE_VERSION=1 /hello/dyn_hello_again  This assumes that we already generated the dynamically linked binary with\nan appropriately configured gcc. We generate this with the provided gcc image:  > docker run --rm  -v \"$PWD\":/usr/src/myapp -w /usr/src/myapp sconecuratedimages/muslgcc gcc  hello_again.c -o dyn_hello_again  We provide a little script that generates the image and pushes it to Docker hub (which should fail since you should not have the credentials):  > ./generate.sh  You can run this program inside of enclave (with the output of debug messages):  > docker run -it sconecuratedimages/helloworld:dynamic\nexport SCONE_QUEUES=4\nexport SCONE_SLOTS=256\nexport SCONE_SIGPIPE=0\nexport SCONE_MMAP32BIT=0\nexport SCONE_SSPINS=100\nexport SCONE_SSLEEP=4000\nexport SCONE_KERNEL=0\nexport SCONE_HEAP=67108864\nexport SCONE_CONFIG=/etc/sgx-musl.conf\nexport SCONE_MODE=hw\nConfigure parameters: \n1.1.15\nHello Again  This image is nicely small (only 11MB) since it only contains the runtime environment and no development environment.  Running on a docker engine without access to SGX, we get an error message:  > docker run -it sconecuratedimages/helloworld:dynamic\n[Error] Could not create enclave: Error opening SGX device   Screencast",
            "title": "Generate HelloAgain image (dynamically-linked)"
        },
        {
            "location": "/SCONE_Dockerfile/#generate-helloagain-image-statically-linked",
            "text": "We generate a  hello again  container image.   > cd SCONE_TUTORIAL/DockerFile  The Dockerfile is quite straight forward:  FROM sconecuratedimages/crosscompilers:scone\n\nMAINTAINER Christof Fetzer \"christof.fetzer@gmail.com\"\n\nRUN mkdir /hello\n\nCOPY hello_again.c /hello/\n\nRUN cd /hello && scone-gcc hello_again.c -o again\n\nCMD [\"/hello/again\"]  You can either execute all step manually (see below) or you can just execute  > docker login\n./generate.sh  and watch the outputs. The push of the image should fail since you should not have the access rights to push the image to Docker hub.  We define the image name and tag that we want to generate:  export TAG=\"again\"\nexport FULLTAG=\"sconecuratedimages/helloworld:$TAG\"  We build the image:  > docker build --pull -t $FULLTAG .  > docker run -it $FULLTAG  We push it to docker hub (will fail unless you have the right to push  $FULLTAG ):  > docker push $FULLTAG  Please change the image name to a repository on docker hub to which you can write:  > export TAG=\"latest\"\n> export IMAGE_NAME=\"sconecuratedimages/helloAgain\"  Screencast   \u00a9  scontain.com , November 2017.  Questions or Suggestions?",
            "title": "Generate HelloAgain image (statically-linked)"
        },
        {
            "location": "/SCONE_Swarm_Example/",
            "text": "Starting a SCONE Application on a Swarm\n\n\nWe show how to run a \nsecure nginx\n version, i.e., one that runs inside an enclave in a\ndocker swarm with automatic restarts. To simplify the running of services, we provide \na simple wrapper around the \ndocker service\n command: the \nscone service\n executes\n\ndocker service\n commands on the manager of a swarm. The manager is either specified via\nan option \n--manager \n or via environment variable \nSCONE_MANAGER\n. This is \ndone in the same way as for command \nscone swarm\n.\n\n\nIn what follows, we assume that \nSCONE_MANAGER\n is set to the leader of the swarm.\nThe \nscone\n commands are typically executed in a container running at the \ndeveloper\nsite\n.\n\n\nPrerequisites\n\n\nRegistry support\n\n\nFor running an application in a Docker Swarm, you need to set up a local registry to\nensure that all nodes get access to the same container image. The scone CLI expects the\nregistry to be available at \nlocalhost:5000\n. You can start a default registry with the\nhelp of \nscone\n:\n\n\n$ scone service registry --verbose\nRegistry is already running in swarm beatrix\n\n\n\n\nTo simplify pushing images to the local registry, the scone CLI includes a \nscone service pull\n command to pull\nan image from docker hub and then to push this image to the local registry. For example, to pull image\n\nsconecuratedimages/sconetainer:noshielding\n and store it as \nlocalhost:5000/sconetainer:noshielding\n,\njust execute:\n\n\n$ scone service pull sconecuratedimages/sconetainer:noshielding\nnew tag: localhost:5000/sconetainer:noshielding\n\n\n\n\nSGX Support\n\n\nServices are automatically restarted. In case, there is a persistent failure in some service \nha\n, we would see\nrepeated restarts like: \n\n\n$ scone service ps ha\nID                  NAME                IMAGE                      NODE                DESIRED STATE       CURRENT STATE           ERROR                       PORTS\nf65id6ow5n6w        ha.1                sconecuratedimages/nginx   beatrix             Ready               Ready 1 second ago                                  \njt6wj5e3lso4         \\_ ha.1            sconecuratedimages/nginx   beatrix             Shutdown            Failed 3 seconds ago    \"task: non-zero exit (1)\"   \nsspou3mcis8m         \\_ ha.1            sconecuratedimages/nginx   beatrix             Shutdown            Failed 9 seconds ago    \"task: non-zero exit (1)\"   \np3bw780pu63b         \\_ ha.1            sconecuratedimages/nginx   beatrix             Shutdown            Failed 15 seconds ago   \"task: non-zero exit (1)\"   \n75zjsesil5k4         \\_ ha.1            sconecuratedimages/nginx   beatrix             Shutdown            Failed 22 seconds ago   \"task: non-zero exit (1)\"   \n\n\n\n\nReasons for such failures might be that that the containers might not have access to the sgx device. \n\n\nThere are multiple reasons why the driver might not be accessible inside of a container: Did you indeed install the patched docker version? Did you indeed label the nodes correctly? To automatically diagnoses and in some cases, to perform some automatic corrections, just execute \nscone swarm check\n:\n\n\n$ scone swarm check\nwarning:  'sgx device is not automatically mapped inside of container on host beatrix (stack=198 434 0)'  (Line numer: '198')\nwarning:  '--device=/dev/isgx: device mapper does not work inside of container on host beatrix (stack=199 434 0)'  (Line numer: '199')\n\n\n\n\nTo get a summary view of a swarm after you performed a check, just executed:\n\n\n$ scone swarm ls\nNODENO        SGX VERSION   DOCKER-ENGINE SGX-DRIVER    HOST                 STATUS     AVAILABILITY  MANAGER   \n2             1             SCONE         SCONE         caroline             Ready      Active        Reachable \n3             1             SCONE         SCONE         dorothy              Ready      Active                  \n4             1             SCONE         SCONE         edna                 Ready      Active        Reachable \n1             1             SCONE         SCONE         beatrix              Ready      Active        Leader    \n\n\n\n\nStarting a Service\n\n\nAfter pulling an image into the local registry (see above), we can\nstart a service in the swarm via \nscone service create\n.\nDocker swarm will start the image and it also takes care of failures by restarting failed services.\n\n\nFor the next steps, make sure that all nodes have access to image \nsconecuratedimages/sconetainer:noshielding\n and pull this image via:\n\n\n$ scone service pull sconecuratedimages/sconetainer:noshielding\nnew tag: localhost:5000/sconetainer:noshielding\n\n\n\n\nWe start a nginx service including a version of the scontain.com website. We start two replicas running inside separate enclaves - most likely on two different nodes:\n\n\n$ scone service create --name sconeweb --detach=true  --publish 80:80 --publish 443:443 --replicas=2 localhost:5000/sconetainer:noshielding\n\n\n\n\nIn case the service starts up correctly, you will see a status like this:\n\n\n$ scone service ps sconeweb\nID                  NAME           IMAGE                              NODE                DESIRED STATE       CURRENT STATE            ERROR                              PORTS\nba3odjkz6mx2        sconeweb.1     localhost:5000/nginx:noshielding   alice               Running             Running 8 minutes ago         \nx2xq1c3aede7        sconeweb.2     localhost:5000/nginx:noshielding   beatrix               Running             Running 8 minutes ago         \n\n\n\n\nIf, for example, an image is not available on all nodes, you might see the following status:\n\n\n$ scone service ps sconeweb\nID                  NAME                IMAGE                                        NODE                DESIRED STATE       CURRENT STATE          ERROR                              PORTS\no79714pw2fpn        sconeweb.1          sconecuratedimages/sconetainer:noshielding   alice               Running             Running 4 hours ago                                       \nt0byepte0fzj         \\_ sconeweb.1      sconecuratedimages/sconetainer:noshielding   beatrix             Shutdown            Rejected 4 hours ago   \"No such image: sconecuratedim\u2026\"   \nmg4xdq868syq         \\_ sconeweb.1      sconecuratedimages/sconetainer:noshielding   beatrix             Shutdown            Rejected 4 hours ago   \"No such image: sconecuratedim\u2026\"   \nry1pqen9jgan         \\_ sconeweb.1      sconecuratedimages/sconetainer:noshielding   beatrix             Shutdown            Rejected 4 hours ago   \"No such image: sconecuratedim\u2026\"   \nq05ti7gkxc7r         \\_ sconeweb.1      sconecuratedimages/sconetainer:noshielding   beatrix             Shutdown            Rejected 4 hours ago   \"No such image: sconecuratedim\u2026\"   \nzxj74inh2zdf        sconeweb.2          sconecuratedimages/sconetainer:noshielding   alice               Running             Running 4 hours ago                                       \n\n\n\n\nStopping the service\n\n\nStop the service via:\n\n\n$ scone service rm sconeweb\n\n\n\n\nUpdating the image of a service\n\n\nSay, there is a new version of the sconetainer image available. We can update this image in our local registry\nas follows:\n\n\n$ scone pull sconecuratedimages/sconetainer:noshielding\n\n\n\n\nWe can now update the service as follows:\n\n\n$ scone service update --image  localhost:5000/sconetainer sconeweb\n\n\n\n\nDraining a node\n\n\nTo be able to drain all containers from a node, we need to figure out the node's id. We can do this manually by executing the following command on the leader node:\n\n\n> sudo docker node ls\nID                            HOSTNAME            STATUS              AVAILABILITY        MANAGER STATUS\n91a1vvex4dgozfrzy1y136gmg *   alice               Ready               Active              Leader\njhrayos9ylu02egwvkxpqtbwb     beatrix             Ready               Active              \n\n\n\n\nYou can now take node \nalice\n out of service by executing:\n\n\n> sudo docker node update --availability drain 91a1vvex4dgozfrzy1y136gmg\n\n\n\n\nTo put the node back in service by executing:\n\n\n> sudo docker node update --availability active 91a1vvex4dgozfrzy1y136gmg\n\n\n\n\n\u00a9 \nscontain.com\n, November 2017. \nQuestions or Suggestions?",
            "title": "SCONE Swarm Example"
        },
        {
            "location": "/SCONE_Swarm_Example/#starting-a-scone-application-on-a-swarm",
            "text": "We show how to run a  secure nginx  version, i.e., one that runs inside an enclave in a\ndocker swarm with automatic restarts. To simplify the running of services, we provide \na simple wrapper around the  docker service  command: the  scone service  executes docker service  commands on the manager of a swarm. The manager is either specified via\nan option  --manager   or via environment variable  SCONE_MANAGER . This is \ndone in the same way as for command  scone swarm .  In what follows, we assume that  SCONE_MANAGER  is set to the leader of the swarm.\nThe  scone  commands are typically executed in a container running at the  developer\nsite .",
            "title": "Starting a SCONE Application on a Swarm"
        },
        {
            "location": "/SCONE_Swarm_Example/#prerequisites",
            "text": "Registry support  For running an application in a Docker Swarm, you need to set up a local registry to\nensure that all nodes get access to the same container image. The scone CLI expects the\nregistry to be available at  localhost:5000 . You can start a default registry with the\nhelp of  scone :  $ scone service registry --verbose\nRegistry is already running in swarm beatrix  To simplify pushing images to the local registry, the scone CLI includes a  scone service pull  command to pull\nan image from docker hub and then to push this image to the local registry. For example, to pull image sconecuratedimages/sconetainer:noshielding  and store it as  localhost:5000/sconetainer:noshielding ,\njust execute:  $ scone service pull sconecuratedimages/sconetainer:noshielding\nnew tag: localhost:5000/sconetainer:noshielding  SGX Support  Services are automatically restarted. In case, there is a persistent failure in some service  ha , we would see\nrepeated restarts like:   $ scone service ps ha\nID                  NAME                IMAGE                      NODE                DESIRED STATE       CURRENT STATE           ERROR                       PORTS\nf65id6ow5n6w        ha.1                sconecuratedimages/nginx   beatrix             Ready               Ready 1 second ago                                  \njt6wj5e3lso4         \\_ ha.1            sconecuratedimages/nginx   beatrix             Shutdown            Failed 3 seconds ago    \"task: non-zero exit (1)\"   \nsspou3mcis8m         \\_ ha.1            sconecuratedimages/nginx   beatrix             Shutdown            Failed 9 seconds ago    \"task: non-zero exit (1)\"   \np3bw780pu63b         \\_ ha.1            sconecuratedimages/nginx   beatrix             Shutdown            Failed 15 seconds ago   \"task: non-zero exit (1)\"   \n75zjsesil5k4         \\_ ha.1            sconecuratedimages/nginx   beatrix             Shutdown            Failed 22 seconds ago   \"task: non-zero exit (1)\"     Reasons for such failures might be that that the containers might not have access to the sgx device.   There are multiple reasons why the driver might not be accessible inside of a container: Did you indeed install the patched docker version? Did you indeed label the nodes correctly? To automatically diagnoses and in some cases, to perform some automatic corrections, just execute  scone swarm check :  $ scone swarm check\nwarning:  'sgx device is not automatically mapped inside of container on host beatrix (stack=198 434 0)'  (Line numer: '198')\nwarning:  '--device=/dev/isgx: device mapper does not work inside of container on host beatrix (stack=199 434 0)'  (Line numer: '199')  To get a summary view of a swarm after you performed a check, just executed:  $ scone swarm ls\nNODENO        SGX VERSION   DOCKER-ENGINE SGX-DRIVER    HOST                 STATUS     AVAILABILITY  MANAGER   \n2             1             SCONE         SCONE         caroline             Ready      Active        Reachable \n3             1             SCONE         SCONE         dorothy              Ready      Active                  \n4             1             SCONE         SCONE         edna                 Ready      Active        Reachable \n1             1             SCONE         SCONE         beatrix              Ready      Active        Leader",
            "title": "Prerequisites"
        },
        {
            "location": "/SCONE_Swarm_Example/#starting-a-service",
            "text": "After pulling an image into the local registry (see above), we can\nstart a service in the swarm via  scone service create .\nDocker swarm will start the image and it also takes care of failures by restarting failed services.  For the next steps, make sure that all nodes have access to image  sconecuratedimages/sconetainer:noshielding  and pull this image via:  $ scone service pull sconecuratedimages/sconetainer:noshielding\nnew tag: localhost:5000/sconetainer:noshielding  We start a nginx service including a version of the scontain.com website. We start two replicas running inside separate enclaves - most likely on two different nodes:  $ scone service create --name sconeweb --detach=true  --publish 80:80 --publish 443:443 --replicas=2 localhost:5000/sconetainer:noshielding  In case the service starts up correctly, you will see a status like this:  $ scone service ps sconeweb\nID                  NAME           IMAGE                              NODE                DESIRED STATE       CURRENT STATE            ERROR                              PORTS\nba3odjkz6mx2        sconeweb.1     localhost:5000/nginx:noshielding   alice               Running             Running 8 minutes ago         \nx2xq1c3aede7        sconeweb.2     localhost:5000/nginx:noshielding   beatrix               Running             Running 8 minutes ago           If, for example, an image is not available on all nodes, you might see the following status:  $ scone service ps sconeweb\nID                  NAME                IMAGE                                        NODE                DESIRED STATE       CURRENT STATE          ERROR                              PORTS\no79714pw2fpn        sconeweb.1          sconecuratedimages/sconetainer:noshielding   alice               Running             Running 4 hours ago                                       \nt0byepte0fzj         \\_ sconeweb.1      sconecuratedimages/sconetainer:noshielding   beatrix             Shutdown            Rejected 4 hours ago   \"No such image: sconecuratedim\u2026\"   \nmg4xdq868syq         \\_ sconeweb.1      sconecuratedimages/sconetainer:noshielding   beatrix             Shutdown            Rejected 4 hours ago   \"No such image: sconecuratedim\u2026\"   \nry1pqen9jgan         \\_ sconeweb.1      sconecuratedimages/sconetainer:noshielding   beatrix             Shutdown            Rejected 4 hours ago   \"No such image: sconecuratedim\u2026\"   \nq05ti7gkxc7r         \\_ sconeweb.1      sconecuratedimages/sconetainer:noshielding   beatrix             Shutdown            Rejected 4 hours ago   \"No such image: sconecuratedim\u2026\"   \nzxj74inh2zdf        sconeweb.2          sconecuratedimages/sconetainer:noshielding   alice               Running             Running 4 hours ago",
            "title": "Starting a Service"
        },
        {
            "location": "/SCONE_Swarm_Example/#stopping-the-service",
            "text": "Stop the service via:  $ scone service rm sconeweb",
            "title": "Stopping the service"
        },
        {
            "location": "/SCONE_Swarm_Example/#updating-the-image-of-a-service",
            "text": "Say, there is a new version of the sconetainer image available. We can update this image in our local registry\nas follows:  $ scone pull sconecuratedimages/sconetainer:noshielding  We can now update the service as follows:  $ scone service update --image  localhost:5000/sconetainer sconeweb",
            "title": "Updating the image of a service"
        },
        {
            "location": "/SCONE_Swarm_Example/#draining-a-node",
            "text": "To be able to drain all containers from a node, we need to figure out the node's id. We can do this manually by executing the following command on the leader node:  > sudo docker node ls\nID                            HOSTNAME            STATUS              AVAILABILITY        MANAGER STATUS\n91a1vvex4dgozfrzy1y136gmg *   alice               Ready               Active              Leader\njhrayos9ylu02egwvkxpqtbwb     beatrix             Ready               Active                You can now take node  alice  out of service by executing:  > sudo docker node update --availability drain 91a1vvex4dgozfrzy1y136gmg  To put the node back in service by executing:  > sudo docker node update --availability active 91a1vvex4dgozfrzy1y136gmg  \u00a9  scontain.com , November 2017.  Questions or Suggestions?",
            "title": "Draining a node"
        },
        {
            "location": "/SCONE_Compose/",
            "text": "Compose / Stack Files\n\n\nSCONE supports to run secure applications  consisting of multiple secure containers. To do so, SCONE introduces slightly extended Docker \ncompose\n file. Such an extended compose file defines for each process that runs inside of an enclave, a unique hash value (\nMRENCLAVE\n). During startup, SCONE performs an \nattestation\n for all these \nsecure processes\n to ensure that the hash of the started program is as expected, i.e., is equal to \nMRENCLAVE\n. Only if it is equal, the arguments and the environment variables are passed to the process. In this way, we can pass secrets as arguments or environment variables in a secure fashion to a secure process.\n\n\nBy default, all containers are started with SCONE and the arguments are passed in a secure fashion to the started processes. However, you might not want to run all processes inside of enclaves. For containers that should be directly started with Docker Swarm, you need to set field \nnot_scone: \"true\"\n. In this case, all arguments and the environment variables are directly passed to the container with Docker Swarm (instead of SCONE).\n\n\nLet's consider an example, that consists of a \nhaproxy\n that runs in native mode and is directly started with Docker Swarm (indicated by line \nnot_scone: \"true\"\n) Moreover, we start \nmyapp\n (defined in container image \nmyapp-image\n) and specify \nMRENCLAVE\n. Only after ensuring that the program started in the enclave has the expected \nMRENCLAVE\n the arguments and the environment variables are passed to \nmyapp\n.\n\n\nversion: \"3.1.scone\"\nservices:\n    primary-service:\n        image: myapp-image:latest\n        command: /myapp arg1 arg2 arg3 $my_password\n        mrenclave: 5764436f08dd4cdb526f082be1a07a3422f79ef2b01a5e24f78f9034a838c335\n        environment:\n            - SECURE_ENV=value\n            - MY_PIN=$my_pin\n            - MY_PASSWORD=$my_password\n        working_dir: /\n    proxy:\n        image: haproxy\n        command: haproxy --read_config_from_environ\n        not_scone: \"true\"\n        environment:\n            - \"HAPROXY_CONFIG=a=b,c=d,3=4\"\nsecrets:\n    my_pin:\n        kind: numeric\n        length: \"4\"\n    my_password:\n        kind: ascii\n        length: \"8\"\n\n\n\n\nThe extended compose file is split by SCONE into a standard compose file and a configuration information that is stored in the SCONE \nConfigruation and Attestation Service\n (\nCAS\n):\n\n\n\n\nIn the current version of scone, we assume that the compose file is stored on a trusted host. (We plan to support compose files stored on untrusted host in the near future.)\n\n\nThe \nsplit\n functionality is part of the \nscone CLI\n, i.e., you can split a compose file via\n\n\n$ scone cas split <COMPOSE_FILE>\n\n\n\n\nThis creates a stack file that can be used to start a set of services in your Swarm. Read \nnginx example\n for some more details how to do this.\n\n\nTo be able to use a CAS, you are required to login to a CAS. We assume that your CAS is available at IP address \n$IP\n. If you have set up the CAS in the way specified in \nCAS Setup\n, set environment variable IP to the external IP address of the swarm manager node.\n\n\nYou can login into CAS via:\n\n\n$ scone cas login --host $IP:8081:18765\n\n\n\n\nSee \nnginx example\n for some more details and read \nCAS Setup\n to learn how to run a CAS service for development.\n\n\n\u00a9 \nscontain.com\n, November 2017. \nQuestions or Suggestions?",
            "title": "SCONE Compose"
        },
        {
            "location": "/SCONE_Compose/#compose-stack-files",
            "text": "SCONE supports to run secure applications  consisting of multiple secure containers. To do so, SCONE introduces slightly extended Docker  compose  file. Such an extended compose file defines for each process that runs inside of an enclave, a unique hash value ( MRENCLAVE ). During startup, SCONE performs an  attestation  for all these  secure processes  to ensure that the hash of the started program is as expected, i.e., is equal to  MRENCLAVE . Only if it is equal, the arguments and the environment variables are passed to the process. In this way, we can pass secrets as arguments or environment variables in a secure fashion to a secure process.  By default, all containers are started with SCONE and the arguments are passed in a secure fashion to the started processes. However, you might not want to run all processes inside of enclaves. For containers that should be directly started with Docker Swarm, you need to set field  not_scone: \"true\" . In this case, all arguments and the environment variables are directly passed to the container with Docker Swarm (instead of SCONE).  Let's consider an example, that consists of a  haproxy  that runs in native mode and is directly started with Docker Swarm (indicated by line  not_scone: \"true\" ) Moreover, we start  myapp  (defined in container image  myapp-image ) and specify  MRENCLAVE . Only after ensuring that the program started in the enclave has the expected  MRENCLAVE  the arguments and the environment variables are passed to  myapp .  version: \"3.1.scone\"\nservices:\n    primary-service:\n        image: myapp-image:latest\n        command: /myapp arg1 arg2 arg3 $my_password\n        mrenclave: 5764436f08dd4cdb526f082be1a07a3422f79ef2b01a5e24f78f9034a838c335\n        environment:\n            - SECURE_ENV=value\n            - MY_PIN=$my_pin\n            - MY_PASSWORD=$my_password\n        working_dir: /\n    proxy:\n        image: haproxy\n        command: haproxy --read_config_from_environ\n        not_scone: \"true\"\n        environment:\n            - \"HAPROXY_CONFIG=a=b,c=d,3=4\"\nsecrets:\n    my_pin:\n        kind: numeric\n        length: \"4\"\n    my_password:\n        kind: ascii\n        length: \"8\"  The extended compose file is split by SCONE into a standard compose file and a configuration information that is stored in the SCONE  Configruation and Attestation Service  ( CAS ):   In the current version of scone, we assume that the compose file is stored on a trusted host. (We plan to support compose files stored on untrusted host in the near future.)  The  split  functionality is part of the  scone CLI , i.e., you can split a compose file via  $ scone cas split <COMPOSE_FILE>  This creates a stack file that can be used to start a set of services in your Swarm. Read  nginx example  for some more details how to do this.  To be able to use a CAS, you are required to login to a CAS. We assume that your CAS is available at IP address  $IP . If you have set up the CAS in the way specified in  CAS Setup , set environment variable IP to the external IP address of the swarm manager node.  You can login into CAS via:  $ scone cas login --host $IP:8081:18765  See  nginx example  for some more details and read  CAS Setup  to learn how to run a CAS service for development.  \u00a9  scontain.com , November 2017.  Questions or Suggestions?",
            "title": "Compose / Stack Files"
        },
        {
            "location": "/SCONE_CAS/",
            "text": "SCONE CAS\n\n\nSCONE CAS (\nConfiguration and Attestation Service\n) helps to securely configure secure services. A CAS helps to provide services running inside of enclaves with\n\n\n\n\ntheir command line arguments\n\n\ntheir environment variables\n\n\n\n\nafter the service was attested by the CAS. \n\n\nTo attest a service in a swarm, the CAS requires the help of a local attestation service (LAS).\n\n\nAs part of the SCONE Enterprise Version, you can run your own CAS and LAS infrastructure.\n To do so, we provide you with container images to simplify the execution of CAS and LAS. For other SCONE versions, we can provide you with access to a global CAS.\n\n\nTo set up a development environment, you can perform the following steps.\n\n\nTo run LAS on all nodes of a cluster managed by node \nfaye\n, first pull the newest LAS image and then execute on all nodes of a SWARM:\n\n\n$ export SCONE_MANAGER=faye\n$ scone service pull sconecuratedimages/sconetainer:las\n$ scone service create --detach=true --mode global --publish mode=host,target=18766,published=18766 --name=las localhost:5000/sconetainer:las\n\n\n\n\nCheck that the service \nlas\n is indeed running, check with command \nps\n:\n\n\n$ scone service ps las\nID                  NAME                            IMAGE                            NODE                DESIRED STATE       CURRENT STATE           ERROR               PORTS\nxvnsruar64qx        las.q26sp44pp12uf81zlyhb5pnxf   localhost:5000/sconetainer:las   edna                Running             Running 3 minutes ago                       *:18766->18766/tcp\nzfgx4t292ew6        las.cr3bxhy0nqmg77goxaih5sw8d   localhost:5000/sconetainer:las   dorothy             Running             Running 3 minutes ago                       *:18766->18766/tcp\nn441o9qqmnwa        las.os7ihjrel2jb4bplcn81h7f0i   localhost:5000/sconetainer:las   faye                Running             Running 3 minutes ago                       *:18766->18766/tcp\n\n\n\n\nNow, we can start the CAS service as follows. We first pull the newest\nCAS image and run it on one node of the swarm.\n\n\n$ scone service pull sconecuratedimages/sconetainer:cas\n$ scone service create --name cas --detach=true  --publish 8081:8081 --publish 18765:18765 localhost:5000/sconetainer:cas\n\n\n\n\nLet's check that CAS is indeed running:\n\n\n$ scone service ps cas\nID                  NAME                IMAGE                            NODE                DESIRED STATE       CURRENT STATE         ERROR               PORTS\ne6b0skuph9ve        cas.1               localhost:5000/sconetainer:cas   edna                Running             Running 3 hours ago                       \n\n\n\n\n\u00a9 \nscontain.com\n, December 2017. \nQuestions or Suggestions?",
            "title": "SCONE CAS"
        },
        {
            "location": "/SCONE_CAS/#scone-cas",
            "text": "SCONE CAS ( Configuration and Attestation Service ) helps to securely configure secure services. A CAS helps to provide services running inside of enclaves with   their command line arguments  their environment variables   after the service was attested by the CAS.   To attest a service in a swarm, the CAS requires the help of a local attestation service (LAS).  As part of the SCONE Enterprise Version, you can run your own CAS and LAS infrastructure.  To do so, we provide you with container images to simplify the execution of CAS and LAS. For other SCONE versions, we can provide you with access to a global CAS.  To set up a development environment, you can perform the following steps.  To run LAS on all nodes of a cluster managed by node  faye , first pull the newest LAS image and then execute on all nodes of a SWARM:  $ export SCONE_MANAGER=faye\n$ scone service pull sconecuratedimages/sconetainer:las\n$ scone service create --detach=true --mode global --publish mode=host,target=18766,published=18766 --name=las localhost:5000/sconetainer:las  Check that the service  las  is indeed running, check with command  ps :  $ scone service ps las\nID                  NAME                            IMAGE                            NODE                DESIRED STATE       CURRENT STATE           ERROR               PORTS\nxvnsruar64qx        las.q26sp44pp12uf81zlyhb5pnxf   localhost:5000/sconetainer:las   edna                Running             Running 3 minutes ago                       *:18766->18766/tcp\nzfgx4t292ew6        las.cr3bxhy0nqmg77goxaih5sw8d   localhost:5000/sconetainer:las   dorothy             Running             Running 3 minutes ago                       *:18766->18766/tcp\nn441o9qqmnwa        las.os7ihjrel2jb4bplcn81h7f0i   localhost:5000/sconetainer:las   faye                Running             Running 3 minutes ago                       *:18766->18766/tcp  Now, we can start the CAS service as follows. We first pull the newest\nCAS image and run it on one node of the swarm.  $ scone service pull sconecuratedimages/sconetainer:cas\n$ scone service create --name cas --detach=true  --publish 8081:8081 --publish 18765:18765 localhost:5000/sconetainer:cas  Let's check that CAS is indeed running:  $ scone service ps cas\nID                  NAME                IMAGE                            NODE                DESIRED STATE       CURRENT STATE         ERROR               PORTS\ne6b0skuph9ve        cas.1               localhost:5000/sconetainer:cas   edna                Running             Running 3 hours ago                         \u00a9  scontain.com , December 2017.  Questions or Suggestions?",
            "title": "SCONE CAS"
        },
        {
            "location": "/SCONE_EE2EE/",
            "text": "Example: End-to-End Encryption\n\n\nWe show how to encrypt an application end-to-end, i.e., all the data in the file system is encrypted, all data in main memory is encrypted and all data on the wire is encrypted.\n\n\nIn what follows, we only interact with one swarm. Hence, we set environment variable \nSCONE_MANAGER\n to point to the manager node of the swarm. Say, in our case the manage is called \nfaye\n:\n\n\n$ export SCONE_MANAGER=faye\n\n\n\n\nWe have prepared an nginx image that contains this website as encrypted files and nginx is running inside of an enclave reading the encrypted files\nfrom the file system and decrypting these files inside the enclave.\n\n\nLet's assume that we have the newest image in our local swarm registry:\n\n\n$ scone service pull sconecuratedimages/sconetainer:fss\n...\nfss: digest: sha256:2e76b4a0b090cc75c2e56594e20c7ec36bc1abd13e36e2f00d36e2f67b13c1d5 size: 1783\nnew tag: localhost:5000/sconetainer:fss\n\n\n\n\nWe will start this service as a stack. Hence, we define a simple stack/compose file:\n\n\n$ cat > compose.yml << EOF\nversion: \"3.1.scone\"\nservices:\n    nginx:\n        image: 127.0.0.1:5000/sconetainer:fss\n        command: nginx -p /nginx -c nginx.conf\n        mrenclave: 1516e3d41590cf3842282c6f037535af64336138a6b5eff7e8754e97b4c64ecb\n        fspf_path: /nginx/fspf.pb\n        fspf_key: 970f4925bb7b221461f3d1a3f17450aa42844539de24f5acc1b45b8c140f9467\n        fspf_tag: 5930bffbd9ea2f1317e6872b032334db\n        working_dir: /\n        ports:\n          - 8190:8080\n          - 8192:8082\nEOF\n\n\n\n\nFor now, this file contains some metadata related to the encrypted files:\n\n\n\n\nfspf_path\n the path of the \nfile system protection file\n\n\nfspf_key\n the key used to encrypt the \nfile system protection file\n\n\nfspf_tag\n the tag (i.e., MAC) of the \nfile system protection file\n\n\n\n\nWe provide a low level for encrypting files with the help of \nscone fspf\n. (We will soon release a higher level support that will simplify the encryption of files). \n\n\nIn this swarm, we have a CAS (Configuration and Attestation Service running): the CAS helps\nus to to pass some secrets like the \nfspf_key\n to an enclave and protecting both\nthe confidentiality as well as the integrity of this secret.\n\n\nClients can identify the CAS via its certificate. In this example,\nwe explicitly specify the certificate of the CAS:\n\n\n$ cat > ca.pem << EOF\n-----BEGIN CERTIFICATE-----\nMIICKDCCAc2gAwIBAgIJAMnmUBMT+gOGMAoGCCqGSM49BAMCMHAxCzAJBgNVBAYT\nAlVTMQ8wDQYDVQQIDAZPcmVnb24xETAPBgNVBAcMCFBvcnRsYW5kMRUwEwYDVQQK\nDAxDb21wYW55IE5hbWUxDDAKBgNVBAsMA09yZzEYMBYGA1UEAwwPd3d3LmV4YW1w\nbGUuY29tMB4XDTE3MDQwNzEzNTIyMloXDTE4MDQwNzEzNTIyMlowcDELMAkGA1UE\nBhMCVVMxDzANBgNVBAgMBk9yZWdvbjERMA8GA1UEBwwIUG9ydGxhbmQxFTATBgNV\nBAoMDENvbXBhbnkgTmFtZTEMMAoGA1UECwwDT3JnMRgwFgYDVQQDDA93d3cuZXhh\nbXBsZS5jb20wWTATBgcqhkjOPQIBBggqhkjOPQMBBwNCAARvkkSOfCGXIyWCCdfj\nMil6dhw/a2ZsPaBKOl1RS5MZuoN+7gtSqgnaT5KrT4S/Wj7flP+PJqkUf0M6p1gg\nE5wBo1AwTjAdBgNVHQ4EFgQUzPP33ZM7ZMQ7UDb9THNM5Yoma3YwHwYDVR0jBBgw\nFoAUzPP33ZM7ZMQ7UDb9THNM5Yoma3YwDAYDVR0TBAUwAwEB/zAKBggqhkjOPQQD\nAgNJADBGAiEA34tSQI8V+4UaQG1w7nVJEDLWA17zw5Ls8NZBYVi4lpcCIQDgzln/\n1r25fhGzrVbRgAcbmfLsJxuDYpsfAaNL5/5leg==\n-----END CERTIFICATE-----\nEOF\n\n\n\n\nand now log into the CAS:\n\n\n$ export IP=x.y.z.u\n$ scone cas login -c ca.pem christof cas --host $IP:8081:18765\n\n\n\n\nWe split our stack file into two parts: public part sent to the docker engine and a secret part that is directly sent to the CAS:\n\n\n$ scone cas split compose.yml  --stack 283299\n\n\n\n\nShow public part that is sent to \ndocker stack\n (which is the original stack file name appended with \"docker.yml\"):\n\n\n$ cat compose.yml.docker.yml\n---\nservices: \n  nginx: \n    command: nginx\n    environment: \n      SCONE_CAS_ADDR: \"x.y.z.u:18765\"\n      SCONE_CONFIG_ID: christof/283299/nginx\n      SCONE_LAS_ADDR: \"172.17.0.1:18766\"\n    image: \"127.0.0.1:5000/sconetainer:fss\"\n    ports: \n      - \"8190:8080\"\n      - \"8192:8082\"\nversion: \"3.1\"\n\n\n\n\nWe can now deploy the service with the help of docker/scone stack:\n\n\n$ scone stack deploy --compose-file compose.yml  nginx\nCreating network nginx_default\nCreating service nginx_nginx\n\n\n\n\nShow running stack:\n\n\n$ scone stack ls\nNAME        SERVICES\nnginx       1\n\n\n\n\nWe can get some more information about this stack:\n\n\n$ scone stack ps nginx\nID                  NAME                IMAGE                            NODE                DESIRED STATE       CURRENT STATE             ERROR                       PORTS\nh08r5hmeu9bt        nginx_nginx.1       127.0.0.1:5000/sconetainer:fss   dorothy             Running             Starting 19 seconds ago       \n\n\n\n\nFile Encryption\n\n\nLet's check that the files inside of nginx container are indeed encrypted. To do so, we ssh to node  \ndorothy\n and execute the following commands:\n\n\n> sudo docker ps\nCONTAINER ID        IMAGE                                    COMMAND                  CREATED             STATUS              PORTS                      NAMES\nb01ab65e1579        127.0.0.1:5000/sconetainer:fss           \"nginx\"                  6 minutes ago       Up 6 minutes        8080/tcp, 8082/tcp         nginx_nginx.1.0wh80346mpmm5f3l1rc8dipix\n...\n\n\n\n\nLet's look inside the container:\n\n\n> sudo docker exec -it b01ab65e1579 sh\n$ ls\nbin             media           proc            srv\ndev             mnt             root            sys\netc             nginx           run             tmp\nhome            nginx-etc       sbin            usr\nlib             opt             scone-test.key  var\n\n\n\n\nThe nginx configuration file as well as the html files should be encrypted. Let's verify this:\n\n\n$ cd nginx\n$ ls\ncertificate.pem  key.pem          nginx.conf       www_root\nfspf.pb          mime.types       nginx.pid\n$ head -c 80 nginx.conf\n]P?\u025f\"4???zS????\u01b5?kj\u0198?Ec?!S^!??????8j-?e;?t'?2?L????????y??\u02f2?\u07cb   \n\n\n\n\nOk, the nginx configuration file seems to be encrypted. Now, look at the html files too:\n\n\n$ cd www_root\n$ ls \n4K                               SCONE_TUTORIAL\nGO                               aboutScone\nPython                           appsecurity\n...\n$ head -c 80 index.html \n????V?\u036a?rhk?\"k????$?    .???k?\u042c?y<??\u0660n?+????P2?G;o_'.i?);?I??xFg[???[f?\n\n\n\n\n\u00a9 \nscontain.com\n, December 2017. \nQuestions or Suggestions?",
            "title": "Example E2E encryption"
        },
        {
            "location": "/SCONE_EE2EE/#example-end-to-end-encryption",
            "text": "We show how to encrypt an application end-to-end, i.e., all the data in the file system is encrypted, all data in main memory is encrypted and all data on the wire is encrypted.  In what follows, we only interact with one swarm. Hence, we set environment variable  SCONE_MANAGER  to point to the manager node of the swarm. Say, in our case the manage is called  faye :  $ export SCONE_MANAGER=faye  We have prepared an nginx image that contains this website as encrypted files and nginx is running inside of an enclave reading the encrypted files\nfrom the file system and decrypting these files inside the enclave.  Let's assume that we have the newest image in our local swarm registry:  $ scone service pull sconecuratedimages/sconetainer:fss\n...\nfss: digest: sha256:2e76b4a0b090cc75c2e56594e20c7ec36bc1abd13e36e2f00d36e2f67b13c1d5 size: 1783\nnew tag: localhost:5000/sconetainer:fss  We will start this service as a stack. Hence, we define a simple stack/compose file:  $ cat > compose.yml << EOF\nversion: \"3.1.scone\"\nservices:\n    nginx:\n        image: 127.0.0.1:5000/sconetainer:fss\n        command: nginx -p /nginx -c nginx.conf\n        mrenclave: 1516e3d41590cf3842282c6f037535af64336138a6b5eff7e8754e97b4c64ecb\n        fspf_path: /nginx/fspf.pb\n        fspf_key: 970f4925bb7b221461f3d1a3f17450aa42844539de24f5acc1b45b8c140f9467\n        fspf_tag: 5930bffbd9ea2f1317e6872b032334db\n        working_dir: /\n        ports:\n          - 8190:8080\n          - 8192:8082\nEOF  For now, this file contains some metadata related to the encrypted files:   fspf_path  the path of the  file system protection file  fspf_key  the key used to encrypt the  file system protection file  fspf_tag  the tag (i.e., MAC) of the  file system protection file   We provide a low level for encrypting files with the help of  scone fspf . (We will soon release a higher level support that will simplify the encryption of files).   In this swarm, we have a CAS (Configuration and Attestation Service running): the CAS helps\nus to to pass some secrets like the  fspf_key  to an enclave and protecting both\nthe confidentiality as well as the integrity of this secret.  Clients can identify the CAS via its certificate. In this example,\nwe explicitly specify the certificate of the CAS:  $ cat > ca.pem << EOF\n-----BEGIN CERTIFICATE-----\nMIICKDCCAc2gAwIBAgIJAMnmUBMT+gOGMAoGCCqGSM49BAMCMHAxCzAJBgNVBAYT\nAlVTMQ8wDQYDVQQIDAZPcmVnb24xETAPBgNVBAcMCFBvcnRsYW5kMRUwEwYDVQQK\nDAxDb21wYW55IE5hbWUxDDAKBgNVBAsMA09yZzEYMBYGA1UEAwwPd3d3LmV4YW1w\nbGUuY29tMB4XDTE3MDQwNzEzNTIyMloXDTE4MDQwNzEzNTIyMlowcDELMAkGA1UE\nBhMCVVMxDzANBgNVBAgMBk9yZWdvbjERMA8GA1UEBwwIUG9ydGxhbmQxFTATBgNV\nBAoMDENvbXBhbnkgTmFtZTEMMAoGA1UECwwDT3JnMRgwFgYDVQQDDA93d3cuZXhh\nbXBsZS5jb20wWTATBgcqhkjOPQIBBggqhkjOPQMBBwNCAARvkkSOfCGXIyWCCdfj\nMil6dhw/a2ZsPaBKOl1RS5MZuoN+7gtSqgnaT5KrT4S/Wj7flP+PJqkUf0M6p1gg\nE5wBo1AwTjAdBgNVHQ4EFgQUzPP33ZM7ZMQ7UDb9THNM5Yoma3YwHwYDVR0jBBgw\nFoAUzPP33ZM7ZMQ7UDb9THNM5Yoma3YwDAYDVR0TBAUwAwEB/zAKBggqhkjOPQQD\nAgNJADBGAiEA34tSQI8V+4UaQG1w7nVJEDLWA17zw5Ls8NZBYVi4lpcCIQDgzln/\n1r25fhGzrVbRgAcbmfLsJxuDYpsfAaNL5/5leg==\n-----END CERTIFICATE-----\nEOF  and now log into the CAS:  $ export IP=x.y.z.u\n$ scone cas login -c ca.pem christof cas --host $IP:8081:18765  We split our stack file into two parts: public part sent to the docker engine and a secret part that is directly sent to the CAS:  $ scone cas split compose.yml  --stack 283299  Show public part that is sent to  docker stack  (which is the original stack file name appended with \"docker.yml\"):  $ cat compose.yml.docker.yml\n---\nservices: \n  nginx: \n    command: nginx\n    environment: \n      SCONE_CAS_ADDR: \"x.y.z.u:18765\"\n      SCONE_CONFIG_ID: christof/283299/nginx\n      SCONE_LAS_ADDR: \"172.17.0.1:18766\"\n    image: \"127.0.0.1:5000/sconetainer:fss\"\n    ports: \n      - \"8190:8080\"\n      - \"8192:8082\"\nversion: \"3.1\"  We can now deploy the service with the help of docker/scone stack:  $ scone stack deploy --compose-file compose.yml  nginx\nCreating network nginx_default\nCreating service nginx_nginx  Show running stack:  $ scone stack ls\nNAME        SERVICES\nnginx       1  We can get some more information about this stack:  $ scone stack ps nginx\nID                  NAME                IMAGE                            NODE                DESIRED STATE       CURRENT STATE             ERROR                       PORTS\nh08r5hmeu9bt        nginx_nginx.1       127.0.0.1:5000/sconetainer:fss   dorothy             Running             Starting 19 seconds ago",
            "title": "Example: End-to-End Encryption"
        },
        {
            "location": "/SCONE_EE2EE/#file-encryption",
            "text": "Let's check that the files inside of nginx container are indeed encrypted. To do so, we ssh to node   dorothy  and execute the following commands:  > sudo docker ps\nCONTAINER ID        IMAGE                                    COMMAND                  CREATED             STATUS              PORTS                      NAMES\nb01ab65e1579        127.0.0.1:5000/sconetainer:fss           \"nginx\"                  6 minutes ago       Up 6 minutes        8080/tcp, 8082/tcp         nginx_nginx.1.0wh80346mpmm5f3l1rc8dipix\n...  Let's look inside the container:  > sudo docker exec -it b01ab65e1579 sh\n$ ls\nbin             media           proc            srv\ndev             mnt             root            sys\netc             nginx           run             tmp\nhome            nginx-etc       sbin            usr\nlib             opt             scone-test.key  var  The nginx configuration file as well as the html files should be encrypted. Let's verify this:  $ cd nginx\n$ ls\ncertificate.pem  key.pem          nginx.conf       www_root\nfspf.pb          mime.types       nginx.pid\n$ head -c 80 nginx.conf\n]P?\u025f\"4???zS????\u01b5?kj\u0198?Ec?!S^!??????8j-?e;?t'?2?L????????y??\u02f2?\u07cb     Ok, the nginx configuration file seems to be encrypted. Now, look at the html files too:  $ cd www_root\n$ ls \n4K                               SCONE_TUTORIAL\nGO                               aboutScone\nPython                           appsecurity\n...\n$ head -c 80 index.html \n????V?\u036a?rhk?\"k????$?    .???k?\u042c?y<??\u0660n?+????P2?G;o_'.i?);?I??xFg[???[f?  \u00a9  scontain.com , December 2017.  Questions or Suggestions?",
            "title": "File Encryption"
        },
        {
            "location": "/C/",
            "text": "C Program Language Support\n\n\nSCONE supports native compilation combined with dynamic linking as well as cross-compilation to support, in particular, statically linked binaries.\n\n\nThis page focuses on the SCONE cross compiler. This cross compiler is based on gcc and hence, the command line options are the same as gcc.\n\n\nImage\n\n\nEnsure that you have the newest SCONE cross compiler image:\n\n\n> docker pull sconecuratedimages/crosscompilers:scone\n> docker run -it sconecuratedimages/crosscompilers:scone\n$\n\n\n\n\nHelp\n\n\nIf you need some help, just execute in the container:\n\n\n$ scone gcc --help\nUsage: x86_64-linux-musl-gcc [options] file...\nOptions:\n...\n\n\n\n\nExample\n\n\nLet's try to compile a simple program:\n\n\n$ cat > fib.c << EOF\n#include <stdio.h>\n#include <stdlib.h>\n\nint main(int argc, char** argv) {\n   int n=0, first = 0, second = 1, next = 0, c;\n\n   if (argc > 1)\n        n=atoi(argv[1]);\n   printf(\"fib(%d)= 1\",n);\n   for ( c = 1 ; c < n ; c++ ) {\n        next = first + second;\n        first = second;\n        second = next;\n        printf(\", %d\",next);\n   }\n   printf(\"\\n\");\n}\nEOF\n\n\n\n\nWe compile the program with \nscone gcc\n or \nscone-gcc\n:\n\n\n$ scone gcc fib.c -o fib\n\n\n\n\nTo compute fib(23), execute:\n\n\n\n$ SCONE_VERSION=1 ./fib 23\nfib(23)= 1, 1, 2, 3, 5, 8, 13, 21, 34, 55, 89, 144, 233, 377, 610, 987, 1597, 2584, 4181, 6765, 10946, 17711, 28657\n\n\n\n\n\u00a9 \nscontain.com\n, December 2017. \nQuestions or Suggestions?",
            "title": "C"
        },
        {
            "location": "/C/#c-program-language-support",
            "text": "SCONE supports native compilation combined with dynamic linking as well as cross-compilation to support, in particular, statically linked binaries.  This page focuses on the SCONE cross compiler. This cross compiler is based on gcc and hence, the command line options are the same as gcc.",
            "title": "C Program Language Support"
        },
        {
            "location": "/C/#image",
            "text": "Ensure that you have the newest SCONE cross compiler image:  > docker pull sconecuratedimages/crosscompilers:scone\n> docker run -it sconecuratedimages/crosscompilers:scone\n$",
            "title": "Image"
        },
        {
            "location": "/C/#help",
            "text": "If you need some help, just execute in the container:  $ scone gcc --help\nUsage: x86_64-linux-musl-gcc [options] file...\nOptions:\n...",
            "title": "Help"
        },
        {
            "location": "/C/#example",
            "text": "Let's try to compile a simple program:  $ cat > fib.c << EOF\n#include <stdio.h>\n#include <stdlib.h>\n\nint main(int argc, char** argv) {\n   int n=0, first = 0, second = 1, next = 0, c;\n\n   if (argc > 1)\n        n=atoi(argv[1]);\n   printf(\"fib(%d)= 1\",n);\n   for ( c = 1 ; c < n ; c++ ) {\n        next = first + second;\n        first = second;\n        second = next;\n        printf(\", %d\",next);\n   }\n   printf(\"\\n\");\n}\nEOF  We compile the program with  scone gcc  or  scone-gcc :  $ scone gcc fib.c -o fib  To compute fib(23), execute:  \n$ SCONE_VERSION=1 ./fib 23\nfib(23)= 1, 1, 2, 3, 5, 8, 13, 21, 34, 55, 89, 144, 233, 377, 610, 987, 1597, 2584, 4181, 6765, 10946, 17711, 28657  \u00a9  scontain.com , December 2017.  Questions or Suggestions?",
            "title": "Example"
        },
        {
            "location": "/C++/",
            "text": "C++ Program Language Support\n\n\nSCONE supports native compilation of C++ programs when combined with dynamic linking as well as cross-compilation.  Cross-compilation is required  to support, in particular, statically linked binaries.\n\n\nThis page focuses on the SCONE C++ cross compiler \nscone g++\n (a.k.a. \nscone-g++\n). This cross compiler is based on g++ and hence, the command line options are the same as those of g++.\n\n\nImage\n\n\nEnsure that you have the newest SCONE cross compiler image:\n\n\n> docker pull sconecuratedimages/crosscompilers:scone\n> docker run -it sconecuratedimages/crosscompilers:scone\n$\n\n\n\n\nHelp\n\n\nIf you need some help, just execute in the container:\n\n\n$ scone g++ --help\nUsage: x86_64-linux-musl-g++ [options] file...\nOptions:\n...\n\n\n\n\nExample\n\n\nLet's try to compile a simple program:\n\n\n$ cat > sqrt.cc << EOF\n#include <iostream>\n#include <cmath>\n\nusing namespace std;\n\nint main() {\n    int x = 0;\n    while(x < 10) {\n        double y = sqrt((double)x);\n        cout << \"The square root of \" << x << \" is \" << y << endl;\n        x++;\n    }\n    return 0;\n}\nEOF\n\n\n\n\nWe compile the program with \nscone gcc\n or \nscone-gcc\n:\n\n\n$ scone g++ sqrt.cc -o sqrt\n\n\n\n\nLet's execute the binary:\n\n\n$ SCONE_VERSION=1 ./sqrt\nexport SCONE_QUEUES=4\nexport SCONE_SLOTS=256\nexport SCONE_SIGPIPE=0\nexport SCONE_MMAP32BIT=0\nexport SCONE_SSPINS=100\nexport SCONE_SSLEEP=4000\nexport SCONE_KERNEL=0\nexport SCONE_HEAP=67108864\nexport SCONE_CONFIG=/etc/sgx-musl.conf\nexport SCONE_MODE=hw\nexport SCONE_SGXBOUNDS=no\nexport SCONE_VARYS=no\nexport SCONE_ALLOW_DLOPEN=no\nexport SCONE_ALLOW_DLOPEN2=no\n\nThe square root of 0 is 0\nThe square root of 1 is 1\nThe square root of 2 is 1.41421\nThe square root of 3 is 1.73205\nThe square root of 4 is 2\nThe square root of 5 is 2.23607\nThe square root of 6 is 2.44949\nThe square root of 7 is 2.64575\nThe square root of 8 is 2.82843\nThe square root of 9 is 3\n\n\n\n\n\u00a9 \nscontain.com\n, December 2017. \nQuestions or Suggestions?",
            "title": "C++"
        },
        {
            "location": "/C++/#c-program-language-support",
            "text": "SCONE supports native compilation of C++ programs when combined with dynamic linking as well as cross-compilation.  Cross-compilation is required  to support, in particular, statically linked binaries.  This page focuses on the SCONE C++ cross compiler  scone g++  (a.k.a.  scone-g++ ). This cross compiler is based on g++ and hence, the command line options are the same as those of g++.",
            "title": "C++ Program Language Support"
        },
        {
            "location": "/C++/#image",
            "text": "Ensure that you have the newest SCONE cross compiler image:  > docker pull sconecuratedimages/crosscompilers:scone\n> docker run -it sconecuratedimages/crosscompilers:scone\n$",
            "title": "Image"
        },
        {
            "location": "/C++/#help",
            "text": "If you need some help, just execute in the container:  $ scone g++ --help\nUsage: x86_64-linux-musl-g++ [options] file...\nOptions:\n...",
            "title": "Help"
        },
        {
            "location": "/C++/#example",
            "text": "Let's try to compile a simple program:  $ cat > sqrt.cc << EOF\n#include <iostream>\n#include <cmath>\n\nusing namespace std;\n\nint main() {\n    int x = 0;\n    while(x < 10) {\n        double y = sqrt((double)x);\n        cout << \"The square root of \" << x << \" is \" << y << endl;\n        x++;\n    }\n    return 0;\n}\nEOF  We compile the program with  scone gcc  or  scone-gcc :  $ scone g++ sqrt.cc -o sqrt  Let's execute the binary:  $ SCONE_VERSION=1 ./sqrt\nexport SCONE_QUEUES=4\nexport SCONE_SLOTS=256\nexport SCONE_SIGPIPE=0\nexport SCONE_MMAP32BIT=0\nexport SCONE_SSPINS=100\nexport SCONE_SSLEEP=4000\nexport SCONE_KERNEL=0\nexport SCONE_HEAP=67108864\nexport SCONE_CONFIG=/etc/sgx-musl.conf\nexport SCONE_MODE=hw\nexport SCONE_SGXBOUNDS=no\nexport SCONE_VARYS=no\nexport SCONE_ALLOW_DLOPEN=no\nexport SCONE_ALLOW_DLOPEN2=no\n\nThe square root of 0 is 0\nThe square root of 1 is 1\nThe square root of 2 is 1.41421\nThe square root of 3 is 1.73205\nThe square root of 4 is 2\nThe square root of 5 is 2.23607\nThe square root of 6 is 2.44949\nThe square root of 7 is 2.64575\nThe square root of 8 is 2.82843\nThe square root of 9 is 3  \u00a9  scontain.com , December 2017.  Questions or Suggestions?",
            "title": "Example"
        },
        {
            "location": "/Fortran/",
            "text": "Fortran\n\n\nSCONE supports Fortran with the help of cross-compilation. This page focuses on the SCONE gfortran cross compiler \nscone gfortran\n (a.k.a. \nscone-gfortran\n). This cross compiler is based on gfortran and hence, the command line options are the same as those of gfortran.\n\n\nImage\n\n\nEnsure that you have the newest SCONE cross compiler image:\n\n\n> docker pull sconecuratedimages/crosscompilers:scone\n> docker run -it sconecuratedimages/crosscompilers:scone\n$\n\n\n\n\nHelp\n\n\nIf you need some help, just execute in the container:\n\n\n$ scone gfortran --help\nUsage: x86_64-linux-musl-gfortran [options] file...\nOptions:\n...\n\n\n\n\nLet's try a simple Fortran program.\n\n\n$ cat > gcd.f << EOF\n*     euclid.f (FORTRAN 77)\n*     Find greatest common divisor using the Euclidean algorithm\n\n      PROGRAM EUCLID\n        PRINT *, 'A?'\n        READ *, NA\n        IF (NA.LE.0) THEN\n          PRINT *, 'A must be a positive integer.'\n          STOP\n        END IF\n        PRINT *, 'B?'\n        READ *, NB\n        IF (NB.LE.0) THEN\n          PRINT *, 'B must be a positive integer.'\n          STOP\n        END IF\n        PRINT *, 'The GCD of', NA, ' and', NB, ' is', NGCD(NA, NB), '.'\n        STOP\n      END\n\n      FUNCTION NGCD(NA, NB)\n        IA = NA\n        IB = NB\n    1   IF (IB.NE.0) THEN\n          ITEMP = IA\n          IA = IB\n          IB = MOD(ITEMP, IB)\n          GOTO 1\n        END IF\n        NGCD = IA\n        RETURN\n      END\nEOF\n\n\n\n\nWe compile the program with \nscone gfortran\n (a.k.a. \nscone-gfortran\n):\n\n\n$ scone gfortran gcd.f -o gcd\n\n\n\n\nWe can now run this program as follows:\n\n\n$ SCONE_VERSION=1 ./gcd << EOF\n10\n15\nEOF\n\nexport SCONE_QUEUES=4\nexport SCONE_SLOTS=256\nexport SCONE_SIGPIPE=0\nexport SCONE_MMAP32BIT=0\nexport SCONE_SSPINS=100\nexport SCONE_SSLEEP=4000\nexport SCONE_KERNEL=0\nexport SCONE_HEAP=67108864\nexport SCONE_CONFIG=/etc/sgx-musl.conf\nexport SCONE_MODE=hw\nexport SCONE_SGXBOUNDS=no\nexport SCONE_VARYS=no\nexport SCONE_ALLOW_DLOPEN=no\nexport SCONE_ALLOW_DLOPEN2=no\n\n A?\n B?\n The GCD of          10  and          15  is           5 .\n\n\n\n\n\u00a9 \nscontain.com\n, December 2017. \nQuestions or Suggestions?",
            "title": "Fortran"
        },
        {
            "location": "/Fortran/#fortran",
            "text": "SCONE supports Fortran with the help of cross-compilation. This page focuses on the SCONE gfortran cross compiler  scone gfortran  (a.k.a.  scone-gfortran ). This cross compiler is based on gfortran and hence, the command line options are the same as those of gfortran.",
            "title": "Fortran"
        },
        {
            "location": "/Fortran/#image",
            "text": "Ensure that you have the newest SCONE cross compiler image:  > docker pull sconecuratedimages/crosscompilers:scone\n> docker run -it sconecuratedimages/crosscompilers:scone\n$",
            "title": "Image"
        },
        {
            "location": "/Fortran/#help",
            "text": "If you need some help, just execute in the container:  $ scone gfortran --help\nUsage: x86_64-linux-musl-gfortran [options] file...\nOptions:\n...  Let's try a simple Fortran program.  $ cat > gcd.f << EOF\n*     euclid.f (FORTRAN 77)\n*     Find greatest common divisor using the Euclidean algorithm\n\n      PROGRAM EUCLID\n        PRINT *, 'A?'\n        READ *, NA\n        IF (NA.LE.0) THEN\n          PRINT *, 'A must be a positive integer.'\n          STOP\n        END IF\n        PRINT *, 'B?'\n        READ *, NB\n        IF (NB.LE.0) THEN\n          PRINT *, 'B must be a positive integer.'\n          STOP\n        END IF\n        PRINT *, 'The GCD of', NA, ' and', NB, ' is', NGCD(NA, NB), '.'\n        STOP\n      END\n\n      FUNCTION NGCD(NA, NB)\n        IA = NA\n        IB = NB\n    1   IF (IB.NE.0) THEN\n          ITEMP = IA\n          IA = IB\n          IB = MOD(ITEMP, IB)\n          GOTO 1\n        END IF\n        NGCD = IA\n        RETURN\n      END\nEOF  We compile the program with  scone gfortran  (a.k.a.  scone-gfortran ):  $ scone gfortran gcd.f -o gcd  We can now run this program as follows:  $ SCONE_VERSION=1 ./gcd << EOF\n10\n15\nEOF\n\nexport SCONE_QUEUES=4\nexport SCONE_SLOTS=256\nexport SCONE_SIGPIPE=0\nexport SCONE_MMAP32BIT=0\nexport SCONE_SSPINS=100\nexport SCONE_SSLEEP=4000\nexport SCONE_KERNEL=0\nexport SCONE_HEAP=67108864\nexport SCONE_CONFIG=/etc/sgx-musl.conf\nexport SCONE_MODE=hw\nexport SCONE_SGXBOUNDS=no\nexport SCONE_VARYS=no\nexport SCONE_ALLOW_DLOPEN=no\nexport SCONE_ALLOW_DLOPEN2=no\n\n A?\n B?\n The GCD of          10  and          15  is           5 .  \u00a9  scontain.com , December 2017.  Questions or Suggestions?",
            "title": "Help"
        },
        {
            "location": "/GO/",
            "text": "GO\n\n\nSCONE supports cross-compiling GO programs to run these inside of SGX enclaves. The GO cross-compiler is part of image \nsconecuratedimages/crosscompilers:scone\n.\n\n\nExample\n\n\nStart the SCONE crosscompiler container:\n\n\n> docker run -it -p 8080:8080 sconecuratedimages/crosscompilers:scone\n$\n\n\n\n\nLets consider a simple GO program (take from a \nGO tutorial\n):\n\n\n$ cat > web-srv.go << EOF\npackage main\n\nimport (\n    \"os\"\n    \"fmt\"\n    \"net/http\"\n)\n\nfunc handler(w http.ResponseWriter, r *http.Request) {\n    fmt.Fprintf(w, \"Hi there, I love %s!\\n\", r.URL.Path[1:])\n    if r.URL.Path[1:] == \"EXIT\" {\n        os.Exit(0)\n    }\n}\n\nfunc main() {\n    http.HandleFunc(\"/\", handler)\n    http.ListenAndServe(\":8080\", nil)\n}\nEOF\n\n\n\n\nYou can cross-compile this program as follows:\n\n\n$ SCONE_HEAP=1G scone-gccgo web-srv.go -O3 -o web-srv-go -g\n\n\n\n\nYou can start the compiled program (and enable some debug messages) as follows:\n\n\n$ SCONE_VERSION=1 ./web-srv-go &\nexport SCONE_QUEUES=4\nexport SCONE_SLOTS=256\nexport SCONE_SIGPIPE=0\nexport SCONE_MMAP32BIT=0\nexport SCONE_SSPINS=100\nexport SCONE_SSLEEP=4000\nexport SCONE_KERNEL=0\nexport SCONE_HEAP=1073741824\nexport SCONE_CONFIG=/etc/sgx-musl.conf\nexport SCONE_MODE=hw\nexport SCONE_SGXBOUNDS=no\nexport SCONE_ALLOW_DLOPEN=no\nRevision: 9b355b99170ad434010353bb9f4dca24e532b1b7\nBranch: master\nConfigure options: --enable-file-prot --enable-shared --enable-debug --prefix=/scone/src/built/cross-compiler/x86_64-linux-musl\n\n\n\n\n\n\nYou can now connect to port 8080, for example, with \ncurl\n:\n\n\n$ curl localhost:8080/SCONE\nHi there, I love SCONE!\n\n\n\n\nYou can terminate the server with\n\n\n$ curl localhost:8080/EXIT\ncurl: (52) Empty reply from server\n\n\n\n\nDebugging\n\n\nSCONE supports debugging of programs running inside of an enclave with the help of gdb.\n\n\nDebugging inside of a container\n\n\nStandard containers have not sufficient rights to use the debugger.  Hence, you must start a container with \nSYS_PTRACE\n capability. For example:\n\n\n$ docker run --cap-add SYS_PTRACE -it -p 8080:8080 -v \"$PWD\"/EXAMPLE:/usr/src/myapp -w /usr/src/myapp  sconecuratedimages/crosscompilers:scone\n\n\n\n\nHandling Illegal instructions\n\n\nSome instructions, like CPUID, are not permitted inside of enclaves. For some of these instructions, like CPUID, we provide an automatic emulation. However, we recommend not to use any illegal instructions inside of enclaves despite having an automatic emulation of these instructions. For example, we provide static replacements of the CPUID instruction. \n\n\nIf your program contains some illegal instructions, you need to ask the debugger to forward the signals,\nthat these illegal instructions cause, to the program via \nhandle SIGILL nostop pass\n: \n\n\n$ scone-gdb ./web-srv-go\nGNU gdb (Ubuntu 7.12.50.20170314-0ubuntu1.1) 7.12.50.20170314-git\nCopyright (C) 2017 Free Software Foundation, Inc.\nLicense GPLv3+: GNU GPL version 3 or later <http://gnu.org/licenses/gpl.html>\nThis is free software: you are free to change and redistribute it.\nThere is NO WARRANTY, to the extent permitted by law.  Type \"show copying\"\nand \"show warranty\" for details.\nThis GDB was configured as \"x86_64-linux-gnu\".\nType \"show configuration\" for configuration details.\nFor bug reporting instructions, please see:\n<http://www.gnu.org/software/gdb/bugs/>.\nFind the GDB manual and other documentation resources online at:\n<http://www.gnu.org/software/gdb/documentation/>.\nFor help, type \"help\".\nType \"apropos word\" to search for commands related to \"word\"...\nSource directories searched: /opt/scone/scone-gdb/gdb-sgxmusl-plugin:$cdir:$cwd\nSetting environment variable \"LD_PRELOAD\" to null value.\nReading symbols from ./web-srv-go...done.\n[SCONE] Initializing...\n(gdb) handle SIGILL nostop pass\nSignal        Stop  Print   Pass to program Description\nSIGILL        No    Yes Yes     Illegal instruction\n(gdb)\n\n\n\n\nSince we do not patch the CPUID instructions in this run, you will see something like this:\n\n\n(gdb) run\nStarting program: /usr/src/myapp/web-srv-go \nwarning: Error disabling address space randomization: Operation not permitted\n[Thread debugging using libthread_db enabled]\nUsing host libthread_db library \"/lib/x86_64-linux-gnu/libthread_db.so.1\".\n[SCONE] Enclave base: 1000000000\n[SCONE] Loaded debug symbols\n[New Thread 0x7f1786d26700 (LWP 105)]\n[New Thread 0x7f1786525700 (LWP 106)]\n[New Thread 0x7f1785d24700 (LWP 107)]\n[New Thread 0x7f1785523700 (LWP 108)]\n[New Thread 0x7f1787502700 (LWP 109)]\n[New Thread 0x7f17874fa700 (LWP 110)]\n[New Thread 0x7f17874f2700 (LWP 111)]\n[New Thread 0x7f17874ea700 (LWP 112)]\n\nThread 6 \"web-srv-go\" received signal SIGILL, Illegal instruction.\n\nThread 6 \"web-srv-go\" received signal SIGILL, Illegal instruction.\n\nThread 6 \"web-srv-go\" received signal SIGILL, Illegal instruction.\n\nThread 6 \"web-srv-go\" received signal SIGILL, Illegal instruction.\n\nThread 6 \"web-srv-go\" received signal SIGILL, Illegal instruction.\n\nThread 8 \"web-srv-go\" received signal SIGILL, Illegal instruction.\n\n\n\n\n\nYou could interrupt this execution via control c:\n\n\n^C\nThread 1 \"web-srv-go\" received signal SIGINT, Interrupt.\n0x00007f17870f69dd in pthread_join (threadid=139739022911232, thread_return=0x7ffe1c807928) at pthread_join.c:90\n90  pthread_join.c: No such file or directory.\n(gdb) where\n#0  0x00007f17870f69dd in pthread_join (threadid=139739022911232, thread_return=0x7ffe1c807928) at pthread_join.c:90\n#1  0x0000002000004053 in main (argc=1, argv=0x7ffe1c807c18, envp=0x7ffe1c807c28) at ./tools/starter-exec.c:764\n(gdb) cont\nContinuing.\n\n\n\n\nBreakpoints\n\n\nOf course, you might also set breakpoints. Say, we want to get control in the debugger whenever a request is being\nprocessed by the handler. We would set a breakpoint at function \nmain.handler\n as follows:\n\n\n$ scone-gdb ./web-srv-go\n...\n[SCONE] Initializing...\n(gdb) handle SIGILL nostop pass\nSignal        Stop      Print   Pass to program Description\nSIGILL        No        Yes     Yes             Illegal instruction\n(gdb) break main.handler\nFunction \"main.handler\" not defined.\nMake breakpoint pending on future shared library load? (y or [n]) y\nBreakpoint 1 (main.handler) pending.\n(gdb) run\nStarting program: /usr/src/myapp/web-srv-go \nwarning: Error disabling address space randomization: Operation not permitted\n[Thread debugging using libthread_db enabled]\nUsing host libthread_db library \"/lib/x86_64-linux-gnu/libthread_db.so.1\".\n[SCONE] Enclave base: 1000000000\n[SCONE] Loaded debug symbols\n[New Thread 0x7fb6cad32700 (LWP 243)]\n[New Thread 0x7fb6ca531700 (LWP 244)]\n[New Thread 0x7fb6c9d30700 (LWP 245)]\n[New Thread 0x7fb6c952f700 (LWP 246)]\n[New Thread 0x7fb6cb50e700 (LWP 247)]\n[New Thread 0x7fb6cb506700 (LWP 248)]\n[New Thread 0x7fb6cb4fe700 (LWP 249)]\n[New Thread 0x7fb6cb4f6700 (LWP 250)]\n\nThread 6 \"web-srv-go\" received signal SIGILL, Illegal instruction.\n\nThread 6 \"web-srv-go\" received signal SIGILL, Illegal instruction.\n\nThread 6 \"web-srv-go\" received signal SIGILL, Illegal instruction.\n\nThread 6 \"web-srv-go\" received signal SIGILL, Illegal instruction.\n\nThread 6 \"web-srv-go\" received signal SIGILL, Illegal instruction.\n\nThread 6 \"web-srv-go\" received signal SIGILL, Illegal instruction.\n\n\n\n\nNote that at the time when we are setting the breakpoint, the symbols of the code running inside\nof the enclave are not yet known. Hence, we just let gdb know that the symbol will be defined later on.\n\n\nWe are now sending a request with the help of \ncurl\n from a different window. This triggers the breakpoint:\n\n\n[Switching to Thread 0x7fb6cb506700 (LWP 248)]\n\nThread 7 \"web-srv-go\" hit Breakpoint 1, main.handler (w=..., r=0x100909e300) at web-srv.go:8\n8       func handler(w http.ResponseWriter, r *http.Request) {\n(gdb) n\n9           fmt.Fprintf(w, \"Hi there, I love %s!\", r.URL.Path[1:])\n(gdb) n\n8       func handler(w http.ResponseWriter, r *http.Request) {\n(gdb) c\nContinuing.\n\n\n\n\n\u00a9 \nscontain.com\n, November 2017. \nQuestions or Suggestions?",
            "title": "GO"
        },
        {
            "location": "/GO/#go",
            "text": "SCONE supports cross-compiling GO programs to run these inside of SGX enclaves. The GO cross-compiler is part of image  sconecuratedimages/crosscompilers:scone .",
            "title": "GO"
        },
        {
            "location": "/GO/#example",
            "text": "Start the SCONE crosscompiler container:  > docker run -it -p 8080:8080 sconecuratedimages/crosscompilers:scone\n$  Lets consider a simple GO program (take from a  GO tutorial ):  $ cat > web-srv.go << EOF\npackage main\n\nimport (\n    \"os\"\n    \"fmt\"\n    \"net/http\"\n)\n\nfunc handler(w http.ResponseWriter, r *http.Request) {\n    fmt.Fprintf(w, \"Hi there, I love %s!\\n\", r.URL.Path[1:])\n    if r.URL.Path[1:] == \"EXIT\" {\n        os.Exit(0)\n    }\n}\n\nfunc main() {\n    http.HandleFunc(\"/\", handler)\n    http.ListenAndServe(\":8080\", nil)\n}\nEOF  You can cross-compile this program as follows:  $ SCONE_HEAP=1G scone-gccgo web-srv.go -O3 -o web-srv-go -g  You can start the compiled program (and enable some debug messages) as follows:  $ SCONE_VERSION=1 ./web-srv-go &\nexport SCONE_QUEUES=4\nexport SCONE_SLOTS=256\nexport SCONE_SIGPIPE=0\nexport SCONE_MMAP32BIT=0\nexport SCONE_SSPINS=100\nexport SCONE_SSLEEP=4000\nexport SCONE_KERNEL=0\nexport SCONE_HEAP=1073741824\nexport SCONE_CONFIG=/etc/sgx-musl.conf\nexport SCONE_MODE=hw\nexport SCONE_SGXBOUNDS=no\nexport SCONE_ALLOW_DLOPEN=no\nRevision: 9b355b99170ad434010353bb9f4dca24e532b1b7\nBranch: master\nConfigure options: --enable-file-prot --enable-shared --enable-debug --prefix=/scone/src/built/cross-compiler/x86_64-linux-musl  You can now connect to port 8080, for example, with  curl :  $ curl localhost:8080/SCONE\nHi there, I love SCONE!  You can terminate the server with  $ curl localhost:8080/EXIT\ncurl: (52) Empty reply from server",
            "title": "Example"
        },
        {
            "location": "/GO/#debugging",
            "text": "SCONE supports debugging of programs running inside of an enclave with the help of gdb.  Debugging inside of a container  Standard containers have not sufficient rights to use the debugger.  Hence, you must start a container with  SYS_PTRACE  capability. For example:  $ docker run --cap-add SYS_PTRACE -it -p 8080:8080 -v \"$PWD\"/EXAMPLE:/usr/src/myapp -w /usr/src/myapp  sconecuratedimages/crosscompilers:scone  Handling Illegal instructions  Some instructions, like CPUID, are not permitted inside of enclaves. For some of these instructions, like CPUID, we provide an automatic emulation. However, we recommend not to use any illegal instructions inside of enclaves despite having an automatic emulation of these instructions. For example, we provide static replacements of the CPUID instruction.   If your program contains some illegal instructions, you need to ask the debugger to forward the signals,\nthat these illegal instructions cause, to the program via  handle SIGILL nostop pass :   $ scone-gdb ./web-srv-go\nGNU gdb (Ubuntu 7.12.50.20170314-0ubuntu1.1) 7.12.50.20170314-git\nCopyright (C) 2017 Free Software Foundation, Inc.\nLicense GPLv3+: GNU GPL version 3 or later <http://gnu.org/licenses/gpl.html>\nThis is free software: you are free to change and redistribute it.\nThere is NO WARRANTY, to the extent permitted by law.  Type \"show copying\"\nand \"show warranty\" for details.\nThis GDB was configured as \"x86_64-linux-gnu\".\nType \"show configuration\" for configuration details.\nFor bug reporting instructions, please see:\n<http://www.gnu.org/software/gdb/bugs/>.\nFind the GDB manual and other documentation resources online at:\n<http://www.gnu.org/software/gdb/documentation/>.\nFor help, type \"help\".\nType \"apropos word\" to search for commands related to \"word\"...\nSource directories searched: /opt/scone/scone-gdb/gdb-sgxmusl-plugin:$cdir:$cwd\nSetting environment variable \"LD_PRELOAD\" to null value.\nReading symbols from ./web-srv-go...done.\n[SCONE] Initializing...\n(gdb) handle SIGILL nostop pass\nSignal        Stop  Print   Pass to program Description\nSIGILL        No    Yes Yes     Illegal instruction\n(gdb)  Since we do not patch the CPUID instructions in this run, you will see something like this:  (gdb) run\nStarting program: /usr/src/myapp/web-srv-go \nwarning: Error disabling address space randomization: Operation not permitted\n[Thread debugging using libthread_db enabled]\nUsing host libthread_db library \"/lib/x86_64-linux-gnu/libthread_db.so.1\".\n[SCONE] Enclave base: 1000000000\n[SCONE] Loaded debug symbols\n[New Thread 0x7f1786d26700 (LWP 105)]\n[New Thread 0x7f1786525700 (LWP 106)]\n[New Thread 0x7f1785d24700 (LWP 107)]\n[New Thread 0x7f1785523700 (LWP 108)]\n[New Thread 0x7f1787502700 (LWP 109)]\n[New Thread 0x7f17874fa700 (LWP 110)]\n[New Thread 0x7f17874f2700 (LWP 111)]\n[New Thread 0x7f17874ea700 (LWP 112)]\n\nThread 6 \"web-srv-go\" received signal SIGILL, Illegal instruction.\n\nThread 6 \"web-srv-go\" received signal SIGILL, Illegal instruction.\n\nThread 6 \"web-srv-go\" received signal SIGILL, Illegal instruction.\n\nThread 6 \"web-srv-go\" received signal SIGILL, Illegal instruction.\n\nThread 6 \"web-srv-go\" received signal SIGILL, Illegal instruction.\n\nThread 8 \"web-srv-go\" received signal SIGILL, Illegal instruction.  You could interrupt this execution via control c:  ^C\nThread 1 \"web-srv-go\" received signal SIGINT, Interrupt.\n0x00007f17870f69dd in pthread_join (threadid=139739022911232, thread_return=0x7ffe1c807928) at pthread_join.c:90\n90  pthread_join.c: No such file or directory.\n(gdb) where\n#0  0x00007f17870f69dd in pthread_join (threadid=139739022911232, thread_return=0x7ffe1c807928) at pthread_join.c:90\n#1  0x0000002000004053 in main (argc=1, argv=0x7ffe1c807c18, envp=0x7ffe1c807c28) at ./tools/starter-exec.c:764\n(gdb) cont\nContinuing.  Breakpoints  Of course, you might also set breakpoints. Say, we want to get control in the debugger whenever a request is being\nprocessed by the handler. We would set a breakpoint at function  main.handler  as follows:  $ scone-gdb ./web-srv-go\n...\n[SCONE] Initializing...\n(gdb) handle SIGILL nostop pass\nSignal        Stop      Print   Pass to program Description\nSIGILL        No        Yes     Yes             Illegal instruction\n(gdb) break main.handler\nFunction \"main.handler\" not defined.\nMake breakpoint pending on future shared library load? (y or [n]) y\nBreakpoint 1 (main.handler) pending.\n(gdb) run\nStarting program: /usr/src/myapp/web-srv-go \nwarning: Error disabling address space randomization: Operation not permitted\n[Thread debugging using libthread_db enabled]\nUsing host libthread_db library \"/lib/x86_64-linux-gnu/libthread_db.so.1\".\n[SCONE] Enclave base: 1000000000\n[SCONE] Loaded debug symbols\n[New Thread 0x7fb6cad32700 (LWP 243)]\n[New Thread 0x7fb6ca531700 (LWP 244)]\n[New Thread 0x7fb6c9d30700 (LWP 245)]\n[New Thread 0x7fb6c952f700 (LWP 246)]\n[New Thread 0x7fb6cb50e700 (LWP 247)]\n[New Thread 0x7fb6cb506700 (LWP 248)]\n[New Thread 0x7fb6cb4fe700 (LWP 249)]\n[New Thread 0x7fb6cb4f6700 (LWP 250)]\n\nThread 6 \"web-srv-go\" received signal SIGILL, Illegal instruction.\n\nThread 6 \"web-srv-go\" received signal SIGILL, Illegal instruction.\n\nThread 6 \"web-srv-go\" received signal SIGILL, Illegal instruction.\n\nThread 6 \"web-srv-go\" received signal SIGILL, Illegal instruction.\n\nThread 6 \"web-srv-go\" received signal SIGILL, Illegal instruction.\n\nThread 6 \"web-srv-go\" received signal SIGILL, Illegal instruction.  Note that at the time when we are setting the breakpoint, the symbols of the code running inside\nof the enclave are not yet known. Hence, we just let gdb know that the symbol will be defined later on.  We are now sending a request with the help of  curl  from a different window. This triggers the breakpoint:  [Switching to Thread 0x7fb6cb506700 (LWP 248)]\n\nThread 7 \"web-srv-go\" hit Breakpoint 1, main.handler (w=..., r=0x100909e300) at web-srv.go:8\n8       func handler(w http.ResponseWriter, r *http.Request) {\n(gdb) n\n9           fmt.Fprintf(w, \"Hi there, I love %s!\", r.URL.Path[1:])\n(gdb) n\n8       func handler(w http.ResponseWriter, r *http.Request) {\n(gdb) c\nContinuing.  \u00a9  scontain.com , November 2017.  Questions or Suggestions?",
            "title": "Debugging"
        },
        {
            "location": "/Python/",
            "text": "Python\n\n\nSCONE supports running Python programs inside of SGX enclaves. To support this, we provide a simple Docker image (you need access rights for that).\n\n\nImage\n\n\nCurrently, we provde a simple Python 2.7 image that is based on the standard Python image python:2.7-alpine.\n\n\nYou can pull this image as follows:\n\n\n> docker pull sconecuratedimages/crosscompilers:python27\n\n\n\n\nPython Interpreter\n\n\nTo run the Python interpreter inside an enclave in interactive mode, just execute:\n\n\n> docker run -it -v \"$PWD\":/usr/src/myapp -w /usr/src/myapp sconecuratedimages/crosscompilers:python27\nexport SCONE_QUEUES=4\nexport SCONE_SLOTS=256\nexport SCONE_SIGPIPE=0\nexport SCONE_MMAP32BIT=0\nexport SCONE_SSPINS=100\nexport SCONE_SSLEEP=4000\nexport SCONE_KERNEL=0\nexport SCONE_HEAP=11000000\nexport SCONE_CONFIG=/etc/sgx-musl.conf\nexport SCONE_MODE=hw\nexport SCONE_SGXBOUNDS=no\nexport SCONE_ALLOW_DLOPEN=yes\nRevision: 9b355b99170ad434010353bb9f4dca24e532b1b7\nBranch: master\nConfigure options: --enable-file-prot --enable-shared --enable-debug --prefix=/scone/src/built/cross-compiler/x86_64-linux-musl\n\nPython 2.7.14 (default, Nov  4 2017, 00:12:32) \n[GCC 5.3.0] on linux2\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\n>>> \n\n\n\n\nRunning an application\n\n\nSay, you have a Python application called \nmyapp.py\n in your current directory.\nTo execute this with Pyhton 2.7 inside an enclave, you need to set some environment variables. \n\n\n\n\n\n\nTo run Python inside of an enclave, you can set the environment variable \nSCONE_MODE=HW\n and \nSCONE_ALPINE=1\n.\n\n\n\n\n\n\nTo issue some debug messages that show that we are running inside an enclave, set \nSCONE_VERSION=1\n\n\n\n\n\n\nIn general, we only permit the loading of dynamic libraries after the initial loading of the problem, only by explicitly defining \nSCONE_ALLOW_DLOPEN\n. To enable the loading of dynamic libraries after startup (and without requiring the authentication of this library via the file shield), we set \nSCONE_ALLOW_DLOPEN=2\n.\n\n\n\n\n\n\nExample:\n\n\n> docker run --rm -v \"$PWD\":/usr/src/myapp -w /usr/src/myapp -e SCONE_MODE=HW -e SCONE_ALLOW_DLOPEN=2 -e SCONE_ALPINE=1 -e SCONE_VERSION=1 sconecuratedimages/crosscompilers:python27 python myapp.py\n\nexport SCONE_QUEUES=4\nexport SCONE_SLOTS=256\nexport SCONE_SIGPIPE=0\nexport SCONE_MMAP32BIT=0\n...\n\n\n\n\nNOTE\n In some cases the SGX device cannot be opened inside of a container - despite being mapped in the container with \n--device\n and being visible inside of the container. In this case, try to map the devices inside of the container as a volume via \n-v /dev/isgx:/dev/isgx --privileged\n. For example, you might execute:\n\n\n> sudo docker run -v /dev/isgx:/dev/isgx --privileged -e SCONE_MODE=HW -e SCONE_ALLOW_DLOPEN=2 -e SCONE_ALPINE=1 -e SCONE_VERSION=1 sconecuratedimages/crosscompilers:python27 python -c \"print('hello')\"\n\n\n\n\nExample\n\n\nLet's look at another example:\n\n\n\n\n\n\nWe use \npip\n to install a Python chess library.\n\n\n\n\n\n\nThen we run Python inside of an enclave and  import the chess library. \n\n\n\n\n\n\nWe use the Scholar's mate example from \nhttps://pypi.python.org/pypi/python-chess\n\n\n\n\n\n\n\n\n\u00a9 \nscontain.com\n, November 2017. \nQuestions or Suggestions?",
            "title": "Python"
        },
        {
            "location": "/Python/#python",
            "text": "SCONE supports running Python programs inside of SGX enclaves. To support this, we provide a simple Docker image (you need access rights for that).",
            "title": "Python"
        },
        {
            "location": "/Python/#image",
            "text": "Currently, we provde a simple Python 2.7 image that is based on the standard Python image python:2.7-alpine.  You can pull this image as follows:  > docker pull sconecuratedimages/crosscompilers:python27",
            "title": "Image"
        },
        {
            "location": "/Python/#python-interpreter",
            "text": "To run the Python interpreter inside an enclave in interactive mode, just execute:  > docker run -it -v \"$PWD\":/usr/src/myapp -w /usr/src/myapp sconecuratedimages/crosscompilers:python27\nexport SCONE_QUEUES=4\nexport SCONE_SLOTS=256\nexport SCONE_SIGPIPE=0\nexport SCONE_MMAP32BIT=0\nexport SCONE_SSPINS=100\nexport SCONE_SSLEEP=4000\nexport SCONE_KERNEL=0\nexport SCONE_HEAP=11000000\nexport SCONE_CONFIG=/etc/sgx-musl.conf\nexport SCONE_MODE=hw\nexport SCONE_SGXBOUNDS=no\nexport SCONE_ALLOW_DLOPEN=yes\nRevision: 9b355b99170ad434010353bb9f4dca24e532b1b7\nBranch: master\nConfigure options: --enable-file-prot --enable-shared --enable-debug --prefix=/scone/src/built/cross-compiler/x86_64-linux-musl\n\nPython 2.7.14 (default, Nov  4 2017, 00:12:32) \n[GCC 5.3.0] on linux2\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\n>>>",
            "title": "Python Interpreter"
        },
        {
            "location": "/Python/#running-an-application",
            "text": "Say, you have a Python application called  myapp.py  in your current directory.\nTo execute this with Pyhton 2.7 inside an enclave, you need to set some environment variables.     To run Python inside of an enclave, you can set the environment variable  SCONE_MODE=HW  and  SCONE_ALPINE=1 .    To issue some debug messages that show that we are running inside an enclave, set  SCONE_VERSION=1    In general, we only permit the loading of dynamic libraries after the initial loading of the problem, only by explicitly defining  SCONE_ALLOW_DLOPEN . To enable the loading of dynamic libraries after startup (and without requiring the authentication of this library via the file shield), we set  SCONE_ALLOW_DLOPEN=2 .    Example:  > docker run --rm -v \"$PWD\":/usr/src/myapp -w /usr/src/myapp -e SCONE_MODE=HW -e SCONE_ALLOW_DLOPEN=2 -e SCONE_ALPINE=1 -e SCONE_VERSION=1 sconecuratedimages/crosscompilers:python27 python myapp.py\n\nexport SCONE_QUEUES=4\nexport SCONE_SLOTS=256\nexport SCONE_SIGPIPE=0\nexport SCONE_MMAP32BIT=0\n...  NOTE  In some cases the SGX device cannot be opened inside of a container - despite being mapped in the container with  --device  and being visible inside of the container. In this case, try to map the devices inside of the container as a volume via  -v /dev/isgx:/dev/isgx --privileged . For example, you might execute:  > sudo docker run -v /dev/isgx:/dev/isgx --privileged -e SCONE_MODE=HW -e SCONE_ALLOW_DLOPEN=2 -e SCONE_ALPINE=1 -e SCONE_VERSION=1 sconecuratedimages/crosscompilers:python27 python -c \"print('hello')\"",
            "title": "Running an application"
        },
        {
            "location": "/Python/#example_1",
            "text": "Let's look at another example:    We use  pip  to install a Python chess library.    Then we run Python inside of an enclave and  import the chess library.     We use the Scholar's mate example from  https://pypi.python.org/pypi/python-chess     \u00a9  scontain.com , November 2017.  Questions or Suggestions?",
            "title": "Example"
        },
        {
            "location": "/Rust/",
            "text": "Rust\n\n\nSCONE supports the Rust programming language. Rust combines speed and strong type safety and it is hence our language of choice for new applications that need to run inside of enclaves.\n\n\nTo build Rust applications, we provide variants of the \nrustc\n and \ncargo\n command line utilities as part of image \nsconecuratedimages/crosscompilers:scone\n: \n\n\nscone-rustc\n /  \nscone rustc\n\n\nYou can compile Rust programs but links against the SCONE libc instead of a standard libc. To print the version of Rust execute (inside container \nsconecuratedimages/crosscompilers:scone\n):\n\n\n> docker run -it sconecuratedimages/crosscompilers:scone\n$ scone rustc --version\nrustc 1.20.0 (f3d6973f4 2017-08-27)\n\n\n\n\nLet's try a simple hello world program.\n\n\n$ mkdir ~/projects\n$ cd ~/projects\n$ mkdir hello_world\n$ cd hello_world\n\n\n\n\nLet's try our rust program:\n\n\n$ cat > main.rs << EOF\nfn main() {\n    println!(\"Hello, world!\");\n}\nEOF\n\n\n\n\nLet's compile the program for running inside of enclaves:\n\n\n$ scone rustc main.rs\n$ ls\nmain  main.rs\n\n\n\n\nLet's run main inside an enclave and print some debug information:\n\n\n$ SCONE_MODE=HW SCONE_VERSION=1 ./main\nexport SCONE_QUEUES=4\nexport SCONE_SLOTS=256\nexport SCONE_SIGPIPE=0\nexport SCONE_MMAP32BIT=0\nexport SCONE_SSPINS=100\nexport SCONE_SSLEEP=4000\nexport SCONE_KERNEL=0\nexport SCONE_HEAP=67108864\nexport SCONE_CONFIG=/etc/sgx-musl.conf\nexport SCONE_MODE=hw\nexport SCONE_SGXBOUNDS=no\nexport SCONE_VARYS=no\nexport SCONE_ALLOW_DLOPEN=no\nexport SCONE_ALLOW_DLOPEN2=no\n\nHello, world!\n\n\n\n\nscone-cargo\n and \nscone cargo\n:\n\n\nYou can build projects with \nscone cargo\n:\n\n\n$ scone cargo build --target=scone\n\n\n\n\nAlternatively, you can use \nscone-cargo\n if, for example,  you need a command without a space.\n\n\nscone cargo\n has access to the SCONE-compiled rust standard library and the target file. \n--target=scone\n instructs it to use our target file - essentially triggering a cross-compiler build.\n\n\nDue to the cross-compilation, crates that depend on compiled (C) libraries, such as openssl or error-chain, do not work out of the box. Cargo will not use the system installed libraries because it wrongly assumes that they do not fit the target architecture. To solve this issue, one has to either provide the compiled libraries or deactivate the crate.\n\n\nThe following is an example of how an executable with \nopenssl\n can be compiled: \n\n\n$ OPENSSL_LIB_DIR=/libressl-2.4.5 OPENSSL_INCLUDE_DIR=/libressl-2.4.5/include/ OPENSSL_STATIC=1 PKG_CONFIG_ALLOW_CROSS=1 scone-cargo build --target=scone \n\n\n\n\nIn the case of error-chain, one can just deactivate its optional backtrace feature that actually requires a precompiled library.\n\n\n\u00a9 \nscontain.com\n, December 2017. \nQuestions or Suggestions?",
            "title": "Rust"
        },
        {
            "location": "/Rust/#rust",
            "text": "SCONE supports the Rust programming language. Rust combines speed and strong type safety and it is hence our language of choice for new applications that need to run inside of enclaves.  To build Rust applications, we provide variants of the  rustc  and  cargo  command line utilities as part of image  sconecuratedimages/crosscompilers:scone :",
            "title": "Rust"
        },
        {
            "location": "/Rust/#scone-rustc-scone-rustc",
            "text": "You can compile Rust programs but links against the SCONE libc instead of a standard libc. To print the version of Rust execute (inside container  sconecuratedimages/crosscompilers:scone ):  > docker run -it sconecuratedimages/crosscompilers:scone\n$ scone rustc --version\nrustc 1.20.0 (f3d6973f4 2017-08-27)  Let's try a simple hello world program.  $ mkdir ~/projects\n$ cd ~/projects\n$ mkdir hello_world\n$ cd hello_world  Let's try our rust program:  $ cat > main.rs << EOF\nfn main() {\n    println!(\"Hello, world!\");\n}\nEOF  Let's compile the program for running inside of enclaves:  $ scone rustc main.rs\n$ ls\nmain  main.rs  Let's run main inside an enclave and print some debug information:  $ SCONE_MODE=HW SCONE_VERSION=1 ./main\nexport SCONE_QUEUES=4\nexport SCONE_SLOTS=256\nexport SCONE_SIGPIPE=0\nexport SCONE_MMAP32BIT=0\nexport SCONE_SSPINS=100\nexport SCONE_SSLEEP=4000\nexport SCONE_KERNEL=0\nexport SCONE_HEAP=67108864\nexport SCONE_CONFIG=/etc/sgx-musl.conf\nexport SCONE_MODE=hw\nexport SCONE_SGXBOUNDS=no\nexport SCONE_VARYS=no\nexport SCONE_ALLOW_DLOPEN=no\nexport SCONE_ALLOW_DLOPEN2=no\n\nHello, world!",
            "title": "scone-rustc /  scone rustc"
        },
        {
            "location": "/Rust/#scone-cargo-and-scone-cargo",
            "text": "You can build projects with  scone cargo :  $ scone cargo build --target=scone  Alternatively, you can use  scone-cargo  if, for example,  you need a command without a space.  scone cargo  has access to the SCONE-compiled rust standard library and the target file.  --target=scone  instructs it to use our target file - essentially triggering a cross-compiler build.  Due to the cross-compilation, crates that depend on compiled (C) libraries, such as openssl or error-chain, do not work out of the box. Cargo will not use the system installed libraries because it wrongly assumes that they do not fit the target architecture. To solve this issue, one has to either provide the compiled libraries or deactivate the crate.  The following is an example of how an executable with  openssl  can be compiled:   $ OPENSSL_LIB_DIR=/libressl-2.4.5 OPENSSL_INCLUDE_DIR=/libressl-2.4.5/include/ OPENSSL_STATIC=1 PKG_CONFIG_ALLOW_CROSS=1 scone-cargo build --target=scone   In the case of error-chain, one can just deactivate its optional backtrace feature that actually requires a precompiled library.  \u00a9  scontain.com , December 2017.  Questions or Suggestions?",
            "title": "scone-cargo and scone cargo:"
        },
        {
            "location": "/SCONE_HOST/",
            "text": "scone host\n\n\nThis page describes the CLI \nscone host\n in more details. For an more general introduction on how to install a host and how to set up ssh to this host, please read section \nSCONE Host Setup\n. Read section \nSCONE Command Line Interface\n to see how to install command \nscone\n.\n\n\nCommands\n\n\nscone host\n supports the following commands:\n\n\n\n\n\n\ncheck\n:     checks that host is properly installed (patched docker engine and patched sgx driver)\n\n\n\n\n\n\ninstall\n:   installs the patched SGX driver and patched docker engine\n\n\n\n\n\n\nreboot\n:    reboots a host\n\n\n\n\n\n\nuninstall\n: uninstalls SGX driver and patched docker engine\n\n\n\n\n\n\nswarm\n:    join a new or another swarm\n\n\n\n\n\n\nscone host install\n\n\nCommand \ninstall\n installs a patched docker engine and a patched Intel SGX driver. It also supports that the installed host joins an existing Docker swarm or it becomes the manager of a newly created Docker swarm.\n\n\nNOTE: if a docker engine or an Intel SGX driver is already installed, the installed software is uninstalled and replaced by a patched versions. In this process, all containers that run on this machine and all enclaves that execute on this host are removed. Use command install with care.\n \n\n\nOptions\n\n\n\n\nYou must always specify option \n--name HOST\n, where, \nHOST\n is the name of the host that you want to install. You need to have \nssh access\n to this host - without the need to type in a password.\n\n\n\n\n\nExample:\n To install host \ndorothy\n without joining any swarm, just execute\n\n\n\n\n$ scone host install --name dorothy \n\n\n\n\n\n\nIf you want to install a host and make this host a manager of a new or an existing swarm, you need to add option \n--as-manager\n.\n\n\n\n\n\nExample:\n To install host \nfaye\n and create a new swarm with \nfaye\n as a manager, just execute\n\n\n\n\n$ scone host install --name faye --as-manager\n\n\n\n\nYou can now check your swarm with \nscone swarm\n to see the members of your new swarm:\n\n\n$ scone swarm ls --manager faye\nNODENO        SGX VERSION   DOCKER-ENGINE SGX-DRIVER    HOST                 STATUS     AVAILABILITY  MANAGER   \n1             1             SCONE         SCONE         faye                 Ready      Active        Leader    \n\n\n\n\n\n\nIf you want to join an existing swarm as \na worker\n, you have to specify option \n--join MANAGER\n\n\n\n\n\n\nExample:\n To install host \nedna\n and then join the swarm managed by manager \nfaye\n, execute:\n\n\n\n\n$ scone host install --name edna --join faye\n\n\n\n\nYou can now check your swarm with \nscone swarm\n to see the members of your new swarm:\n\n\n$ scone swarm ls --manager faye`\nNODENO        SGX VERSION   DOCKER-ENGINE SGX-DRIVER    HOST                 STATUS     AVAILABILITY  MANAGER   \n1             1             SCONE         SCONE         edna                 Ready      Active                  \n2             1             SCONE         SCONE         faye                 Ready      Active        Leader    \n\n\n\n\n\n\nIf you want to join an existing swarm as \na manager\n, you have to specify options \n--join MANAGER --as-manager\n\n\n\n\n\n\nExample:\n To install host \nedna\n and then join the swarm managed by manager \nfaye\n as a manager, execute:\n\n\n\n\n$ scone host install --name edna --join faye --as-manager\n\n\n\n\nscone host swarm\n\n\nIn the above example, we installed host \ndorothy\n without joining any swarm. Sometimes one wants to revise this decision via command \nswarm\n. You can add an existing node to another swarm or you can as a node to become a manager of an existing or a new node. You need to set the options \n--join MANAGER\n and \n--as-manager\n as described for \nscone host install\n.\n\n\nExample:\n To add host \ndorothy\n to the swarm managed by host \nfaye\n, just execute:\n\n\n$ scone host swarm --name dorothy --join faye \n\n\n\n\nExample:\n To add host \ndorothy\n to the swarm managed by host \nfaye\n as a manager, just execute:\n\n\n$ scone host swarm --name dorothy --join faye --as-manager\n\n\n\n\nscone host check\n\n\nTo check if a host is properly installed, you can use command \ncheck\n.\n\n\nExample:\n To check if host \nfaye\n is properly installed, just execute:\n\n\n$ scone host check --name faye\n\n\n\n\nIf the host is not properly installed, warnings or errors are issued. Typically, you can fix errors and warnings be reinstalling the host.\n\n\nscone host reboot\n\n\nSometimes, the installation of an host fails because the existing SGX driver cannot be removed. Most of the time, the issue is an enclave that currently uses the SGX driver. You can find these processes, for example, with Linux utility \nlsof\n:\n\n\n$ sudo lsof | grep dev/isgx \n\n\n\n\nSometimes removing an existing Intel SGX driver fails, despite the fact that no process seems to use the driver. In case this happens, one last resort is to reboot the machine. You issue the reboot manually or you can perform this with \nscone host reboot\n. \n\n\nYou must specify options \n--name HOST\n to indicate which host to reboot. Moreover,   \nscone host reboot\n will exit with an error unless you specify option \n--force\n.\n\n\nExample:\n To reboot host dorothy, just execute the following:\n\n\n$ scone host reboot --name  dorothy --force \n\n\n\n\nIf you want to wait until the host is again available, you can specify option \n--wait\n.\n\n\nExample:\n To reboot host dorothy and only return after the host has indeed rebooted, just execute the following:\n\n\n$ scone host reboot --name  dorothy --force --wait\n\n\n\n\nscone host uninstall\n\n\nWith the help of \nscone host uninstall\n you can force an host to \n\n\n\n\n\n\nleave any Docker swarm it might be part of \n\n\n\n\n\n\nuninstall the patched Docker engine\n\n\n\n\n\n\nuninstall the patched SGX driver\n\n\n\n\n\n\nNOTE: all containers that might run on this host, will be destroyed.\n\n\nOptions\n\n\n\n\n\n\nYou must define the name of the host to be uninstalled via option \n--name HOST\n. \n\n\n\n\n\n\nYou must always give the \n--force\n option, otherwise, an error is issued.\n\n\n\n\n\n\nIf a node is part of a swarm, you must explicitly specify the option \n--manager MANAGERHOST\n. \n\n\n\n\n\n\nIf the node is not part of a swarm, you must specify the option \n--noswarm\n. \n\n\n\n\n\n\nNote:\n While other objects / commands support the use of environment variable \nSCONE_MANAGER\n\nas an implicit definition of \n--manager $SCONE_MANAGER\n, we decided that users must explicitly specify the manager of the swarm for command \nuninstall\n.\n\n\nExample:\n To uninstall a host \nedna\n that is part of a swarm managed by host \nfaye\n, execute:\n\n\n$ scone host uninstall --name  edna --force --manager faye\n\n\n\n\nExample:\n Let's assume that host \nedna\n is not part of a swarm. To uninstall host \nedna\n, execute:\n\n\n$ scone host uninstall --name  edna --force --noswarm\n\n\n\n\nGeneral options\n\n\n\n\n\n\n--help\n (or, \n-h\n): issue help message for object \nhost\n. If a command is specified, it issues a help message specific to this command.\n\n\n\n\n\n\n--debug\n (or, \n-x\n): display all commands that are executed by \nscone host\n. This can be helpful in case a command fails. When you submit a support request regarding a failed command, please send a copy of the output of the failing command with \n--debug\n set.\n\n\n\n\n\n\n--verbose\n (or, \n-v\n): display all commands that are executed by \nscone host\n. This can be helpful in case commands fail. When you submit a support request regarding a failed command, please send a copy of the log of the output that includes \n\n\n\n\n\n\n\u00a9 \nscontain.com\n, December 2017. \nQuestions or Suggestions?",
            "title": "manual scone host"
        },
        {
            "location": "/SCONE_HOST/#scone-host",
            "text": "This page describes the CLI  scone host  in more details. For an more general introduction on how to install a host and how to set up ssh to this host, please read section  SCONE Host Setup . Read section  SCONE Command Line Interface  to see how to install command  scone .",
            "title": "scone host"
        },
        {
            "location": "/SCONE_HOST/#commands",
            "text": "scone host  supports the following commands:    check :     checks that host is properly installed (patched docker engine and patched sgx driver)    install :   installs the patched SGX driver and patched docker engine    reboot :    reboots a host    uninstall : uninstalls SGX driver and patched docker engine    swarm :    join a new or another swarm    scone host install  Command  install  installs a patched docker engine and a patched Intel SGX driver. It also supports that the installed host joins an existing Docker swarm or it becomes the manager of a newly created Docker swarm.  NOTE: if a docker engine or an Intel SGX driver is already installed, the installed software is uninstalled and replaced by a patched versions. In this process, all containers that run on this machine and all enclaves that execute on this host are removed. Use command install with care.    Options   You must always specify option  --name HOST , where,  HOST  is the name of the host that you want to install. You need to have  ssh access  to this host - without the need to type in a password.   Example:  To install host  dorothy  without joining any swarm, just execute   $ scone host install --name dorothy    If you want to install a host and make this host a manager of a new or an existing swarm, you need to add option  --as-manager .   Example:  To install host  faye  and create a new swarm with  faye  as a manager, just execute   $ scone host install --name faye --as-manager  You can now check your swarm with  scone swarm  to see the members of your new swarm:  $ scone swarm ls --manager faye\nNODENO        SGX VERSION   DOCKER-ENGINE SGX-DRIVER    HOST                 STATUS     AVAILABILITY  MANAGER   \n1             1             SCONE         SCONE         faye                 Ready      Active        Leader       If you want to join an existing swarm as  a worker , you have to specify option  --join MANAGER    Example:  To install host  edna  and then join the swarm managed by manager  faye , execute:   $ scone host install --name edna --join faye  You can now check your swarm with  scone swarm  to see the members of your new swarm:  $ scone swarm ls --manager faye`\nNODENO        SGX VERSION   DOCKER-ENGINE SGX-DRIVER    HOST                 STATUS     AVAILABILITY  MANAGER   \n1             1             SCONE         SCONE         edna                 Ready      Active                  \n2             1             SCONE         SCONE         faye                 Ready      Active        Leader       If you want to join an existing swarm as  a manager , you have to specify options  --join MANAGER --as-manager    Example:  To install host  edna  and then join the swarm managed by manager  faye  as a manager, execute:   $ scone host install --name edna --join faye --as-manager  scone host swarm  In the above example, we installed host  dorothy  without joining any swarm. Sometimes one wants to revise this decision via command  swarm . You can add an existing node to another swarm or you can as a node to become a manager of an existing or a new node. You need to set the options  --join MANAGER  and  --as-manager  as described for  scone host install .  Example:  To add host  dorothy  to the swarm managed by host  faye , just execute:  $ scone host swarm --name dorothy --join faye   Example:  To add host  dorothy  to the swarm managed by host  faye  as a manager, just execute:  $ scone host swarm --name dorothy --join faye --as-manager  scone host check  To check if a host is properly installed, you can use command  check .  Example:  To check if host  faye  is properly installed, just execute:  $ scone host check --name faye  If the host is not properly installed, warnings or errors are issued. Typically, you can fix errors and warnings be reinstalling the host.  scone host reboot  Sometimes, the installation of an host fails because the existing SGX driver cannot be removed. Most of the time, the issue is an enclave that currently uses the SGX driver. You can find these processes, for example, with Linux utility  lsof :  $ sudo lsof | grep dev/isgx   Sometimes removing an existing Intel SGX driver fails, despite the fact that no process seems to use the driver. In case this happens, one last resort is to reboot the machine. You issue the reboot manually or you can perform this with  scone host reboot .   You must specify options  --name HOST  to indicate which host to reboot. Moreover,    scone host reboot  will exit with an error unless you specify option  --force .  Example:  To reboot host dorothy, just execute the following:  $ scone host reboot --name  dorothy --force   If you want to wait until the host is again available, you can specify option  --wait .  Example:  To reboot host dorothy and only return after the host has indeed rebooted, just execute the following:  $ scone host reboot --name  dorothy --force --wait  scone host uninstall  With the help of  scone host uninstall  you can force an host to     leave any Docker swarm it might be part of     uninstall the patched Docker engine    uninstall the patched SGX driver    NOTE: all containers that might run on this host, will be destroyed.  Options    You must define the name of the host to be uninstalled via option  --name HOST .     You must always give the  --force  option, otherwise, an error is issued.    If a node is part of a swarm, you must explicitly specify the option  --manager MANAGERHOST .     If the node is not part of a swarm, you must specify the option  --noswarm .     Note:  While other objects / commands support the use of environment variable  SCONE_MANAGER \nas an implicit definition of  --manager $SCONE_MANAGER , we decided that users must explicitly specify the manager of the swarm for command  uninstall .  Example:  To uninstall a host  edna  that is part of a swarm managed by host  faye , execute:  $ scone host uninstall --name  edna --force --manager faye  Example:  Let's assume that host  edna  is not part of a swarm. To uninstall host  edna , execute:  $ scone host uninstall --name  edna --force --noswarm  General options    --help  (or,  -h ): issue help message for object  host . If a command is specified, it issues a help message specific to this command.    --debug  (or,  -x ): display all commands that are executed by  scone host . This can be helpful in case a command fails. When you submit a support request regarding a failed command, please send a copy of the output of the failing command with  --debug  set.    --verbose  (or,  -v ): display all commands that are executed by  scone host . This can be helpful in case commands fail. When you submit a support request regarding a failed command, please send a copy of the log of the output that includes     \u00a9  scontain.com , December 2017.  Questions or Suggestions?",
            "title": "Commands"
        },
        {
            "location": "/SCONE_SERVICE/",
            "text": "scone service\n\n\nscone service\n manages remote docker services. \nscone service\n is mainly a thin wrapper around \ndocker service\n. However, instead of executing commands locally,  it forwards the commands to the manager of a remote swarm.\n\n\nThe remote swarm is specified via command line option \n--manager MANAGER\n. Alternatively, one can export an environment variable \nSCONE_MANAGER\n: this variable defines the default swarm to be used by \nscone service\n in case option \n--manager\n is not given.\n\n\nscone service\n supports all commands and all the options of \ndocker service\n. The semantics of command \ncreate\n is, however, slightly modified: \nscone service create\n only creates services on SGX-enabled nodes. More precisely, it limits the nodes that can run a service to those that have a label \nSGX VERSION\n greater than 0.\n\n\nIn addition to the docker defined commands, \nscone service\n supports two new commands:\n\n\n\n\n\n\n\n\nscone service\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nregistry\n\n\nchecks if a registry service is running in the swarm and if it is not, it starts a new registry service.\n\n\n\n\n\n\npull\n\n\npulls an image from a repository and stores it in the registry of the swarm.\n\n\n\n\n\n\n\n\nscone service commands\n\n\nThe following commands of scone service are implemented by \ndocker service\n:\n\n\n\n\n\n\n\n\nscone service\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\ncreate\n\n\nCreates a new service \non a SGX-enabled machine\n.\n\n\n\n\n\n\ninspect\n\n\nDisplay detailed information on one or more services\n\n\n\n\n\n\nlogs\n\n\nFetch the logs of a service or task\n\n\n\n\n\n\nls\n\n\nList services\n\n\n\n\n\n\nps\n\n\nList the tasks of one or more services\n\n\n\n\n\n\nrm\n\n\nRemove one or more services\n\n\n\n\n\n\nrollback\n\n\nRevert changes to a service's configuration\n\n\n\n\n\n\nscale\n\n\nScale one or multiple replicated services\n\n\n\n\n\n\nupdate\n\n\nUpdate a service\n\n\n\n\n\n\n\n\nThe options of the above commands are the same as for \ndocker service\n with the exception that all commands support\nthe new option \n--manager MANAGER\n.\n\n\nscone service registry [OPTIONS]\n\n\nStarts a local registry service in the swarm managed by node \nMANAGER\n.\nThe identity of \nMANAGER\n is given via option \n--manager=MANAGER\n or via environment variable \nSCONE_MANAGER=MANAGER\n.\n\n\n\n\n\n\n\n\nOptions\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\n--manager MANAGER\n\n\nmanager of swarm (required unless SCONE_MANAGER is defined)\n\n\n\n\n\n\n--verbose\n\n\nprint verbose messages\n\n\n\n\n\n\n--debug\n\n\nprint debug messages\n\n\n\n\n\n\n--help\n\n\nprint usage of this command\n\n\n\n\n\n\n\n\nscone service pull [OPTIONS] REPOSITORY/IMAGE[:TAG]\n\n\nPulls an image from a repository (typically, docker hub) and stores\nthe image in the local registry. First, create this local repository with\n\n\n$  scone service registry --manager MANAGER\n\n\n\n\nIt is expected that the name of the image to be pulled has the \nfollowing format:\n\n\n   REPOSITORY/IMAGE[:TAG]\n\n\n\n\nAfter pulling the image, it is locally available in the swarm.\nThe name of the image id now:\n\n\n   localhost:5000/IMAGE[:TAG]\n\n\n\n\nThe identity of MANAGER is given via option --manager or via\nenvironment variable SCONE_MANAGER.\n\n\n\n\n\n\n\n\nOptions\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\n--manager MANAGER\n\n\nmanager of swarm (required)\n\n\n\n\n\n\n--verbose\n\n\nprint verbose messages\n\n\n\n\n\n\n--debug\n\n\nprint debug messages\n\n\n\n\n\n\n--help\n\n\nprint usage of this command\n\n\n\n\n\n\n\n\nExample\n\n\nTo ensure that a registry is running on the swarm managed by node \nfaye\n, execute:\n\n\n$ export SCONE_MANAGER=faye\n$ scone service registry\n\n\n\n\nTo pull image \nsconecuratedimages/sconetainer:shielded\n from docker hub and store it in the local registry, execute\n\n\n$ scone service pull sconecuratedimages/sconetainer:shielded\n\n\n\n\nYou can then start this image as a service on the swarm managed by \nfaye\n as follows:\n\n\n$ scone service create --name nginx-shielded --detach=true  --publish 8090:8080 --publish 8092:8082 localhost:5000/sconetainer:shielded\n\n\n\n\n\u00a9 \nscontain.com\n, December 2017. \nQuestions or Suggestions?",
            "title": "manual scone service"
        },
        {
            "location": "/SCONE_SERVICE/#scone-service",
            "text": "scone service  manages remote docker services.  scone service  is mainly a thin wrapper around  docker service . However, instead of executing commands locally,  it forwards the commands to the manager of a remote swarm.  The remote swarm is specified via command line option  --manager MANAGER . Alternatively, one can export an environment variable  SCONE_MANAGER : this variable defines the default swarm to be used by  scone service  in case option  --manager  is not given.  scone service  supports all commands and all the options of  docker service . The semantics of command  create  is, however, slightly modified:  scone service create  only creates services on SGX-enabled nodes. More precisely, it limits the nodes that can run a service to those that have a label  SGX VERSION  greater than 0.  In addition to the docker defined commands,  scone service  supports two new commands:     scone service  Description      registry  checks if a registry service is running in the swarm and if it is not, it starts a new registry service.    pull  pulls an image from a repository and stores it in the registry of the swarm.",
            "title": "scone service"
        },
        {
            "location": "/SCONE_SERVICE/#scone-service-commands",
            "text": "The following commands of scone service are implemented by  docker service :     scone service  Description      create  Creates a new service  on a SGX-enabled machine .    inspect  Display detailed information on one or more services    logs  Fetch the logs of a service or task    ls  List services    ps  List the tasks of one or more services    rm  Remove one or more services    rollback  Revert changes to a service's configuration    scale  Scale one or multiple replicated services    update  Update a service     The options of the above commands are the same as for  docker service  with the exception that all commands support\nthe new option  --manager MANAGER .  scone service registry [OPTIONS]  Starts a local registry service in the swarm managed by node  MANAGER .\nThe identity of  MANAGER  is given via option  --manager=MANAGER  or via environment variable  SCONE_MANAGER=MANAGER .     Options  Description      --manager MANAGER  manager of swarm (required unless SCONE_MANAGER is defined)    --verbose  print verbose messages    --debug  print debug messages    --help  print usage of this command     scone service pull [OPTIONS] REPOSITORY/IMAGE[:TAG]  Pulls an image from a repository (typically, docker hub) and stores\nthe image in the local registry. First, create this local repository with  $  scone service registry --manager MANAGER  It is expected that the name of the image to be pulled has the \nfollowing format:     REPOSITORY/IMAGE[:TAG]  After pulling the image, it is locally available in the swarm.\nThe name of the image id now:     localhost:5000/IMAGE[:TAG]  The identity of MANAGER is given via option --manager or via\nenvironment variable SCONE_MANAGER.     Options  Description      --manager MANAGER  manager of swarm (required)    --verbose  print verbose messages    --debug  print debug messages    --help  print usage of this command     Example  To ensure that a registry is running on the swarm managed by node  faye , execute:  $ export SCONE_MANAGER=faye\n$ scone service registry  To pull image  sconecuratedimages/sconetainer:shielded  from docker hub and store it in the local registry, execute  $ scone service pull sconecuratedimages/sconetainer:shielded  You can then start this image as a service on the swarm managed by  faye  as follows:  $ scone service create --name nginx-shielded --detach=true  --publish 8090:8080 --publish 8092:8082 localhost:5000/sconetainer:shielded  \u00a9  scontain.com , December 2017.  Questions or Suggestions?",
            "title": "scone service commands"
        },
        {
            "location": "/SCONE_STACK/",
            "text": "scone stack\n\n\nscone stack\n manages remote docker stacks. \nscone stack\n is mainly a thin wrapper around \ndocker stack\n that  forwards the commands to a remote swarm - instead of executing the commands locally.\n\n\nThe remote swarm is specified via command line option \n--manager MANAGER\n. Alternatively, one can export an environment variable \nSCONE_MANAGER\n - which defines the default swarm to be used by \nscone swarm\n, in case option \n--manager\n is not given.\n\n\nscone stack commands\n\n\n\n\n\n\n\n\nscone stack\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\ndeploy\n\n\ndeploy a new stack or update an existing stack\n\n\n\n\n\n\nls\n\n\nlist stacks\n\n\n\n\n\n\nps\n\n\nlist the tasks in the stack\n\n\n\n\n\n\nrm\n\n\nremove one or more stacks\n\n\n\n\n\n\nservices\n\n\nlist the services in the stack\n\n\n\n\n\n\n\n\nscone stack deploy\n\n\nThe command \ndeploy\n is used to create a set of services specified by a \nstack file\n. \n\n\nscone stack deploy\n expects the \nstack file\n specified via option \n-compose-file\n or \n-c\n. We expect this file to reside on your local machine/container (i.e., where you start  \nscone stack deploy\n).   \n\n\nscone stack deploy\n copies the stack file to the remote swarm manager before executing \ndocker stack deploy\n on the swarm manager.\n\n\nExample\n\n\nTo deploy a stack that is called \nnginx\n on a remote swarm managed by node \nfaye\n, execute the following:\n\n\n$ scone stack deploy --compose-file compose.yml --manager faye nginx\n\n\n\n\nIn case you set the environment variable \nSCONE_MANAGER\n to \nfaye\n, you can drop option \n--manager\n:\n\n\n$ export SCONE_MANAGER=faye\n$ scone stack deploy --compose-file compose.yml nginx\n\n\n\n\n\u00a9 \nscontain.com\n, December 2017. \nQuestions or Suggestions?",
            "title": "manual scone stack"
        },
        {
            "location": "/SCONE_STACK/#scone-stack",
            "text": "scone stack  manages remote docker stacks.  scone stack  is mainly a thin wrapper around  docker stack  that  forwards the commands to a remote swarm - instead of executing the commands locally.  The remote swarm is specified via command line option  --manager MANAGER . Alternatively, one can export an environment variable  SCONE_MANAGER  - which defines the default swarm to be used by  scone swarm , in case option  --manager  is not given.",
            "title": "scone stack"
        },
        {
            "location": "/SCONE_STACK/#scone-stack-commands",
            "text": "scone stack  Description      deploy  deploy a new stack or update an existing stack    ls  list stacks    ps  list the tasks in the stack    rm  remove one or more stacks    services  list the services in the stack",
            "title": "scone stack commands"
        },
        {
            "location": "/SCONE_STACK/#scone-stack-deploy",
            "text": "The command  deploy  is used to create a set of services specified by a  stack file .   scone stack deploy  expects the  stack file  specified via option  -compose-file  or  -c . We expect this file to reside on your local machine/container (i.e., where you start   scone stack deploy ).     scone stack deploy  copies the stack file to the remote swarm manager before executing  docker stack deploy  on the swarm manager.",
            "title": "scone stack deploy"
        },
        {
            "location": "/SCONE_STACK/#example",
            "text": "To deploy a stack that is called  nginx  on a remote swarm managed by node  faye , execute the following:  $ scone stack deploy --compose-file compose.yml --manager faye nginx  In case you set the environment variable  SCONE_MANAGER  to  faye , you can drop option  --manager :  $ export SCONE_MANAGER=faye\n$ scone stack deploy --compose-file compose.yml nginx  \u00a9  scontain.com , December 2017.  Questions or Suggestions?",
            "title": "Example"
        },
        {
            "location": "/SCONE_SWARM/",
            "text": "scone swarm\n\n\nCommand \nscone swarm ls\n  lists the nodes of a docker swarm. This is an extension of \ndocker node ls\n in the sense that\n\n\n\n\n\n\none can list the nodes of a swarm managed by some remote host, and\n\n\n\n\n\n\nseveral SCONE-related attributes of the nodes of a swarm are also printed.\n\n\n\n\n\n\nThe SCONE-related attributes need to be updated when nodes leave or join a swarm. While the scone commands try to update the labels whenever they might cause a label change, users might add nodes to a swarm with Docker commands.\n\n\nCommands\n\n\nscone swarm\n supports the following commands:\n\n\n\n\n\n\ncheck\n:  checks that the SCONE-related labels of all nodes of a swarm and corrects these if not correct.\n\n\n\n\n\n\nls\n:  lists all nodes of a swarm and their SCONE-related labels.\n\n\n\n\n\n\nAttributes\n\n\nAs we mentioned above, scone introduces multiple new attributes:\n\n\n\n\n\n\nNODENO\n: each host in the swarm has a unique number in the range [1, \nnumber of swarm nodes\n]. The hosts are alphabetically sorted and the node with the smallest hostname gets assigned NODENO 1 and the host with the largest name, gets assigned the largest NODENO.\n\n\n\n\n\n\n\n\n\nSGX VERSION\n: denotes the SGX version of the CPU of a host:\n\n\n\n\n\n0\n: the host does not support SGX (or, does not have a SGX driver installed)\n\n\n1\n: the host supports SGX version 1\n\n\n2\n: the host supports SGX version 2 (CPUs are not yet available)\n\n\n\n\n\n\n\n\n\n\n\nDOCKER-ENGINE\n: shows the version of the Docker engine that is installed. It will show \nSCONE\n if the latest patched Docker engine is installed.\n\n\n\n\n\n\n\n\n\nSGX-DRIVER\n: shows the version of the SGX driver. It will show \nSCONE\n if the latest patched Intel driver is installed.\n\n\n\n\n\n\n\n\n\nscone swarm ls\n\n\nYou can list all nodes of a swarm and their attributes with the help of command \nscone swarm ls\n.\n\n\nThis command requires you to specify a manager of the swarm with the help of option \n--manager MANAGER\n\n\nExample:\n To list all nodes of a swarm managed by host \nfaye\n, execute\n\n\n$ scone swarm ls --manager faye\nNODENO        SGX VERSION   DOCKER-ENGINE SGX-DRIVER    HOST                 STATUS     AVAILABILITY  MANAGER   \n1             1             SCONE         SCONE         dorothy              Ready      Active                  \n2             1             SCONE         SCONE         edna                 Ready      Active                  \n3             1             SCONE         SCONE         faye                 Ready      Active        Leader   \n\n\n\n\nTo list the nodes of the swarm managed by host \nalice\n, execute\n\n\n$ scone swarm ls --manager alice\nNODENO        SGX VERSION   DOCKER-ENGINE SGX-DRIVER    HOST                 STATUS     AVAILABILITY  MANAGER   \n1             1             SCONE         SCONE         alice                Ready      Active        Leader    \n2             1             SCONE         SCONE         beatrix              Ready      Active                  \n3             1             SCONE         SCONE         caroline             Ready      Active                  \n\n\n\n\nEnvironment Variable\n\n\nIn case you mainly work with one swarm, you can set environment variable \nSCONE_MANAGER\n. If option \n--manager\n is not specified and \nSCONE_MANAGER\n is defined, the value stored in \nSCONE_MANAGER\n  is used as the name of the manager.\n\n\nExample:\n To list all nodes of a swarm managed by host \nfaye\n:\n\n\n$ export SCONE_MANAGER=faye\n$ scone swarm ls \nNODENO        SGX VERSION   DOCKER-ENGINE SGX-DRIVER    HOST                 STATUS     AVAILABILITY  MANAGER   \n1             1             SCONE         SCONE         dorothy              Ready      Active                  \n2             1             SCONE         SCONE         edna                 Ready      Active                  \n3             1             SCONE         SCONE         faye                 Ready      Active        Leader   \n\n\n\n\nscone swarm check\n\n\nscone\n stores the attributes of a node using Docker: the attributes of a node are stored as node \nlabels\n. For example, the attributes if a node supports SGX (\nSGX VERSION\n), if the patched docker engine (\nDOCKER-ENGINE\n) and the patched Intel driver (\nSGX-DRIVER\n) is installed are all stored as labels. \n\n\nAttributes might change over time. After a node departs from a swarm or when a node joins a swarm, we need to update the labels. Otherwise, the Docker scheduler might not properly schedule containers on the nodes of a swarm. Also, when nodes of a swarm are listed, warnings might be issued.\n\n\nTo check and update the labels of the swarm nodes, you can execute command \nscone swarm check\n. You must specify the manager of the swarm by defining option \n--manager MANAGER\n or by defining environment variable \nSCONE_MANAGER\n.\n\n\nExample:\n To check the labels of the swarm managed by node \nfaye\n, execute:\n\n\n$ scone swarm check --manager faye --verbose\n\n\n\n\nGeneral options\n\n\n\n\n\n\n--help\n (or, \n-h\n): issue help message for object *\nhost\n. If a command is specified, it issues a help message specific to this command.\n\n\n\n\n\n\n--debug\n (or, \n-x\n): display all commands that are executed by \nscone host\n. This can be helpful in case commands fail. When you submit a support request regarding a failed command, please send a copy of the output of the failing command with \n--debug\n set.\n\n\n\n\n\n\n--verbose\n (or, \n-v\n): display all commands that are executed by \nscone host\n. This can be helpful in case commands fail. When you submit a support request regarding a failed command, please send a copy of the log of the output that includes \n\n\n\n\n\n\nScreencast\n\n\n\n\n\u00a9 \nscontain.com\n, December 2017. \nQuestions or Suggestions?",
            "title": "manual scone swarm"
        },
        {
            "location": "/SCONE_SWARM/#scone-swarm",
            "text": "Command  scone swarm ls   lists the nodes of a docker swarm. This is an extension of  docker node ls  in the sense that    one can list the nodes of a swarm managed by some remote host, and    several SCONE-related attributes of the nodes of a swarm are also printed.    The SCONE-related attributes need to be updated when nodes leave or join a swarm. While the scone commands try to update the labels whenever they might cause a label change, users might add nodes to a swarm with Docker commands.",
            "title": "scone swarm"
        },
        {
            "location": "/SCONE_SWARM/#commands",
            "text": "scone swarm  supports the following commands:    check :  checks that the SCONE-related labels of all nodes of a swarm and corrects these if not correct.    ls :  lists all nodes of a swarm and their SCONE-related labels.    Attributes  As we mentioned above, scone introduces multiple new attributes:    NODENO : each host in the swarm has a unique number in the range [1,  number of swarm nodes ]. The hosts are alphabetically sorted and the node with the smallest hostname gets assigned NODENO 1 and the host with the largest name, gets assigned the largest NODENO.     SGX VERSION : denotes the SGX version of the CPU of a host:   0 : the host does not support SGX (or, does not have a SGX driver installed)  1 : the host supports SGX version 1  2 : the host supports SGX version 2 (CPUs are not yet available)      DOCKER-ENGINE : shows the version of the Docker engine that is installed. It will show  SCONE  if the latest patched Docker engine is installed.     SGX-DRIVER : shows the version of the SGX driver. It will show  SCONE  if the latest patched Intel driver is installed.     scone swarm ls  You can list all nodes of a swarm and their attributes with the help of command  scone swarm ls .  This command requires you to specify a manager of the swarm with the help of option  --manager MANAGER  Example:  To list all nodes of a swarm managed by host  faye , execute  $ scone swarm ls --manager faye\nNODENO        SGX VERSION   DOCKER-ENGINE SGX-DRIVER    HOST                 STATUS     AVAILABILITY  MANAGER   \n1             1             SCONE         SCONE         dorothy              Ready      Active                  \n2             1             SCONE         SCONE         edna                 Ready      Active                  \n3             1             SCONE         SCONE         faye                 Ready      Active        Leader     To list the nodes of the swarm managed by host  alice , execute  $ scone swarm ls --manager alice\nNODENO        SGX VERSION   DOCKER-ENGINE SGX-DRIVER    HOST                 STATUS     AVAILABILITY  MANAGER   \n1             1             SCONE         SCONE         alice                Ready      Active        Leader    \n2             1             SCONE         SCONE         beatrix              Ready      Active                  \n3             1             SCONE         SCONE         caroline             Ready      Active                    Environment Variable  In case you mainly work with one swarm, you can set environment variable  SCONE_MANAGER . If option  --manager  is not specified and  SCONE_MANAGER  is defined, the value stored in  SCONE_MANAGER   is used as the name of the manager.  Example:  To list all nodes of a swarm managed by host  faye :  $ export SCONE_MANAGER=faye\n$ scone swarm ls \nNODENO        SGX VERSION   DOCKER-ENGINE SGX-DRIVER    HOST                 STATUS     AVAILABILITY  MANAGER   \n1             1             SCONE         SCONE         dorothy              Ready      Active                  \n2             1             SCONE         SCONE         edna                 Ready      Active                  \n3             1             SCONE         SCONE         faye                 Ready      Active        Leader     scone swarm check  scone  stores the attributes of a node using Docker: the attributes of a node are stored as node  labels . For example, the attributes if a node supports SGX ( SGX VERSION ), if the patched docker engine ( DOCKER-ENGINE ) and the patched Intel driver ( SGX-DRIVER ) is installed are all stored as labels.   Attributes might change over time. After a node departs from a swarm or when a node joins a swarm, we need to update the labels. Otherwise, the Docker scheduler might not properly schedule containers on the nodes of a swarm. Also, when nodes of a swarm are listed, warnings might be issued.  To check and update the labels of the swarm nodes, you can execute command  scone swarm check . You must specify the manager of the swarm by defining option  --manager MANAGER  or by defining environment variable  SCONE_MANAGER .  Example:  To check the labels of the swarm managed by node  faye , execute:  $ scone swarm check --manager faye --verbose  General options    --help  (or,  -h ): issue help message for object * host . If a command is specified, it issues a help message specific to this command.    --debug  (or,  -x ): display all commands that are executed by  scone host . This can be helpful in case commands fail. When you submit a support request regarding a failed command, please send a copy of the output of the failing command with  --debug  set.    --verbose  (or,  -v ): display all commands that are executed by  scone host . This can be helpful in case commands fail. When you submit a support request regarding a failed command, please send a copy of the log of the output that includes     Screencast   \u00a9  scontain.com , December 2017.  Questions or Suggestions?",
            "title": "Commands"
        },
        {
            "location": "/SCONE_VOLUME/",
            "text": "scone volume\n\n\nscone volume\n provides functionality to create \nvolumes\n that are available on all nodes of a swarm. This is handy in case you want to be able to schedule a service on any node of a swarm and still be able to give it access to  volumes.\n\n\nThe underlying technology of \nscone volume\n is \ninfinit\n - a software created by a Docker Inc subsidiary. Please read the \ninfinit\n documentation to understand the infinit concepts of \nuser\n, \nnetwork\n, and \nsilo\n.\n\n\nNOTE: infinit is labeled as alpha software: use this only for development or testing\n. \n\n\nThe performance of \ninfinit\n needs to be improved. Hence, for production, one would most likely install a more mature distributed file system on the swarm nodes.\n\n\nThe remote swarm is specified via command line option \n--manager MANAGER\n. Alternatively, one can export an environment variable \nSCONE_MANAGER\n - which defines the default swarm to be used by \nscone swarm\n, in case option \n--manager\n is not given.\n\n\nscone volume commands\n\n\n\n\n\n\n\n\nscone volume\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\ninstall\n\n\ninstall infinit on all nodes of a swarm.\n\n\n\n\n\n\ncreate\n\n\ncreate new volume and install infinit storage platform if required\n\n\n\n\n\n\ncheck\n\n\ncheck that all volumes are available on all hosts\n\n\n\n\n\n\ndelete\n\n\ndelete a volume\n\n\n\n\n\n\n\n\nscone volume install [OPTIONS]\n\n\nscone volume install\n ensures that infinit is installed on all nodes of a swarm.\n\n\n\n\n\n\n\n\nOptions\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\n--manager MANAGER\n\n\nmanager of swarm (required)\n\n\n\n\n\n\n--as user\n\n\nrun as user (default random id)\n\n\n\n\n\n\n--network name\n\n\nname of network to use/create (default scone-network)\n\n\n\n\n\n\n--silo name\n\n\nname of silo to use/create (default scone-silo)\n\n\n\n\n\n\n--capacity sz\n\n\nsize of silo in GB (default 10)\n\n\n\n\n\n\n--help\n\n\nshow this help message\n\n\n\n\n\n\n\n\nNotes:\n\nIf you do not specify a default \nuser\n via option \n--user USER\n, a random default user is created. \n\n\nIf you do not specify a network name via option \n--network name\n, the default network name is set to \nscone-network\n. \n\n\nYou can specify a silo name via potion \n--silo name\n. The default silo name is \n\"scone-silo\"\n. \n\n\nThe capacity of the silo (in GB) is given via  \n--capacity name\n. The default capacity is 10GB.\n\n\nExamples: \n\n\nTo install infinit with a storage capacity of 15 GB on each node, execute the following:\n\n\n$ scone volume  install --verbose --manager alice  --capacity 15\n\n\n\n\nIf you want to keep explicit control over user and network names, execute the following:\n\n\n$ scone volume  install --verbose --manager alice  --as scone --capacity 5 --silo  my-silo --network scone-networkg\n\n\n\n\nIf you want to see the user and network names for a given swarm, execute:\n\n\n$ scone volume install --manager alice --help\n\n\n\n\nscone volume create\n\n\nscone volume create\n creates a new infinit volume for a given user (option \n--as USER\n) \nand with a given volume name (via option \n--name VOLUME\n). If required, it (re-)installs the infinit storage platform. If you are sure that the infinit is already properly installed, pass the \n--fast\n option to avoid checks and reinstallation of infinit.\n\n\nThis volume will be available on all hosts of a swarm at location:\n   \n/mnt/infinit/USER/VOLUME\n\n\n\n\n\n\n\n\nOptions\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\n--manager HOST\n\n\nmanager of swarm (required)\n\n\n\n\n\n\n--name volume\n\n\nname of volume to create (required)\n\n\n\n\n\n\n--help\n\n\nshow this help message\n\n\n\n\n\n\n--as user\n\n\nrun as user (default )\n\n\n\n\n\n\n--network name\n\n\nname of network to use/create (default )\n\n\n\n\n\n\n--silo name\n\n\nname of silo to use/create (default )\n\n\n\n\n\n\n--capacity sz\n\n\nsize of silo in GB (default )\n\n\n\n\n\n\n--fast\n\n\ncreate volume without reinstalling all software (default=false)\n\n\n\n\n\n\n--verbose\n\n\nprint location of created volume\n\n\n\n\n\n\n\n\nExample: \n\n\nIn case you already installed infinit on your swarm and you just want to create a new volume,\nexecute the following:\n\n\n$ scone volume  create --verbose --fast --name new-volume --verbose\n\n\n\n\nIn case you do not know if infinit is already installed when you create a volume and\nyour want to keep control over the names used by infinit, execute the following:\n\n\n$ scone volume  create --verbose --manager alice  --as scone --name  my-volume --capacity 5 --silo  my-silo --network scone-network  --export config\n\n\n\n\nscone volume check\n\n\nChecks if all created volumes are available on all swarm nodes.\n\n\nNote: the current implementation of \nscone volume check\n uses the metadata stored in the container/environment in which \nscone volume check\n executes. In case you created some volumes in a different container, the checks will complain.\n  Will plan to fix this issue in a future version of \nscone volume check\n.\n\n\n\n\n\n\n\n\nOptions\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\n--manager HOST\n\n\nmanager of swarm (required)\n\n\n\n\n\n\n\n\nExample:\n \n\n\n$ scone volume  check --verbose --manager alice\n\n\n\n\nscone volume delete\n\n\nscone volume delete\n deletes a volume. This removes the volume from all nodes of a swarm.\n\n\n\n\n\n\n\n\nOptions\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\n--manager HOST\n\n\nmanager of swarm (required)\n\n\n\n\n\n\n--name volume\n\n\nname of volume to be deleted (required)\n\n\n\n\n\n\n\n\nExample:\n\n\n$ scone volume  delete --verbose --manager alice --volume new-volume\n\n\n\n\nExample\n\n\nTo install \ninfinit\n on all nodes of a swarm managed by node \nfaye\n and reserve a capacity of 15GB per node, execute the following:\n\n\n$ scone volume  install --verbose --manager faye  --capacity 15 \n\n\n\n\nTo create a volume named \ntest_scone_volume\n for some arbitrary infinit user \ntest_scone_user\n, \nexecute the following:\n\n\n$ scone volume create --verbose --manager faye --as test_scone_user --name  test_scone_volume \n\n\n\n\nNote that the infinit user \ntest_scone_user\n is automatically created if it does not yet exists.\n\n\nAfter completion of this command, you will have access to the newly created volume at location \n/mnt/infinit/test_scone_user/test_scone_volume\n on all nodes of the swarm.\n\n\nYou can give a service access to this volume with the help of its stack file.\nSay, the service needs to access this mapped at \n/mnt/vol\n  as follows:\n\n\nmyservice:\n   volumes:\n        - /mnt/infinit/test_scone_user/test_scone_volume:/mnt/vol\n\n\n\n\nNote that \nscone volume create\n will check that \ninfinit\n is properly installed and will reinstall parts that\nare missing. If you are sure that infinit is properly installed, you can add option \n--fast\n to omit the checks that \ninfinit\n is properly installed.\n\n\nTo create a volume \ntest_scone2_volume\n on a swarm managed by \nfaye\n when you know that \ninfinit\n is installed, just executed\n\n\n$ scone volume  create --verbose --fast --manager faye --as test_scone_user  --name  test_scone2_volume \n\n\n\n\nTo check that the volumes on the swarm are properly installed, execute command \ncheck\n:\n\n\n$ scone volume  check --verbose --manager faye \n\n\n\n\nTo delete a volume, we provide command \ndelete\n. For example, to delete volume \ntest_scone2_volume\n, execute:\n\n\n$ scone volume delete --verbose --manager faye  --as test_scone_user --name  test_scone2_volume\n\n\n\n\n\u00a9 \nscontain.com\n, December 2017. \nQuestions or Suggestions?",
            "title": "manual scone volume"
        },
        {
            "location": "/SCONE_VOLUME/#scone-volume",
            "text": "scone volume  provides functionality to create  volumes  that are available on all nodes of a swarm. This is handy in case you want to be able to schedule a service on any node of a swarm and still be able to give it access to  volumes.  The underlying technology of  scone volume  is  infinit  - a software created by a Docker Inc subsidiary. Please read the  infinit  documentation to understand the infinit concepts of  user ,  network , and  silo .  NOTE: infinit is labeled as alpha software: use this only for development or testing .   The performance of  infinit  needs to be improved. Hence, for production, one would most likely install a more mature distributed file system on the swarm nodes.  The remote swarm is specified via command line option  --manager MANAGER . Alternatively, one can export an environment variable  SCONE_MANAGER  - which defines the default swarm to be used by  scone swarm , in case option  --manager  is not given.",
            "title": "scone volume"
        },
        {
            "location": "/SCONE_VOLUME/#scone-volume-commands",
            "text": "scone volume  Description      install  install infinit on all nodes of a swarm.    create  create new volume and install infinit storage platform if required    check  check that all volumes are available on all hosts    delete  delete a volume     scone volume install [OPTIONS]  scone volume install  ensures that infinit is installed on all nodes of a swarm.     Options  Description      --manager MANAGER  manager of swarm (required)    --as user  run as user (default random id)    --network name  name of network to use/create (default scone-network)    --silo name  name of silo to use/create (default scone-silo)    --capacity sz  size of silo in GB (default 10)    --help  show this help message     Notes: \nIf you do not specify a default  user  via option  --user USER , a random default user is created.   If you do not specify a network name via option  --network name , the default network name is set to  scone-network .   You can specify a silo name via potion  --silo name . The default silo name is  \"scone-silo\" .   The capacity of the silo (in GB) is given via   --capacity name . The default capacity is 10GB.  Examples:   To install infinit with a storage capacity of 15 GB on each node, execute the following:  $ scone volume  install --verbose --manager alice  --capacity 15  If you want to keep explicit control over user and network names, execute the following:  $ scone volume  install --verbose --manager alice  --as scone --capacity 5 --silo  my-silo --network scone-networkg  If you want to see the user and network names for a given swarm, execute:  $ scone volume install --manager alice --help  scone volume create  scone volume create  creates a new infinit volume for a given user (option  --as USER ) \nand with a given volume name (via option  --name VOLUME ). If required, it (re-)installs the infinit storage platform. If you are sure that the infinit is already properly installed, pass the  --fast  option to avoid checks and reinstallation of infinit.  This volume will be available on all hosts of a swarm at location:\n    /mnt/infinit/USER/VOLUME     Options  Description      --manager HOST  manager of swarm (required)    --name volume  name of volume to create (required)    --help  show this help message    --as user  run as user (default )    --network name  name of network to use/create (default )    --silo name  name of silo to use/create (default )    --capacity sz  size of silo in GB (default )    --fast  create volume without reinstalling all software (default=false)    --verbose  print location of created volume     Example:   In case you already installed infinit on your swarm and you just want to create a new volume,\nexecute the following:  $ scone volume  create --verbose --fast --name new-volume --verbose  In case you do not know if infinit is already installed when you create a volume and\nyour want to keep control over the names used by infinit, execute the following:  $ scone volume  create --verbose --manager alice  --as scone --name  my-volume --capacity 5 --silo  my-silo --network scone-network  --export config",
            "title": "scone volume commands"
        },
        {
            "location": "/SCONE_VOLUME/#scone-volume-check",
            "text": "Checks if all created volumes are available on all swarm nodes.  Note: the current implementation of  scone volume check  uses the metadata stored in the container/environment in which  scone volume check  executes. In case you created some volumes in a different container, the checks will complain.   Will plan to fix this issue in a future version of  scone volume check .     Options  Description      --manager HOST  manager of swarm (required)     Example:    $ scone volume  check --verbose --manager alice",
            "title": "scone volume check"
        },
        {
            "location": "/SCONE_VOLUME/#scone-volume-delete",
            "text": "scone volume delete  deletes a volume. This removes the volume from all nodes of a swarm.     Options  Description      --manager HOST  manager of swarm (required)    --name volume  name of volume to be deleted (required)     Example:  $ scone volume  delete --verbose --manager alice --volume new-volume",
            "title": "scone volume delete"
        },
        {
            "location": "/SCONE_VOLUME/#example",
            "text": "To install  infinit  on all nodes of a swarm managed by node  faye  and reserve a capacity of 15GB per node, execute the following:  $ scone volume  install --verbose --manager faye  --capacity 15   To create a volume named  test_scone_volume  for some arbitrary infinit user  test_scone_user , \nexecute the following:  $ scone volume create --verbose --manager faye --as test_scone_user --name  test_scone_volume   Note that the infinit user  test_scone_user  is automatically created if it does not yet exists.  After completion of this command, you will have access to the newly created volume at location  /mnt/infinit/test_scone_user/test_scone_volume  on all nodes of the swarm.  You can give a service access to this volume with the help of its stack file.\nSay, the service needs to access this mapped at  /mnt/vol   as follows:  myservice:\n   volumes:\n        - /mnt/infinit/test_scone_user/test_scone_volume:/mnt/vol  Note that  scone volume create  will check that  infinit  is properly installed and will reinstall parts that\nare missing. If you are sure that infinit is properly installed, you can add option  --fast  to omit the checks that  infinit  is properly installed.  To create a volume  test_scone2_volume  on a swarm managed by  faye  when you know that  infinit  is installed, just executed  $ scone volume  create --verbose --fast --manager faye --as test_scone_user  --name  test_scone2_volume   To check that the volumes on the swarm are properly installed, execute command  check :  $ scone volume  check --verbose --manager faye   To delete a volume, we provide command  delete . For example, to delete volume  test_scone2_volume , execute:  $ scone volume delete --verbose --manager faye  --as test_scone_user --name  test_scone2_volume  \u00a9  scontain.com , December 2017.  Questions or Suggestions?",
            "title": "Example"
        },
        {
            "location": "/SCONE_Fileshield/",
            "text": "SCONE File Protection\n\n\nSCONE supports the transparent encryption and/or authentication of files. By \ntransparent\n, we mean that there are no application code changes needed to support this. We support two ways to use the SCONE file protection:\n\n\n\n\na \nlow-level interface\n intended to be used at the developer site. We assumet that the developer machine is sufficiently trust worthy. This is made available via command \nscone fspf\n and described in this document.\n\n\na \nhigh-level interface\n simplifies the use of the file protection and it does and in particular, takes care of key management. (\nThe high-level interface is not yet available\n).\n\n\n\n\nConcepts\n\n\nThe underlying idea of SCONE file protection is that a user specifies that each file is \neither\n: \n\n\n\n\n\n\nauthenticated\n, i.e., SCONE checks that the content was not modified by some unauthorized entity,\n\n\n\n\n\n\nencrypted\n, i.e., the confidentiality is protected by encryption. Encrypted files are always authenticated, or \n\n\n\n\n\n\nnot-protected\n, i.e. SCONE reads and write  the files without any extra protection mechanisms. For example, you might use \nnot-protected\n if your application already encrypts its files or if you need direct access to devices.\n\n\n\n\n\n\nMarking all files individually as either \nauthenticated\n, \nencrypted\n, or \nnot-protected\n would not be very practical. Hence, we support to partition the filesystem into \nregions\n: regions do not overlap and each file belongs to exactly one region.\n\n\nA region is defined by a path. For example, region \n/\n is the root region and you could, for example, specify that all files in region \n/\n must be authenticated. You can define a second region, for example,  region \n/data/db\n and that this region is encrypted.\n\n\nEach file belongs to exactly one region: it belongs to the region that has the longest common path prefix with this file. For example, file \n/etc/db.conf\n would belong, in this case, to region \n/\n and file \n/data/db/table.db\n would belong to region \n/data/db\n.\n\n\nSCONE supports \nephemeral\n regions: files are stored in main memory outside of the enclave. Since the main memory is not protected, we recommend that an ephemeral regions is either authenticated or encrypted.  When a program starts, all its ephemeral regions are empty. The only way to add files to an ephemeral region is by the application writing to this region. \nAll files in an ephemeral region are lost when the application exits.\n\n\nAll files that need to be persistent should be stored in a non-ephemeral region instead. We refer to this as a \nkernel\n  region. For each region, you need to specify if the region is either \nephemeral\n or \nkernel\n.\n\n\nEach region belongs to one of the following six classes:\n   \n{\nephemeral\n | \nkernel\n} X {\nnot-protected\n | \nauthenticated\n | \nencrypted\n }\n\n\nExample\n\n\nSometimes, we might only need to protect the files that are passed to a container via some volume. In this case, it would be sufficient that the volume is either authenticated or encrypted.\n\n\nLet us demonstrate this via a simple example in which we pass an encrypted volume to a container. We create this encrypted volume in our local filesystem (in directory \nvolume\n) and we will later mount this in the container as \n/data\n. The original (non-encrypted) files are stored in directory \ndata-original\n.\n\n\n> mkdir -p volume\n> mkdir -p data-original\n\n\n\n\nLet's write some files in the \ndata-original\n directory:\n\n\ncat > data-original/hello.txt << EOF\nHello World\nEOF\ncat > data-original/world.py << EOF\nf = open('/data/hello.txt', 'r')\nprint str(f.read())\nEOF\n\n\n\n\nLet's check that \nvolume\n is empty and we print the hash values of the two files in \ndata-original\n:\n\n\n> ls volume\n> shasum data-original/*\n648a6a6ffffdaa0badb23b8baf90b6168dd16b3a  data-original/hello.txt\ndeda99d44e880ea8f2250f45c5c20c15d568d84c  data-original/world.py\n\n\n\n\nNow, we start the SCONE crosscompiler in a container to create the encrypted volume:\n\n\n> docker run -it -v \"$PWD/volume:/data\" -v \"$PWD/data-original:/data-original\" sconecuratedimages/crosscompilers:scone\n\n\n\n\nFile System Protection File\n\n\nAll the metadata required for checking the consistency of the files is stored in a \nfile system protection file\n, or, short \nfspf\n. SCONE supports multiple \nfspf\ns. \n\n\nLet's start with a simple example with a single \nfspf\n. The \nfspf\n file is created via command \nscone fspf create\n and let us name this file \nfspf.pb\n. We execute the following commands\ninside the container (as indicated by the $ prompt):\n\n\n$ cd /data\n$ scone fspf create fspf.pb\nCreated empty file system protection file in fspf.pb. AES-GCM tag: 0e3da7ad62f5bc7c7bb08c67b16f2423\n\n\n\n\nWe can now split the file system in \nregions\n, a region is a subtree. You can add regions to a \nfspf\n with the help of command \nscone fspf addr\n.\n\n\nEach region has exactly one of the following properties:\n\n\n\n\nauthenticated\n: the integrity of files is checked, i.e., any unauthorized modification of this file is detected and results in a reading error inside of the enclave. Specify command line option \n--authenticated\n.\n\n\nencrypted\n: the confidentiality and integrity of files is protected, i.e., encrypted always implies that the files are also authenticated. Specify command line option \n--encrypted\n.\n\n\nnot-protected\n: files are neither authenticated nor encrypted. Specify command line option \n--not-protected\n.\n\n\n\n\nFile system changes of containers are typically ephemeral in the sense that file updates are lost when a container terminates. When specifying option \n--ephemeral\n, files in this region are not written to disk, the are written to an in memory file system instead. \n\n\nSay for now, that by default we do not protect files and we want to read files and write  back  changed files to the file system. To do so, we define that the root tree is  \n--kernel\n as well as  \n--not-protected\n:\n\n\n$ scone fspf addr fspf.pb / --kernel / --not-protected\nAdded region / to file system protection file fspf.pb new AES-GCM tag: dd961af10b5aaa5cb1044c35a3f42c84\n\n\n\n\nLet us add another region \n/data\n that should be encrypted and persisted. To encrypt the files,  we specify option \n--encrypted\n. We specify option \n--kernel\n followed by a path (here, also \n/data\n) to request that files in this region are written to directory \n/data\n.\n\n\n$ scone fspf addr fspf.pb /data --encrypted --kernel /data\nAdded region /data to file system protection file fspf.pb new AES-GCM tag: 8481369d3ffdd9b6aeb30d044bf5c1c7\n\n\n\n\nThe encryption key for a file is chosen at random and stored in \nfspf.pb\n. We use the Intel random number generator \nRdRand\n to generate the key. The default key length of a region is 32 bytes. Alternatives are key length of 16 and 24 bytes. These can be selected via option  \n--key-length 16\n and \n--key-length 24\n when creating a region with command \nscone fspf addr\n.\n\n\nNow, that we defined the regions, i.e., \n/\n and \n/data\n, we can add files to region \n/data\n. Let's just add all files in \n/data-original\n and encrypt these and write the encrypted files to \n/data\n:\n\n\n$ scone fspf addf fspf.pb /data /data-original /data\nAdded files to file system protection file fspf.pb new AES-GCM tag: 39a268166e628cf76e3fca80aa2d4f63\n\n\n\n\nWe can now compare the hash values of the original files and the encrypted files:\n\n\n$ shasum /data/*\nshasum /data/*\n87fd97468024e3d2864516ff5840e15d9615340d  /data/fspf.pb\n31732914910f4a08b9832c442074b0932915476c  /data/hello.txt\n8d07f3f576785c373a5e70e8dbcfa8ee06ca6d0c  /data/world.py\n\n\n\n\nThe fspf itself is not yet encrypted. We encrypt this file via command\n\nscone fspf encrypt fspf.pb\n\n\n$ scone fspf encrypt fspf.pb > /data-original/keytag\n\n\n\n\nWe store the random encryption key as well as the tag of file \nfspf.pb\n in\nfile  \n/data-original/keytag\n.\n\n\nWe introduce a very simple program that reads the two files:\n\n\n$ cat > example.c << EOF\n#include <stdio.h>\n#include <stdlib.h>\n\nvoid printfile(const char* fn) {\n    FILE *fp = fopen(fn, \"r\");\n    char c;\n    while((c=fgetc(fp))!=EOF){\n        printf(\"%c\",c);\n    }\n    fclose(fp);\n}\n\nint main() {\n    printfile(\"/data/hello.txt\");\n    printfile(\"/data/world.py\");\n}\nEOF\n\n\n\n\nLet's crosscompile this program:\n\n\nscone gcc example.c -o example\n\n\n\n\nExecuting this program results in an output like this:\n\n\n$./example\nR??C?\n    q?z??E??|\u042e?}\u00fc ?o\n$??!rga??\u0387*`?????????Gw?\n\n\n\n\nWe need to activate the file system shield via environment variables by setting the location of the file system protection file (in \nSCONE_FSPF\n), the encryption key of the file (in \nSCONE_FSPF_KEY\n) and the tag of the fspf (in \nSCONE_FSPF_TAG\n). \nWe can extract the encryption key as well as the tag of \nfspf.pb\n from file \n/data-original/keytag\n:\n\n\n$ export SCONE_FSPF_KEY=$(cat /data-original/keytag | awk '{print $11}')\n$ export SCONE_FSPF_TAG=$(cat /data-original/keytag | awk '{print $9}')\n$ export SCONE_FSPF=/data/fspf.pb\n\n\n\n\nWe can now execute this program again:\n\n\n$ ./example\nHello World\nf = open('/data/hello.txt', 'r')\nprint str(f.read())\n\n\n\n\nVariables \nSCONE_FSPF_KEY\n, \nSCONE_FSPF_TAG\n and \nSCONE_FSPF\n should only be set manually for debugging since they cannot securely be passed in this way to programs running inside enclaves. To securely pass environment variables, please read the section about \nend-to-end encryption\n.\n\n\nPython\n\n\nLet's try a similar approach for Python. \nIn the above example, we encrypted a Python program. Let's try to execute this encrypted program that accesses an encrypted file:\n\n\ndocker run -it -v \"$PWD/volume:/data\" sconecuratedimages/crosscompilers:python27 bash\n\n\n\n\nThe files \n/data/world.py\n and  \n/data/hello.txt\n are encrypted:\n\n\n$ cat /data/world.py\n?=??J??0?6+?Q?nKd?*N,??.?G???????R?cO?t?y??>f?\n\n\n\n\nLet's activate the file shield:\n\n\n$ export SCONE_FSPF_KEY=... extract from data-original/keytag ...\n$ export SCONE_FSPF_TAG=... extract from data-original/keytag ...\n$ export SCONE_FSPF=/data/fspf.pb\n\n\n\n\nWe can now run the encrypted \nworld.py\n program with the  the Python interpreter:\n\n\nSCONE_HEAP=100000000 SCONE_ALPINE=1 SCONE_VERSION=1 /usr/local/bin/python /data/world.py\nexport SCONE_QUEUES=1\n...\nHello World\n\n\n\n\nProtecting the Root Region\n\n\nNote that the the protection in the above Python example is \ninsufficient\n: Python will read files outside of the protected directory \n/data\n. This could be used to run modified code inside of the Python enclave. \n\n\nTo deal with this threat, we recommend to use a \nwhite list\n approach: only files that are either authenticated or encrypted should be accessible by an enclave, i.e., one should avoid to use \nnot-protected\n regions. We support \nnot-protected\n regions to give your application, for example, direct access to devices needed by your application. \n\n\nWe typically want to authenticate all files in region \n/\n:\n\n\n$ scone fspf addr fspf.pb / --kernel / --authenticated\n\n\n\n\nWe need to add all files that our application might access. Often, these files in the root region might be defined in some container image. Let's see how we can add these files to our region \n/\n.\n\n\nAdding files from an existing container image\n\n\nWe show how to add a subset of the files of container image \nsconecuratedimages/crosscompilers:python27\n to our root region.\nTo do so, we ensure that we have the newest images:\n\n\n> docker pull sconecuratedimages/crosscompilers:python27\n> docker pull sconecuratedimages/crosscompilers:scone\n\n\n\n\nHow can we add all files in a container to the \nfspf\n? One way to do so requires to run Docker inside of a Docker container. To be able to do so, we need to permit our outermost docker container to have access to  \n/var/run/docker.sock\n:\n\n\n> docker run -it -v /var/run/docker.sock:/var/run/docker.sock -v \"$PWD/volume:/data\" -v \"$PWD/data-original:/data-original\" sconecuratedimages/crosscompilers:scone\n\n\n\n\nLet us ensure that Docker is installed in this container (should be by default):\n\n\napt-get update\napt-get install -y docker.io\n\n\n\n\nNow, we want to add all files of some target container. In our example,\nthis is an instance of image \nsconecuratedimages/crosscompilers:python27\n.\nWe ensure that we pulled the latest image before we start the container:\n\n\nCONTAINER_ID=`docker run -d sconecuratedimages/crosscompilers:python27 printf OK` \n\n\n\n\nWe can now copy all files from this container into a new directory \nrootvol\n:\n\n\n$ cd\n$ mkdir -p rootvol\n$ docker cp $CONTAINER_ID:/ ./rootvol\n\n\n\n\nNow that we have a copy of the files, we should not forget to garbage collect this container:\n\n\ndocker rm $CONTAINER_ID\n\n\n\n\nLet's remove some directories that we do not want our program to access, like for example, \n/dev\n:\n\n\n$ rm -rf rootvol/dev rootvol/proc rootvol/bin rootvol/media rootvol/mnt rootvol/usr/share/X11 rootvol/usr/share/terminfo rootvol/optrootvol/usr/include/c++/ rootvol/usr/lib/tcl8.6 rootvol/usr/lib/gcc rootvol/opt rootvol/sys rootvol/usr/include/c++\n\n\n\n\nNow, we create a root \nfspf\n:\n\n\n$ scone fspf create fspf.pb\n$ scone fspf addr fspf.pb / --kernel / --authenticated\n$ scone fspf addf fspf.pb / ./rootvol /\n$ scone fspf encrypt fspf.pb > keytag\n\n\n\n\nWe can now create a new container image with this file system protection file\nusing this Dockerfile\n\n\n$ cat > Dockerfile << EOF\nFROM sconecuratedimages/crosscompilers:python27\nCOPY fspf.pb /\nEOF\n$ docker build -t sconecuratedimages/crosscompilers:python27-authenticated .\n\n\n\n\nWe can run a container as follows:\n\n\n$ docker run -it sconecuratedimages/crosscompilers:python27-authenticated sh\n\n\n\n\nLet us activate the file shield:\n\n\n$ export SCONE_FSPF_KEY=... extract from data-original/keytag ...\n$ export SCONE_FSPF_TAG=... extract from data-original/keytag ...\n$ export SCONE_FSPF=/fspf.pb\n\n\n\n\nLet's run python with authenticated file system:\n\n\nSCONE_HEAP=1000000000 SCONE_ALLOW_DLOPEN=2  SCONE_ALPINE=1 SCONE_VERSION=1 /usr/local/bin/python\n\n\n\n\nChecking the File System Shield\n\n\nLet's us check the file shield by creating a new python program (\nhelloworld-manual.py\n) in\nside of a python container:\n\n\n> docker run -i sconecuratedimages/crosscompilers:python27-authenticated sh\n$ cat > helloworld-manual.py << EOF\nprint \"Hello World\"\nEOF\n\n\n\n\nWhen we switch on the file shield, the execution of this program inside the enclave will fail: since this file was not part of the original file system, the file system shield will prevent accessing this file.\n\n\n$ export SCONE_FSPF_KEY=... extract from data-original/keytag ...\n$ export SCONE_FSPF_TAG=... extract from data-original/keytag ...\n$ export SCONE_FSPF=/fspf.pb\n$ SCONE_HEAP=1000000000 SCONE_ALLOW_DLOPEN=2  SCONE_ALPINE=1 SCONE_VERSION=1 /usr/local/bin/python helloworld-manual.py\n(fails)\n\n\n\n\nWe can, however, add a new file via programs that have access to the key of the \nfspf\n. We can, for example, write a python program to add a new python program to the file system.\n\n\nBy default, we disable that the root fspf is updated. We can enable updates by setting environment variable \nSCONE_FSPF_MUTABLE=1\n. We plan to permit updates of the root fspf by default in the near future (i.e., we will remove variable \nSCONE_FSPF_MUTABLE=1\n).\n\n\n$ SCONE_HEAP=1000000000 SCONE_FSPF_MUTABLE=1 SCONE_ALLOW_DLOPEN=2  SCONE_ALPINE=1 SCONE_VERSION=1 /usr/local/bin/python  << PYTHON\nf = open('helloworld.py', 'w')\nf.write('print \"Hello World\"\\n')\nf.close()\nPYTHON```\n\n\n\n\nThe tag of the file system protection file is now changed. We can determine the new TAG with the help of command \nscone fspf show\n:\n\n\n$ export SCONE_FSPF_TAG=$(scone fspf show --tag /fspf.pb)\n\n\n\n\nNow, we can run the new \nhelloworld.py\n:\n\n\n$ SCONE_HEAP=1000000000 SCONE_ALLOW_DLOPEN=2  SCONE_ALPINE=1 SCONE_VERSION=1 /usr/local/bin/python helloworld.py\n...\nHello World\n\n\n\n\nExtended Example\n\n\nTo learn how to use multiple file system protection files, \nplease have a look at the following screencast.\n\n\n\n\nBelow is the script that is executed in the screencast:\n\n\n> docker run -it -v $PWD:/mnt sconecuratedimages/crosscompilers:scone\n\n$ mkdir -p /example\n$ mkdir -p /mnt/authenticated/\n$ mkdir -p /mnt/encrypted/\n$ cd /example\n$ mkdir -p .original\n\n$ scone fspf create fspf.pb\n$ scone fspf create authenticated.pb\n$ scone fspf create encrypted.pb\n\n# add protection regions\n$ scone fspf addr fspf.pb / -e --ephemeral\n$ scone fspf addr authenticated.pb /mnt/authenticated -a --kernel /mnt/authenticated\n$ scone fspf addr encrypted.pb /mnt/encrypted -e --kernel /mnt/encrypted\n\n# add files\n\n# enclave program should expect the files (directories) found by the client in ./original in /mnt/authenticated\n$ scone fspf addf authenticated.pb /mnt/authenticated ./original\n\n# enclave program should expect the files (directories) found by the client in ./original in encrypted form in /mnt/encrypted\n# the client will write the encrypted files to ./mnt/encrypted\n$ scone fspf addf encrypted.pb /mnt/encrypted ./original ./mnt/encrypted\nencrypted_key=`scone fspf encrypt encrypted.pb | awk '{print $11}'`\n\n$ echo \"encrypted.pb key: ${encrypted_key}\"\n$ scone fspf addfspf fspf.pb authenticated.pb\n$ scone fspf addfspf fspf.pb encrypted.pb ${encrypted_key}\n$ cat > example.c << EOF\n#include <stdio.h>\n\nint main() {\n    FILE *fp = fopen(\"/mnt/authenticated/hello\", \"w\");\n    fprintf(fp, \"hello world\\n\");\n    fclose(fp);\n\n    fp = fopen(\"/mnt/encrypted/hello\", \"w\");\n    fprintf(fp, \"hello world\\n\");\n    fclose(fp);\n}\nEOF\n\n$ scone gcc example.c -o sgxex\n$ cat > /etc/sgx-musl.conf << EOF\nQ 4\ne -1 0 0\ns -1 0 0\ne -1 1 0\ns -1 1 0\ne -1 2 0\ns -1 2 0\ne -1 3 0\ns -1 3 0\nEOF\n$ SCONE_FSPF=fspf.pb ./sgxex\n$ cat /mnt/authenticated/hello\n$ cat /mnt/encrypted/hello \n$ cat > cat.c << EOF\n#include <stdio.h>\n\nint main() {\n    char buf[80];\n    FILE *fp = fopen(\"/mnt/authenticated/hello\", \"r\");\n    fgets(buf, sizeof(buf), fp);\n    fclose(fp);\n    printf(\"read: '%s'\\n\", buf);\n\n    fp = fopen(\"/mnt/encrypted/hello\", \"r\");\n    fgets(buf, sizeof(buf), fp);\n    fclose(fp);\n    printf(\"read: '%s'\\n\", buf);\n}\nEOF\n\n$ scone gcc cat.c -o native_cat\n$ ./native_cat\n$ scone gcc cat.c -o sgxcat\n$ SCONE_FSPF=fspf.pb ./sgxcat\n\n\n\n\nNotes\n\n\nThe SCONE File Protection documentation is not yet completed and more information will be provided soon.\n\n\n\u00a9 \nscontain.com\n, January 2018. \nQuestions or Suggestions?",
            "title": "manual scone fspf"
        },
        {
            "location": "/SCONE_Fileshield/#scone-file-protection",
            "text": "SCONE supports the transparent encryption and/or authentication of files. By  transparent , we mean that there are no application code changes needed to support this. We support two ways to use the SCONE file protection:   a  low-level interface  intended to be used at the developer site. We assumet that the developer machine is sufficiently trust worthy. This is made available via command  scone fspf  and described in this document.  a  high-level interface  simplifies the use of the file protection and it does and in particular, takes care of key management. ( The high-level interface is not yet available ).",
            "title": "SCONE File Protection"
        },
        {
            "location": "/SCONE_Fileshield/#concepts",
            "text": "The underlying idea of SCONE file protection is that a user specifies that each file is  either :     authenticated , i.e., SCONE checks that the content was not modified by some unauthorized entity,    encrypted , i.e., the confidentiality is protected by encryption. Encrypted files are always authenticated, or     not-protected , i.e. SCONE reads and write  the files without any extra protection mechanisms. For example, you might use  not-protected  if your application already encrypts its files or if you need direct access to devices.    Marking all files individually as either  authenticated ,  encrypted , or  not-protected  would not be very practical. Hence, we support to partition the filesystem into  regions : regions do not overlap and each file belongs to exactly one region.  A region is defined by a path. For example, region  /  is the root region and you could, for example, specify that all files in region  /  must be authenticated. You can define a second region, for example,  region  /data/db  and that this region is encrypted.  Each file belongs to exactly one region: it belongs to the region that has the longest common path prefix with this file. For example, file  /etc/db.conf  would belong, in this case, to region  /  and file  /data/db/table.db  would belong to region  /data/db .  SCONE supports  ephemeral  regions: files are stored in main memory outside of the enclave. Since the main memory is not protected, we recommend that an ephemeral regions is either authenticated or encrypted.  When a program starts, all its ephemeral regions are empty. The only way to add files to an ephemeral region is by the application writing to this region.  All files in an ephemeral region are lost when the application exits.  All files that need to be persistent should be stored in a non-ephemeral region instead. We refer to this as a  kernel   region. For each region, you need to specify if the region is either  ephemeral  or  kernel .  Each region belongs to one of the following six classes:\n    { ephemeral  |  kernel } X { not-protected  |  authenticated  |  encrypted  }",
            "title": "Concepts"
        },
        {
            "location": "/SCONE_Fileshield/#example",
            "text": "Sometimes, we might only need to protect the files that are passed to a container via some volume. In this case, it would be sufficient that the volume is either authenticated or encrypted.  Let us demonstrate this via a simple example in which we pass an encrypted volume to a container. We create this encrypted volume in our local filesystem (in directory  volume ) and we will later mount this in the container as  /data . The original (non-encrypted) files are stored in directory  data-original .  > mkdir -p volume\n> mkdir -p data-original  Let's write some files in the  data-original  directory:  cat > data-original/hello.txt << EOF\nHello World\nEOF\ncat > data-original/world.py << EOF\nf = open('/data/hello.txt', 'r')\nprint str(f.read())\nEOF  Let's check that  volume  is empty and we print the hash values of the two files in  data-original :  > ls volume\n> shasum data-original/*\n648a6a6ffffdaa0badb23b8baf90b6168dd16b3a  data-original/hello.txt\ndeda99d44e880ea8f2250f45c5c20c15d568d84c  data-original/world.py  Now, we start the SCONE crosscompiler in a container to create the encrypted volume:  > docker run -it -v \"$PWD/volume:/data\" -v \"$PWD/data-original:/data-original\" sconecuratedimages/crosscompilers:scone  File System Protection File  All the metadata required for checking the consistency of the files is stored in a  file system protection file , or, short  fspf . SCONE supports multiple  fspf s.   Let's start with a simple example with a single  fspf . The  fspf  file is created via command  scone fspf create  and let us name this file  fspf.pb . We execute the following commands\ninside the container (as indicated by the $ prompt):  $ cd /data\n$ scone fspf create fspf.pb\nCreated empty file system protection file in fspf.pb. AES-GCM tag: 0e3da7ad62f5bc7c7bb08c67b16f2423  We can now split the file system in  regions , a region is a subtree. You can add regions to a  fspf  with the help of command  scone fspf addr .  Each region has exactly one of the following properties:   authenticated : the integrity of files is checked, i.e., any unauthorized modification of this file is detected and results in a reading error inside of the enclave. Specify command line option  --authenticated .  encrypted : the confidentiality and integrity of files is protected, i.e., encrypted always implies that the files are also authenticated. Specify command line option  --encrypted .  not-protected : files are neither authenticated nor encrypted. Specify command line option  --not-protected .   File system changes of containers are typically ephemeral in the sense that file updates are lost when a container terminates. When specifying option  --ephemeral , files in this region are not written to disk, the are written to an in memory file system instead.   Say for now, that by default we do not protect files and we want to read files and write  back  changed files to the file system. To do so, we define that the root tree is   --kernel  as well as   --not-protected :  $ scone fspf addr fspf.pb / --kernel / --not-protected\nAdded region / to file system protection file fspf.pb new AES-GCM tag: dd961af10b5aaa5cb1044c35a3f42c84  Let us add another region  /data  that should be encrypted and persisted. To encrypt the files,  we specify option  --encrypted . We specify option  --kernel  followed by a path (here, also  /data ) to request that files in this region are written to directory  /data .  $ scone fspf addr fspf.pb /data --encrypted --kernel /data\nAdded region /data to file system protection file fspf.pb new AES-GCM tag: 8481369d3ffdd9b6aeb30d044bf5c1c7  The encryption key for a file is chosen at random and stored in  fspf.pb . We use the Intel random number generator  RdRand  to generate the key. The default key length of a region is 32 bytes. Alternatives are key length of 16 and 24 bytes. These can be selected via option   --key-length 16  and  --key-length 24  when creating a region with command  scone fspf addr .  Now, that we defined the regions, i.e.,  /  and  /data , we can add files to region  /data . Let's just add all files in  /data-original  and encrypt these and write the encrypted files to  /data :  $ scone fspf addf fspf.pb /data /data-original /data\nAdded files to file system protection file fspf.pb new AES-GCM tag: 39a268166e628cf76e3fca80aa2d4f63  We can now compare the hash values of the original files and the encrypted files:  $ shasum /data/*\nshasum /data/*\n87fd97468024e3d2864516ff5840e15d9615340d  /data/fspf.pb\n31732914910f4a08b9832c442074b0932915476c  /data/hello.txt\n8d07f3f576785c373a5e70e8dbcfa8ee06ca6d0c  /data/world.py  The fspf itself is not yet encrypted. We encrypt this file via command scone fspf encrypt fspf.pb  $ scone fspf encrypt fspf.pb > /data-original/keytag  We store the random encryption key as well as the tag of file  fspf.pb  in\nfile   /data-original/keytag .  We introduce a very simple program that reads the two files:  $ cat > example.c << EOF\n#include <stdio.h>\n#include <stdlib.h>\n\nvoid printfile(const char* fn) {\n    FILE *fp = fopen(fn, \"r\");\n    char c;\n    while((c=fgetc(fp))!=EOF){\n        printf(\"%c\",c);\n    }\n    fclose(fp);\n}\n\nint main() {\n    printfile(\"/data/hello.txt\");\n    printfile(\"/data/world.py\");\n}\nEOF  Let's crosscompile this program:  scone gcc example.c -o example  Executing this program results in an output like this:  $./example\nR??C?\n    q?z??E??|\u042e?}\u00fc ?o\n$??!rga??\u0387*`?????????Gw?  We need to activate the file system shield via environment variables by setting the location of the file system protection file (in  SCONE_FSPF ), the encryption key of the file (in  SCONE_FSPF_KEY ) and the tag of the fspf (in  SCONE_FSPF_TAG ). \nWe can extract the encryption key as well as the tag of  fspf.pb  from file  /data-original/keytag :  $ export SCONE_FSPF_KEY=$(cat /data-original/keytag | awk '{print $11}')\n$ export SCONE_FSPF_TAG=$(cat /data-original/keytag | awk '{print $9}')\n$ export SCONE_FSPF=/data/fspf.pb  We can now execute this program again:  $ ./example\nHello World\nf = open('/data/hello.txt', 'r')\nprint str(f.read())  Variables  SCONE_FSPF_KEY ,  SCONE_FSPF_TAG  and  SCONE_FSPF  should only be set manually for debugging since they cannot securely be passed in this way to programs running inside enclaves. To securely pass environment variables, please read the section about  end-to-end encryption .",
            "title": "Example"
        },
        {
            "location": "/SCONE_Fileshield/#python",
            "text": "Let's try a similar approach for Python. \nIn the above example, we encrypted a Python program. Let's try to execute this encrypted program that accesses an encrypted file:  docker run -it -v \"$PWD/volume:/data\" sconecuratedimages/crosscompilers:python27 bash  The files  /data/world.py  and   /data/hello.txt  are encrypted:  $ cat /data/world.py\n?=??J??0?6+?Q?nKd?*N,??.?G???????R?cO?t?y??>f?  Let's activate the file shield:  $ export SCONE_FSPF_KEY=... extract from data-original/keytag ...\n$ export SCONE_FSPF_TAG=... extract from data-original/keytag ...\n$ export SCONE_FSPF=/data/fspf.pb  We can now run the encrypted  world.py  program with the  the Python interpreter:  SCONE_HEAP=100000000 SCONE_ALPINE=1 SCONE_VERSION=1 /usr/local/bin/python /data/world.py\nexport SCONE_QUEUES=1\n...\nHello World",
            "title": "Python"
        },
        {
            "location": "/SCONE_Fileshield/#protecting-the-root-region",
            "text": "Note that the the protection in the above Python example is  insufficient : Python will read files outside of the protected directory  /data . This could be used to run modified code inside of the Python enclave.   To deal with this threat, we recommend to use a  white list  approach: only files that are either authenticated or encrypted should be accessible by an enclave, i.e., one should avoid to use  not-protected  regions. We support  not-protected  regions to give your application, for example, direct access to devices needed by your application.   We typically want to authenticate all files in region  / :  $ scone fspf addr fspf.pb / --kernel / --authenticated  We need to add all files that our application might access. Often, these files in the root region might be defined in some container image. Let's see how we can add these files to our region  / .  Adding files from an existing container image  We show how to add a subset of the files of container image  sconecuratedimages/crosscompilers:python27  to our root region.\nTo do so, we ensure that we have the newest images:  > docker pull sconecuratedimages/crosscompilers:python27\n> docker pull sconecuratedimages/crosscompilers:scone  How can we add all files in a container to the  fspf ? One way to do so requires to run Docker inside of a Docker container. To be able to do so, we need to permit our outermost docker container to have access to   /var/run/docker.sock :  > docker run -it -v /var/run/docker.sock:/var/run/docker.sock -v \"$PWD/volume:/data\" -v \"$PWD/data-original:/data-original\" sconecuratedimages/crosscompilers:scone  Let us ensure that Docker is installed in this container (should be by default):  apt-get update\napt-get install -y docker.io  Now, we want to add all files of some target container. In our example,\nthis is an instance of image  sconecuratedimages/crosscompilers:python27 .\nWe ensure that we pulled the latest image before we start the container:  CONTAINER_ID=`docker run -d sconecuratedimages/crosscompilers:python27 printf OK`   We can now copy all files from this container into a new directory  rootvol :  $ cd\n$ mkdir -p rootvol\n$ docker cp $CONTAINER_ID:/ ./rootvol  Now that we have a copy of the files, we should not forget to garbage collect this container:  docker rm $CONTAINER_ID  Let's remove some directories that we do not want our program to access, like for example,  /dev :  $ rm -rf rootvol/dev rootvol/proc rootvol/bin rootvol/media rootvol/mnt rootvol/usr/share/X11 rootvol/usr/share/terminfo rootvol/optrootvol/usr/include/c++/ rootvol/usr/lib/tcl8.6 rootvol/usr/lib/gcc rootvol/opt rootvol/sys rootvol/usr/include/c++  Now, we create a root  fspf :  $ scone fspf create fspf.pb\n$ scone fspf addr fspf.pb / --kernel / --authenticated\n$ scone fspf addf fspf.pb / ./rootvol /\n$ scone fspf encrypt fspf.pb > keytag  We can now create a new container image with this file system protection file\nusing this Dockerfile  $ cat > Dockerfile << EOF\nFROM sconecuratedimages/crosscompilers:python27\nCOPY fspf.pb /\nEOF\n$ docker build -t sconecuratedimages/crosscompilers:python27-authenticated .  We can run a container as follows:  $ docker run -it sconecuratedimages/crosscompilers:python27-authenticated sh  Let us activate the file shield:  $ export SCONE_FSPF_KEY=... extract from data-original/keytag ...\n$ export SCONE_FSPF_TAG=... extract from data-original/keytag ...\n$ export SCONE_FSPF=/fspf.pb  Let's run python with authenticated file system:  SCONE_HEAP=1000000000 SCONE_ALLOW_DLOPEN=2  SCONE_ALPINE=1 SCONE_VERSION=1 /usr/local/bin/python",
            "title": "Protecting the Root Region"
        },
        {
            "location": "/SCONE_Fileshield/#checking-the-file-system-shield",
            "text": "Let's us check the file shield by creating a new python program ( helloworld-manual.py ) in\nside of a python container:  > docker run -i sconecuratedimages/crosscompilers:python27-authenticated sh\n$ cat > helloworld-manual.py << EOF\nprint \"Hello World\"\nEOF  When we switch on the file shield, the execution of this program inside the enclave will fail: since this file was not part of the original file system, the file system shield will prevent accessing this file.  $ export SCONE_FSPF_KEY=... extract from data-original/keytag ...\n$ export SCONE_FSPF_TAG=... extract from data-original/keytag ...\n$ export SCONE_FSPF=/fspf.pb\n$ SCONE_HEAP=1000000000 SCONE_ALLOW_DLOPEN=2  SCONE_ALPINE=1 SCONE_VERSION=1 /usr/local/bin/python helloworld-manual.py\n(fails)  We can, however, add a new file via programs that have access to the key of the  fspf . We can, for example, write a python program to add a new python program to the file system.  By default, we disable that the root fspf is updated. We can enable updates by setting environment variable  SCONE_FSPF_MUTABLE=1 . We plan to permit updates of the root fspf by default in the near future (i.e., we will remove variable  SCONE_FSPF_MUTABLE=1 ).  $ SCONE_HEAP=1000000000 SCONE_FSPF_MUTABLE=1 SCONE_ALLOW_DLOPEN=2  SCONE_ALPINE=1 SCONE_VERSION=1 /usr/local/bin/python  << PYTHON\nf = open('helloworld.py', 'w')\nf.write('print \"Hello World\"\\n')\nf.close()\nPYTHON```  The tag of the file system protection file is now changed. We can determine the new TAG with the help of command  scone fspf show :  $ export SCONE_FSPF_TAG=$(scone fspf show --tag /fspf.pb)  Now, we can run the new  helloworld.py :  $ SCONE_HEAP=1000000000 SCONE_ALLOW_DLOPEN=2  SCONE_ALPINE=1 SCONE_VERSION=1 /usr/local/bin/python helloworld.py\n...\nHello World",
            "title": "Checking the File System Shield"
        },
        {
            "location": "/SCONE_Fileshield/#extended-example",
            "text": "To learn how to use multiple file system protection files, \nplease have a look at the following screencast.   Below is the script that is executed in the screencast:  > docker run -it -v $PWD:/mnt sconecuratedimages/crosscompilers:scone\n\n$ mkdir -p /example\n$ mkdir -p /mnt/authenticated/\n$ mkdir -p /mnt/encrypted/\n$ cd /example\n$ mkdir -p .original\n\n$ scone fspf create fspf.pb\n$ scone fspf create authenticated.pb\n$ scone fspf create encrypted.pb\n\n# add protection regions\n$ scone fspf addr fspf.pb / -e --ephemeral\n$ scone fspf addr authenticated.pb /mnt/authenticated -a --kernel /mnt/authenticated\n$ scone fspf addr encrypted.pb /mnt/encrypted -e --kernel /mnt/encrypted\n\n# add files\n\n# enclave program should expect the files (directories) found by the client in ./original in /mnt/authenticated\n$ scone fspf addf authenticated.pb /mnt/authenticated ./original\n\n# enclave program should expect the files (directories) found by the client in ./original in encrypted form in /mnt/encrypted\n# the client will write the encrypted files to ./mnt/encrypted\n$ scone fspf addf encrypted.pb /mnt/encrypted ./original ./mnt/encrypted\nencrypted_key=`scone fspf encrypt encrypted.pb | awk '{print $11}'`\n\n$ echo \"encrypted.pb key: ${encrypted_key}\"\n$ scone fspf addfspf fspf.pb authenticated.pb\n$ scone fspf addfspf fspf.pb encrypted.pb ${encrypted_key}\n$ cat > example.c << EOF\n#include <stdio.h>\n\nint main() {\n    FILE *fp = fopen(\"/mnt/authenticated/hello\", \"w\");\n    fprintf(fp, \"hello world\\n\");\n    fclose(fp);\n\n    fp = fopen(\"/mnt/encrypted/hello\", \"w\");\n    fprintf(fp, \"hello world\\n\");\n    fclose(fp);\n}\nEOF\n\n$ scone gcc example.c -o sgxex\n$ cat > /etc/sgx-musl.conf << EOF\nQ 4\ne -1 0 0\ns -1 0 0\ne -1 1 0\ns -1 1 0\ne -1 2 0\ns -1 2 0\ne -1 3 0\ns -1 3 0\nEOF\n$ SCONE_FSPF=fspf.pb ./sgxex\n$ cat /mnt/authenticated/hello\n$ cat /mnt/encrypted/hello \n$ cat > cat.c << EOF\n#include <stdio.h>\n\nint main() {\n    char buf[80];\n    FILE *fp = fopen(\"/mnt/authenticated/hello\", \"r\");\n    fgets(buf, sizeof(buf), fp);\n    fclose(fp);\n    printf(\"read: '%s'\\n\", buf);\n\n    fp = fopen(\"/mnt/encrypted/hello\", \"r\");\n    fgets(buf, sizeof(buf), fp);\n    fclose(fp);\n    printf(\"read: '%s'\\n\", buf);\n}\nEOF\n\n$ scone gcc cat.c -o native_cat\n$ ./native_cat\n$ scone gcc cat.c -o sgxcat\n$ SCONE_FSPF=fspf.pb ./sgxcat",
            "title": "Extended Example"
        },
        {
            "location": "/SCONE_Fileshield/#notes",
            "text": "The SCONE File Protection documentation is not yet completed and more information will be provided soon.  \u00a9  scontain.com , January 2018.  Questions or Suggestions?",
            "title": "Notes"
        },
        {
            "location": "/SCONE_ENV/",
            "text": "SCONE Environment Variables\n\n\nTo simplify development and debugging, SCONE supports a range of environment variables to control its behavior. These environment variables are mainly used for development and debugging. In operational settings, the configuration would be provided in a secure fashion via the SCONE configuration and attestation service.\n\n\nAlso, the performance of SCONE-based applications can be tuned by selecting the appropriate configuration for an application. We have tool support to automatically tune these parameters.\n\n\nSCONE Configuration File\n\n\nThe location of the SCONE configuration file can be controlled with an environment variable:\n\n\n\n\nSCONE_CONFIG\n: if defined, this determines the path of SCONE configuration file. If this environment variable is not defined or the file cannot be opened,  the default configuration file located in \n/etc/sgx-musl.conf\n is read instead. If the default configuration file cannot be read either, the program exits with an error message.\n\n\n\n\nChanging the location of the configuration file is, for example, useful in the context of testing or when you run your application outside of a container when you want to run different applications with different configurations inside of enclaves.\n\n\nThe configuration file can define most of the  behaviors that one can define via environment variables.\nHowever, the \nSCONE_* \n environment variables have higher priority than the settings from the config file.\n\n\nFormat of SCONE Configuration File\n\n\nThe format for the configuration file: on each line, there is a\nstatement beginning with a single-character command code, and up to\nthree numbers. The possible commands currently are:\n\n\n\n\nQ \nn\n  - defines the number of queues used to execute system calls\n\n\nn = number of queues\n: sets the number of syscall-return queue pairs to allocate. This is equivalent to setting the \nSCONE_QUEUES\n environment variable;\n\n\n\n\n\n\nH \ns\n - defines the heap size in bytes\n\n\ns = heap size in bytes\n: sets the size of heap, equivalent to setting \nSCONE_HEAP\n environment variable;\n\n\n\n\n\n\nP \nN\n - determines the backoff behavior of the queues\n\n\nN = spin number\n: equivalent to setting \nSCONE_SSPINS\n environment variable;\n\n\n\n\n\n\nL \nS\n - determines the backoff behavior of the queues\n\n\nS = sleep time\n: equivalent to setting \nSCONE_SSLEEP\n environment variable;\n\n\n\n\n\n\ns \nC Q R\n  - \nsthread\n serving system calls outside of an enclave\n\n\nC = core-no\n: if non-negative number: pin this sthread to core \nC\n; if a negative number, do not pin this thread\n\n\nQ = queue-no in [0..n]\n: this sthreads serves this queue\n\n\nR = realtime?\n: always set this to 0\n\n\n\n\n\n\ne \nC Q R\n  \nethread\n running inside of enclave and executes application threads (which we call lthreads)\n\n\nC = core-no\n: non-negative number: pin to this core; negative number: no pinning ot this thread\n\n\nQ = queue-no in [0..n]\n: this sthreads serves this queue\n\n\nR = realtime?\n: always set this to 0\n\n\n\n\n\n\n\n\nThe number of \nsthreads\n is automatically increased if more \nsthreads\n are needed to process system calls. An \nsthread\n will block if it does not have any work left to do. Hence, we recommend to start exactly one \nsthread\n per \nethread\n.\n\n\nethreads\n will leave the enclave it there is no more work for them to do. Hence, it makes sense to start one ehthread per core. In some situations, it might even make sense to start one \nethread\n per hyper-thread.\n\n\nUnless you want to limit the the number of CPU resources an enclave should use,  the following is a good generic configuration file:\n\n\n$ sudo tee  /etc/sgx-musl.conf << EOF\nQ 4\ne -1 0 0\ns -1 0 0\ne -1 1 0\ns -1 1 0\ne -1 2 0\ns -1 2 0\ne -1 3 0\ns -1 3 0\nEOF\n\n\n\n\nRun Mode\n\n\n\n\n\n\nSCONE_MODE\n: defines if application should run inside of an enclave or outside in simulation mode.\n\n\n\n\n\n\n=HW\n: run program inside of enclave. If program fails to create an enclave, it will fail.\n\n\n\n\n\n\n=SIM\n: run program outside of enclave. All SCONE related software runs but just outside of the enclave. This is not 100% compatible with hardware mode since, for example, some instructions are permitted in native mode that are not permitted in hardware mode.\n\n\n\n\n\n\n=AUTO\n: run program inside of enclave if SGX is available. Otherwise, run in simulation mode. AUTO is the default mode.\n\n\n\n\n\n\n\n\n\n\nSCONE_HEAP\n: size of the heap allocated for the program during the startup of the enclave.\n\n\n\n\n=s\n  the requested heap size in bytes\n\n\n\n\n\n\n\n\nDebug\n\n\n\n\nSCONE_VERSION\n if defined, SCONE will print the values of some of the SCONE environment variables during startup.\n\n\n\n\nExample output:\n\n\nexport SCONE_QUEUES=4\nexport SCONE_SLOTS=256\nexport SCONE_SIGPIPE=0\nexport SCONE_MMAP32BIT=0\nexport SCONE_SSPINS=100\nexport SCONE_SSLEEP=4000\nexport SCONE_KERNEL=0\nexport SCONE_HEAP=1073741824\nexport SCONE_CONFIG=/etc/sgx-musl.conf\nexport SCONE_MODE=hw\nexport SCONE_SGXBOUNDS=no\nexport SCONE_ALLOW_DLOPEN=no\nRevision: 9b355b99170ad434010353bb9f4dca24e532b1b7\nBranch: master\nConfigure options: --enable-file-prot --enable-shared --enable-debug --prefix=/scone/src/built/cross-compiler/x86_64-linux-musl\n\n\n\n\nDynamic library support:\n\n\n\n\n\n\nSCONE_ALLOW_DLOPEN\n: if defined, permit to load shared libraries during startup. All libraries that are loaded during startup are measured and contribute to the hash of the enclave, i.e., the are part of \nMRENCLAVE\n.\n\n\n\n\n\n\nSCONE_ALLOW_DLOPEN=\"2\"\n: not only can dynamic libraries be loaded during startup but also later on. For example, Python programs might dynamically load modules after startup. Our approach to enforce the integrity of these dynamic libraries with the help of a file protection shield.\n\n\n\n\n\n\nPerformance tuning variables:\n\n\n\n\n\n\nSCONE_QUEUES\n: number of systemcall queues to be used.\n\n\n\n\n\n\nSCONE_SLOTS\n: systemcall queue length: must be larger than the maximum number of \nlthreads\n.\n\n\n\n\n\n\nSCONE_SIGPIPE\n: if set to \"1\", \nSIGPIPE\n signals are delivered to the application\n\n\n\n\n\n\nSCONE_SSPINS=N\n: In case an Ethread does not have any lthread to execute, an Ethread first pauses for up to N times to wait for more work to show up. After that, it sleeps for up to N times. Each time increasing the sleep time. \n\n\n\n\n\n\nSCONE_SSLEEP\n: determines how fast we increase the backoff.\n\n\n\n\n\n\nSafety\n\n\n\n\nSCONE_SGXBOUNDS\n: must be defined to enable bounds checking. Furthermore, you need to compile your program with our SGX boundschecker.\n\n\n\n\nDynamic link loader\n\n\nThe dynamic link loader is part of image \nsconecuratedimages/crosscompilers:runtime\n (\nsee tutorial\n).\n\n\n\n\n\n\nLD_LIBRARY_PATH\n: you can control from where the dynamic link loader looks for shared libraries.\n\n\n\n\n\n\nLD_PRELOAD\n: you can instruct the dynamic link loader to load libraries before loading the libraries specified by the program itself.\n\n\n\n\n\n\nSCONE_ALPINE=1\n: run dynamically-linked program inside of an enclave.\n\n\n\n\n\n\n\u00a9 \nscontain.com\n, November 2017. \nQuestions or Suggestions?",
            "title": "Environment variables"
        },
        {
            "location": "/SCONE_ENV/#scone-environment-variables",
            "text": "To simplify development and debugging, SCONE supports a range of environment variables to control its behavior. These environment variables are mainly used for development and debugging. In operational settings, the configuration would be provided in a secure fashion via the SCONE configuration and attestation service.  Also, the performance of SCONE-based applications can be tuned by selecting the appropriate configuration for an application. We have tool support to automatically tune these parameters.",
            "title": "SCONE Environment Variables"
        },
        {
            "location": "/SCONE_ENV/#scone-configuration-file",
            "text": "The location of the SCONE configuration file can be controlled with an environment variable:   SCONE_CONFIG : if defined, this determines the path of SCONE configuration file. If this environment variable is not defined or the file cannot be opened,  the default configuration file located in  /etc/sgx-musl.conf  is read instead. If the default configuration file cannot be read either, the program exits with an error message.   Changing the location of the configuration file is, for example, useful in the context of testing or when you run your application outside of a container when you want to run different applications with different configurations inside of enclaves.  The configuration file can define most of the  behaviors that one can define via environment variables.\nHowever, the  SCONE_*   environment variables have higher priority than the settings from the config file.  Format of SCONE Configuration File  The format for the configuration file: on each line, there is a\nstatement beginning with a single-character command code, and up to\nthree numbers. The possible commands currently are:   Q  n   - defines the number of queues used to execute system calls  n = number of queues : sets the number of syscall-return queue pairs to allocate. This is equivalent to setting the  SCONE_QUEUES  environment variable;    H  s  - defines the heap size in bytes  s = heap size in bytes : sets the size of heap, equivalent to setting  SCONE_HEAP  environment variable;    P  N  - determines the backoff behavior of the queues  N = spin number : equivalent to setting  SCONE_SSPINS  environment variable;    L  S  - determines the backoff behavior of the queues  S = sleep time : equivalent to setting  SCONE_SSLEEP  environment variable;    s  C Q R   -  sthread  serving system calls outside of an enclave  C = core-no : if non-negative number: pin this sthread to core  C ; if a negative number, do not pin this thread  Q = queue-no in [0..n] : this sthreads serves this queue  R = realtime? : always set this to 0    e  C Q R    ethread  running inside of enclave and executes application threads (which we call lthreads)  C = core-no : non-negative number: pin to this core; negative number: no pinning ot this thread  Q = queue-no in [0..n] : this sthreads serves this queue  R = realtime? : always set this to 0     The number of  sthreads  is automatically increased if more  sthreads  are needed to process system calls. An  sthread  will block if it does not have any work left to do. Hence, we recommend to start exactly one  sthread  per  ethread .  ethreads  will leave the enclave it there is no more work for them to do. Hence, it makes sense to start one ehthread per core. In some situations, it might even make sense to start one  ethread  per hyper-thread.  Unless you want to limit the the number of CPU resources an enclave should use,  the following is a good generic configuration file:  $ sudo tee  /etc/sgx-musl.conf << EOF\nQ 4\ne -1 0 0\ns -1 0 0\ne -1 1 0\ns -1 1 0\ne -1 2 0\ns -1 2 0\ne -1 3 0\ns -1 3 0\nEOF",
            "title": "SCONE Configuration File"
        },
        {
            "location": "/SCONE_ENV/#run-mode",
            "text": "SCONE_MODE : defines if application should run inside of an enclave or outside in simulation mode.    =HW : run program inside of enclave. If program fails to create an enclave, it will fail.    =SIM : run program outside of enclave. All SCONE related software runs but just outside of the enclave. This is not 100% compatible with hardware mode since, for example, some instructions are permitted in native mode that are not permitted in hardware mode.    =AUTO : run program inside of enclave if SGX is available. Otherwise, run in simulation mode. AUTO is the default mode.      SCONE_HEAP : size of the heap allocated for the program during the startup of the enclave.   =s   the requested heap size in bytes",
            "title": "Run Mode"
        },
        {
            "location": "/SCONE_ENV/#debug",
            "text": "SCONE_VERSION  if defined, SCONE will print the values of some of the SCONE environment variables during startup.   Example output:  export SCONE_QUEUES=4\nexport SCONE_SLOTS=256\nexport SCONE_SIGPIPE=0\nexport SCONE_MMAP32BIT=0\nexport SCONE_SSPINS=100\nexport SCONE_SSLEEP=4000\nexport SCONE_KERNEL=0\nexport SCONE_HEAP=1073741824\nexport SCONE_CONFIG=/etc/sgx-musl.conf\nexport SCONE_MODE=hw\nexport SCONE_SGXBOUNDS=no\nexport SCONE_ALLOW_DLOPEN=no\nRevision: 9b355b99170ad434010353bb9f4dca24e532b1b7\nBranch: master\nConfigure options: --enable-file-prot --enable-shared --enable-debug --prefix=/scone/src/built/cross-compiler/x86_64-linux-musl",
            "title": "Debug"
        },
        {
            "location": "/SCONE_ENV/#dynamic-library-support",
            "text": "SCONE_ALLOW_DLOPEN : if defined, permit to load shared libraries during startup. All libraries that are loaded during startup are measured and contribute to the hash of the enclave, i.e., the are part of  MRENCLAVE .    SCONE_ALLOW_DLOPEN=\"2\" : not only can dynamic libraries be loaded during startup but also later on. For example, Python programs might dynamically load modules after startup. Our approach to enforce the integrity of these dynamic libraries with the help of a file protection shield.",
            "title": "Dynamic library support:"
        },
        {
            "location": "/SCONE_ENV/#performance-tuning-variables",
            "text": "SCONE_QUEUES : number of systemcall queues to be used.    SCONE_SLOTS : systemcall queue length: must be larger than the maximum number of  lthreads .    SCONE_SIGPIPE : if set to \"1\",  SIGPIPE  signals are delivered to the application    SCONE_SSPINS=N : In case an Ethread does not have any lthread to execute, an Ethread first pauses for up to N times to wait for more work to show up. After that, it sleeps for up to N times. Each time increasing the sleep time.     SCONE_SSLEEP : determines how fast we increase the backoff.",
            "title": "Performance tuning variables:"
        },
        {
            "location": "/SCONE_ENV/#safety",
            "text": "SCONE_SGXBOUNDS : must be defined to enable bounds checking. Furthermore, you need to compile your program with our SGX boundschecker.",
            "title": "Safety"
        },
        {
            "location": "/SCONE_ENV/#dynamic-link-loader",
            "text": "The dynamic link loader is part of image  sconecuratedimages/crosscompilers:runtime  ( see tutorial ).    LD_LIBRARY_PATH : you can control from where the dynamic link loader looks for shared libraries.    LD_PRELOAD : you can instruct the dynamic link loader to load libraries before loading the libraries specified by the program itself.    SCONE_ALPINE=1 : run dynamically-linked program inside of an enclave.    \u00a9  scontain.com , November 2017.  Questions or Suggestions?",
            "title": "Dynamic link loader"
        },
        {
            "location": "/SCONE_Publications/",
            "text": "SCONE Related Publications\n\n\nSCONE: Secure Linux Containers with Intel SGX, USENIX, OSDI 2016\n\n\nThis paper describes how we support unmodified applications inside of enclaves. The focus is on our asynchronous system\n call interface.\n\n\n\n\n\n\nAuthors\n:  Sergei Arnautov, Bohdan Trach, Franz Gregor, Thomas Knauth, Andr\u00e9 Martin, Christian Priebe, Joshua Lind, Divya Muthukumaran, Daniel O'Keeffe, Mark L Stillwell, David Goltzsche, Dave Eyers, R\u00fcdiger Kapitza, Peter Pietzuch, Christof Fetzer\n\n\n\n\n\n\nMedia\n: \npdf\n, \nslides\n\n\naudio\n\n\n\n\n\n\nAbstract\n:  In multi-tenant environments, Linux containers managed by Docker or Kubernetes have a lower resource footprint, faster startup times, and higher I/O performance compared to virtual machines (VMs) on hypervisors. Yet their weaker isolation guarantees, enforced through software kernel mechanisms, make it easier for attackers to compromise the confidentiality and integrity of application data within containers.\nWe describe SCONE, a secure container mechanism for Docker that uses the SGX trusted execution support of Intel CPUs to protect container processes from outside attacks. The design of SCONE leads to (i) a small trusted computing base (TCB) and (ii) a low performance overhead: SCONE offers a secure C standard library interface that transparently encrypts/decrypts I/O data; to reduce the performance impact of thread synchronization and system calls within SGX enclaves, SCONE supports user-level threading and asynchronous system calls. Our evaluation shows that it protects unmodified applications with SGX, achieving 0.6x\u20131.2x of native throughput.\n\n\n\n\n\n\n Building Critical Applications Using Microservices, IEEE Security & Privacy, Volume: 14 Issue: 6, December 2016 \n\n\n\n\n\n\nAuthor\n: Christof Fetzer\n\n\n\n\n\n\nMedia\n: \nhtml\n\n\n\n\n\n\nAbstract\n: Safeguarding the correctness of critical software is a grand challenge. A microservice-based system is described that builds trustworthy systems on top of legacy hardware and software components, ensuring microservices' integrity, confidentiality, and correct execution with the help of secure enclaves.\n\n\n\n\n\n\n SGXBounds: Memory Safety for Shielded Execution, EuroSys 2017\n\n\nTo protect the code running inside of an enclave, we implemented a novel bounds checker for enclaves. While we had expected\n to just be able to use MPX, we had to realized that MPX does not perform that well inside of enclaves. For details regarding\n the overheads,  please see this paper. \nThis won the best paper award of EuroSys 2017.\n\n\n\n\n\n\nAuthors\n: D. Kuvaiskii, O. Oleksenko, S. Arnautov, B. Trach, P. Bhatotia, P. Felber, C. Fetzer \n\n\n\n\n\n\nMedia\n: \npdf\n \n\n\n\n\n\n\nAbstract\n: Shielded execution based on Intel SGX provides strong security guarantees for legacy applications running on untrusted platforms. However, memory safety attacks such as Heartbleed can render the confidentiality and integrity properties of shielded execution completely ineffective. To prevent these attacks, the state-of-the-art memory-safety approaches can be used in the context of shielded execution. In this work, we first showcase that two prominent software- and hardware-based defenses, AddressSanitizer and Intel MPX respectively, are impractical for shielded execution due to high performance and memory overheads. This motivated our design of SGXBounds -- an efficient memory-safety approach for shielded execution exploiting the architectural features of Intel SGX. Our design is based on a simple combination of tagged pointers and compact memory layout. We implemented SGXBounds based on the LLVM compiler framework targeting unmodified multithreaded applications. Our evaluation using Phoenix, PARSEC, and RIPE benchmark suites shows that SGXBounds has performance and memory overheads of 18% and 0.1% respectively, while providing security guarantees similar to AddressSanitizer and Intel MPX. We have obtained similar results with four real-world case studies: SQLite, Memcached, Apache, and Nginx.\n\n\n\n\n\n\n FFQ: A Fast Single-Producer/Multiple-Consumer Concurrent FIFO Queue, IPDPS 2017\n\n\nThis paper describes our new lock-free queue for our asynchronous system calls.\n\n\n\n\n\n\nAuthors\n: Sergei Arnautov, Pascal Felber, Christof Fetzer and Bohdan Trach\n\n\n\n\n\n\nMedia\n: \nsorry, not yet available\n \n\n\n\n\n\n\nAbstract\n:  With the spreading of multi-core architectures, operating systems and applications are becoming increasingly more concurrent and their scalability is often limited by the primitives used to synchronize the different hardware threads. In this paper, we address the problem of how to optimize the throughput of a system with multiple producer and consumer threads. Such applications typically synchronize their threads via multi- producer/multi-consumer FIFO queues, but existing solutions have poor scalability, as we could observe when designing a secure application framework that requires high-throughput communication between many concurrent threads. In our target system, however, the items enqueued by different producers do not necessarily need to be FIFO ordered. Hence, we propose a fast FIFO queue, FFQ, that aims at maximizing throughput by specializing the algorithm for single-producer/multiple-consumer settings: each producer has its own queue from which multiple consumers can concurrently dequeue. Furthermore, while we pro- vide a wait-free interface for producers, we limit ourselves to lock-free consumers to eliminate the need for helping. We also propose a multi-producer variant to show which synchronization operations we were able to remove by focusing on a single producer variant. Our evaluation analyses the performance using micro- benchmarks and compares our results with other state-of-the-art solutions: FFQ exhibits excellent performance and scalability.\n\n\n\n\n\n\n\u00a9 \nscontain.com\n, November 2017. \nQuestions or Suggestions?",
            "title": "Publications"
        },
        {
            "location": "/SCONE_Publications/#scone-related-publications",
            "text": "SCONE: Secure Linux Containers with Intel SGX, USENIX, OSDI 2016  This paper describes how we support unmodified applications inside of enclaves. The focus is on our asynchronous system\n call interface.    Authors :  Sergei Arnautov, Bohdan Trach, Franz Gregor, Thomas Knauth, Andr\u00e9 Martin, Christian Priebe, Joshua Lind, Divya Muthukumaran, Daniel O'Keeffe, Mark L Stillwell, David Goltzsche, Dave Eyers, R\u00fcdiger Kapitza, Peter Pietzuch, Christof Fetzer    Media :  pdf ,  slides  audio    Abstract :  In multi-tenant environments, Linux containers managed by Docker or Kubernetes have a lower resource footprint, faster startup times, and higher I/O performance compared to virtual machines (VMs) on hypervisors. Yet their weaker isolation guarantees, enforced through software kernel mechanisms, make it easier for attackers to compromise the confidentiality and integrity of application data within containers.\nWe describe SCONE, a secure container mechanism for Docker that uses the SGX trusted execution support of Intel CPUs to protect container processes from outside attacks. The design of SCONE leads to (i) a small trusted computing base (TCB) and (ii) a low performance overhead: SCONE offers a secure C standard library interface that transparently encrypts/decrypts I/O data; to reduce the performance impact of thread synchronization and system calls within SGX enclaves, SCONE supports user-level threading and asynchronous system calls. Our evaluation shows that it protects unmodified applications with SGX, achieving 0.6x\u20131.2x of native throughput.     Building Critical Applications Using Microservices, IEEE Security & Privacy, Volume: 14 Issue: 6, December 2016     Author : Christof Fetzer    Media :  html    Abstract : Safeguarding the correctness of critical software is a grand challenge. A microservice-based system is described that builds trustworthy systems on top of legacy hardware and software components, ensuring microservices' integrity, confidentiality, and correct execution with the help of secure enclaves.     SGXBounds: Memory Safety for Shielded Execution, EuroSys 2017  To protect the code running inside of an enclave, we implemented a novel bounds checker for enclaves. While we had expected\n to just be able to use MPX, we had to realized that MPX does not perform that well inside of enclaves. For details regarding\n the overheads,  please see this paper.  This won the best paper award of EuroSys 2017.    Authors : D. Kuvaiskii, O. Oleksenko, S. Arnautov, B. Trach, P. Bhatotia, P. Felber, C. Fetzer     Media :  pdf      Abstract : Shielded execution based on Intel SGX provides strong security guarantees for legacy applications running on untrusted platforms. However, memory safety attacks such as Heartbleed can render the confidentiality and integrity properties of shielded execution completely ineffective. To prevent these attacks, the state-of-the-art memory-safety approaches can be used in the context of shielded execution. In this work, we first showcase that two prominent software- and hardware-based defenses, AddressSanitizer and Intel MPX respectively, are impractical for shielded execution due to high performance and memory overheads. This motivated our design of SGXBounds -- an efficient memory-safety approach for shielded execution exploiting the architectural features of Intel SGX. Our design is based on a simple combination of tagged pointers and compact memory layout. We implemented SGXBounds based on the LLVM compiler framework targeting unmodified multithreaded applications. Our evaluation using Phoenix, PARSEC, and RIPE benchmark suites shows that SGXBounds has performance and memory overheads of 18% and 0.1% respectively, while providing security guarantees similar to AddressSanitizer and Intel MPX. We have obtained similar results with four real-world case studies: SQLite, Memcached, Apache, and Nginx.     FFQ: A Fast Single-Producer/Multiple-Consumer Concurrent FIFO Queue, IPDPS 2017  This paper describes our new lock-free queue for our asynchronous system calls.    Authors : Sergei Arnautov, Pascal Felber, Christof Fetzer and Bohdan Trach    Media :  sorry, not yet available      Abstract :  With the spreading of multi-core architectures, operating systems and applications are becoming increasingly more concurrent and their scalability is often limited by the primitives used to synchronize the different hardware threads. In this paper, we address the problem of how to optimize the throughput of a system with multiple producer and consumer threads. Such applications typically synchronize their threads via multi- producer/multi-consumer FIFO queues, but existing solutions have poor scalability, as we could observe when designing a secure application framework that requires high-throughput communication between many concurrent threads. In our target system, however, the items enqueued by different producers do not necessarily need to be FIFO ordered. Hence, we propose a fast FIFO queue, FFQ, that aims at maximizing throughput by specializing the algorithm for single-producer/multiple-consumer settings: each producer has its own queue from which multiple consumers can concurrently dequeue. Furthermore, while we pro- vide a wait-free interface for producers, we limit ourselves to lock-free consumers to eliminate the need for helping. We also propose a multi-producer variant to show which synchronization operations we were able to remove by focusing on a single producer variant. Our evaluation analyses the performance using micro- benchmarks and compares our results with other state-of-the-art solutions: FFQ exhibits excellent performance and scalability.    \u00a9  scontain.com , November 2017.  Questions or Suggestions?",
            "title": "SCONE Related Publications"
        },
        {
            "location": "/glossary/",
            "text": "Glossary\n\n\n\n\n\n\ncloud-native application\n. An application designed to run inside of a cloud. One requirement is that the application is deployed with the help of containers.\n\n\n\n\n\n\ncontainer\n. An light-weight alternative to a virtual machine (VM). The isolation of containers is implemented by the operating system. Docker and Kubernetes use Linux for isolation. In the case of VMs, the isolation is implemented with the help of CPU extensions.\n\n\n\n\n\n\nenclave\n. This is an alias for \nSGX enclave\n.\n\n\n\n\n\n\nEPC\n. A cache of memory pages belonging to enclaves. This cache resides in a reserved part of the main memory that is directly managed by the CPU (and not by the operating system or the hypervisor). The data in this cache is encrypted. Unlike enclave pages residing in the main memory, the CPU can encrypt and decrypt individual cache lines residing inside the EPC. This results in low overheads.\n\n\n\n\n\n\nSecure container\n. A container which uses additional hardware isolation mechanisms, i.e., SGX to provide better application security. In particular, a secure container runs one or more \nsecure programs\n. Additionally, the integrity and confidentiality of files inside a secure container are protected by SCONE.\n\n\n\n\n\n\nSecure program\n. A program that executes inside an enclave.\n\n\n\n\n\n\nSGX\n. A CPU extension by Intel that permits to create SGX enclaves.\n\n\n\n\n\n\nSGX enclave\n. A protected area inside the address space of a program such that only the code inside this enclave can access the data and code stored in this address range. All pages belonging to an enclave are encrypted by the CPU and only the CPU knows the encryption key. These pages can reside in the main memory or the EPC.\n\n\n\n\n\n\nethread, lthread, sthread\n. SCONE usese different kind of threads:\n\n\n\n\nethread\n: a thread that executes application threads inside of an enclave\n\n\nlthread\n: an application thread. Typically, created by the application directly or indirectly via a \npthread_create\n call. In SCONE, this \npthread_create\n call will create a \nlthread\n. The lthread is executed by some \nethread\n. In this way, we can quickly switch to another application thread whenever an application thread would get block. In this way, we reduce the number of enclave entries and exits - which are costly.\n\n\nsthread\n: a thread that runs outside of the enclave and that executes system calls on behalf of the threads running inside the enclave\n\n\n\n\n\n\n\n\n\u00a9 \nscontain.com\n, November 2017. \nQuestions or Suggestions?",
            "title": "Glossary"
        },
        {
            "location": "/glossary/#glossary",
            "text": "cloud-native application . An application designed to run inside of a cloud. One requirement is that the application is deployed with the help of containers.    container . An light-weight alternative to a virtual machine (VM). The isolation of containers is implemented by the operating system. Docker and Kubernetes use Linux for isolation. In the case of VMs, the isolation is implemented with the help of CPU extensions.    enclave . This is an alias for  SGX enclave .    EPC . A cache of memory pages belonging to enclaves. This cache resides in a reserved part of the main memory that is directly managed by the CPU (and not by the operating system or the hypervisor). The data in this cache is encrypted. Unlike enclave pages residing in the main memory, the CPU can encrypt and decrypt individual cache lines residing inside the EPC. This results in low overheads.    Secure container . A container which uses additional hardware isolation mechanisms, i.e., SGX to provide better application security. In particular, a secure container runs one or more  secure programs . Additionally, the integrity and confidentiality of files inside a secure container are protected by SCONE.    Secure program . A program that executes inside an enclave.    SGX . A CPU extension by Intel that permits to create SGX enclaves.    SGX enclave . A protected area inside the address space of a program such that only the code inside this enclave can access the data and code stored in this address range. All pages belonging to an enclave are encrypted by the CPU and only the CPU knows the encryption key. These pages can reside in the main memory or the EPC.    ethread, lthread, sthread . SCONE usese different kind of threads:   ethread : a thread that executes application threads inside of an enclave  lthread : an application thread. Typically, created by the application directly or indirectly via a  pthread_create  call. In SCONE, this  pthread_create  call will create a  lthread . The lthread is executed by some  ethread . In this way, we can quickly switch to another application thread whenever an application thread would get block. In this way, we reduce the number of enclave entries and exits - which are costly.  sthread : a thread that runs outside of the enclave and that executes system calls on behalf of the threads running inside the enclave     \u00a9  scontain.com , November 2017.  Questions or Suggestions?",
            "title": "Glossary"
        },
        {
            "location": "/aboutScone/",
            "text": "About SCONE\n\n\nThe SCONE platform is commercially supported by \nscontain.com\n. \n\n\nThe SCONE platform has been developed at the \nSystems Engineering group\n at \nTU Dresden\n in the context of the following EU projects:\n\n\n\n\n\n\nSereca\n which investigates how to use Intel SGX enclave in the context of reactive programs written in \nVert.x\n.\n\n\n\n\n\n\nSecure Cloud\n which focuses on the processing of (big) data in untrusted clouds.\n\n\n\n\n\n\nWe investigate use cases and extensions of SCONE in the context of the following EU projects:\n\n\n\n\n\n\nSELIS\n: we investigate how to secure data processing within a \nShared European Logistics Intelligent Information Space\n with the help of SCONE.\n\n\n\n\n\n\nATMOSPHERE\n: a new EU project in which we address secure data management services. This will help to extend the SCONE platform.\n\n\n\n\n\n\nLEGATO\n: a new EU project in which we address \nhigh integrity computations\n inside of enclaves to be able to detect and tolerate miscomputations inside of enclaves.\n\n\n\n\n\n\nComputing Resources\n\n\nscontain.com\n can provide access to SGX-capable machines with the help of \nCLOUD&HEAT\n.\n\n\nConsulting Services\n\n\nscontain.com\n provides consulting services as well as helping you to port your applications to SGX. We also have external partners like \nSIListra Systems\n that can port applications to run software inside of SGX enclaves with the help of SCONE.\n\n\nContact\n\n\nIf you want to evaluate the SCONE platform, want to rent some SGX-capable computing resources, need SGX and SCONE-related consulting, or have some technical questions, please contact us at \ninfo@scontain.com\n.\n\n\nLegal Notice\n\n\nThe content of this documentation is maintained by \nscontain.com\n.\n\n\n\u00a9 \nscontain.com\n, November 2017. \nQuestions or Suggestions?",
            "title": "About Scone"
        },
        {
            "location": "/aboutScone/#about-scone",
            "text": "The SCONE platform is commercially supported by  scontain.com .   The SCONE platform has been developed at the  Systems Engineering group  at  TU Dresden  in the context of the following EU projects:    Sereca  which investigates how to use Intel SGX enclave in the context of reactive programs written in  Vert.x .    Secure Cloud  which focuses on the processing of (big) data in untrusted clouds.    We investigate use cases and extensions of SCONE in the context of the following EU projects:    SELIS : we investigate how to secure data processing within a  Shared European Logistics Intelligent Information Space  with the help of SCONE.    ATMOSPHERE : a new EU project in which we address secure data management services. This will help to extend the SCONE platform.    LEGATO : a new EU project in which we address  high integrity computations  inside of enclaves to be able to detect and tolerate miscomputations inside of enclaves.",
            "title": "About SCONE"
        },
        {
            "location": "/aboutScone/#computing-resources",
            "text": "scontain.com  can provide access to SGX-capable machines with the help of  CLOUD&HEAT .",
            "title": "Computing Resources"
        },
        {
            "location": "/aboutScone/#consulting-services",
            "text": "scontain.com  provides consulting services as well as helping you to port your applications to SGX. We also have external partners like  SIListra Systems  that can port applications to run software inside of SGX enclaves with the help of SCONE.",
            "title": "Consulting Services"
        },
        {
            "location": "/aboutScone/#contact",
            "text": "If you want to evaluate the SCONE platform, want to rent some SGX-capable computing resources, need SGX and SCONE-related consulting, or have some technical questions, please contact us at  info@scontain.com .",
            "title": "Contact"
        },
        {
            "location": "/aboutScone/#legal-notice",
            "text": "The content of this documentation is maintained by  scontain.com .  \u00a9  scontain.com , November 2017.  Questions or Suggestions?",
            "title": "Legal Notice"
        }
    ]
}